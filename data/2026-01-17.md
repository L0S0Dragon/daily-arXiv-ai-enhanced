<div id=toc></div>

# Table of Contents

- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 4]
- [quant-ph](#quant-ph) [Total: 70]


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [1] [Emergent Nonperturbative Universal Floquet Localization](https://arxiv.org/abs/2601.09793)
*Soumadip Pakrashi,Atanu Rajak,Sambuddha Sanyal*

Main category: cond-mat.dis-nn

TL;DR: A robust, nonperturbative localization plateau emerges in periodically driven quasiperiodic lattices at a fine-tuned amplitude-to-frequency ratio, making all Floquet states localized despite dense resonances.


<details>
  <summary>Details</summary>
Motivation: To understand localization phenomena in periodically driven quasiperiodic systems and identify conditions where robust localization emerges independent of static properties and drive protocols.

Method: Exact Floquet dynamics, Floquet perturbation theory, and optimal-order van Vleck analysis to study driven quasiperiodic lattices and identify fine-tuned amplitude-to-frequency ratios.

Result: A nonperturbative localization plateau emerges at specific amplitude-to-frequency ratios where all Floquet states become localized, with van Vleck expansion achieving superasymptotic accuracy until breaking down due to resonant hybridization.

Conclusion: The observed localization is fundamentally nonperturbative, emerging from fine-tuned drive parameters and persisting despite dense resonances, with van Vleck analysis revealing the breakdown mechanism through resonant hybridization.

Abstract: We show that a robust, nonperturbative localization plateau emerges in periodically driven quasiperiodic lattices, independent of the static localization properties and drive protocol. Using exact Floquet dynamics, Floquet perturbation theory, and optimal-order van Vleck analysis, we identify a fine-tuned amplitude-to-frequency ratio where all Floquet states become localized despite dense resonances. The van Vleck expansion achieves superasymptotic accuracy up to an optimal orde; it ultimately breaks down due to resonant hybridization at a weak quasiperiodic potential, revealing that the observed localization is nonperturbative.

</details>


### [2] [Integral Variable Range Hopping for Modeling Electrical Transport in Disordered Systems](https://arxiv.org/abs/2601.10226)
*Chenxin Qin,Chenyan Wang,Mouyang Cheng,Ji Chen*

Main category: cond-mat.dis-nn

TL;DR: The paper introduces an Integral Variable Range Hopping (IVRH) model that replaces empirical temperature power-law dependencies in standard VRH theories with a physics-inspired integral formulation, providing more stable and physically meaningful fitting for disordered systems.


<details>
  <summary>Details</summary>
Motivation: Standard VRH models rely on oversimplified assumptions that restrict applicability and cause problematic fitting behaviors, yet they are overused despite these limitations.

Method: Developed an IVRH model based on hopping probability ω(R) with respect to hopping distance R, incorporating density of accessible electronic states through an effective volume function V(R) that reflects system geometry influence.

Result: IVRH inherently reproduces both Mott behavior at low temperatures and Arrhenius behavior at high temperatures with smooth transition between regimes. Monte Carlo simulations validate predictions and yield consistent fitting parameters with substantially reduced variances compared to standard VRH. Improved robustness extends to transport measurements in monolayer MoS₂ and WS₂ systems.

Conclusion: IVRH offers a more stable and physically sound framework for interpreting hopping transport in low-dimensional amorphous materials, providing deeper insights into universal geometric scaling factors governing charge transport in disordered systems.

Abstract: The variable range hopping (VRH) model has been widely applied to describe electrical transport in disordered systems, providing theoretical formulas to fit temperature-dependent electric conductivity. These models rely on oversimplified assumptions that restrict their applicability and result in problematic fitting behaviors, yet their overusing situation is becoming increasingly serious. In this work we formulate an integral variable range hopping (IVRH) model, which replaces the empirical temperature power-law dependence in standard VRH theories with a physics-inspired integral formulation. The model builds upon the standard hopping probability $ω(R)$ w.r.t. hopping distance $R$ and incorporates the density of accessible electronic states through an effective volume function $V(R)$, which reflects the influence of system geometry. The IVRH formulation inherently reproduces both the Mott behavior at low temperatures and the Arrhenius behavior at high temperatures, respectively, and enables a smooth transition between the two regimes. We apply the IVRH model to two-dimensional, three-dimensional, and multi-layered systems. Monte Carlo simulations validate the model's predictions and yield consistent values for the fitting parameters, with substantially reduced variances compared to fitting using the standard VRH model. Furthermore, the improved robustness of IVRH also extends to the transport measurements in monolayer MoS$_2$ system and monolayer WS$_2$ system, enabling more physically meaningful interpretation.IVRH model offers a more stable and physically sound framework for interpreting hopping transport in low-dimensional amorphous materials, providing deeper insights into the universal geometric scaling factors that govern charge transport in disordered systems.

</details>


### [3] [Computer Generation of Disordered Networks with Targeted Structural Properties](https://arxiv.org/abs/2601.10333)
*Florin Hemmann,Vincent Glauser,Ullrich Steiner,Matthias Saba*

Main category: cond-mat.dis-nn

TL;DR: Extension of Wooten-Weaire-Winer algorithm to arbitrary coordination numbers using bond repulsion in Keating strain energy, enabling targeted generation of disordered spatial networks with tunable disorder via bond-bending force constants and temperature profiles.


<details>
  <summary>Details</summary>
Motivation: Need for efficient numerical methods to generate disordered spatial networks with targeted structural properties for studying complex phenomena like structural phase transitions, localization, diffusion, and band gaps. Conventional methods are limited to 3D networks with coordination numbers ≤4.

Method: Extended Wooten-Weaire-Winer algorithm by introducing bond repulsion in Keating strain energy to handle arbitrary coordination number statistics. Tuned disorder by varying bond-bending force constant and temperature profile. Used order metrics in both direct and reciprocal space. Trained feedforward neural network to predict structural characteristics from algorithm inputs.

Result: Successfully generated disordered networks with tailored structural properties. Demonstrated capability to statistically reproduce four disordered biophotonic networks exhibiting structural color. Neural network enables targeted network generation by predicting structural characteristics from inputs.

Conclusion: Presents versatile method for generating disordered networks with tailored structural properties, enabling new insights into structure-property relations such as photonic band gaps in disordered networks.

Abstract: Disordered spatial networks are model systems that describe structures and interactions across multiple length scales. Scattering and interference of waves in these networks can give rise to structural phase transitions, localization, diffusion, and band gaps. The study of these complex phenomena requires efficient numerical methods to computer-generate disordered networks with targeted structural properties. In the established Wooten-Weaire-Winer algorithm, a series of bond switch moves introduces disorder into an initial network. Conventional strain energies that govern this evolution are limited to 3D networks with coordination numbers of no more than four. We extend the algorithm to arbitrary coordination number statistics by introducing bond repulsion in the Keating strain energy. We tune the degree and type of disorder introduced into initially crystalline networks by varying the bond-bending force constant in the strain energy and the temperature profile. The effects of these variables are analyzed using a list of order metrics that capture both direct and reciprocal space. A feedforward neural network is trained to predict the structural characteristics from the algorithm inputs, enabling targeted network generation. As a case study, we statistically reproduce four disordered biophotonic networks exhibiting structural color. This work presents a versatile method for generating disordered networks with tailored structural properties. It will enable new insights into structure-property relations, such as photonic band gaps in disordered networks.

</details>


### [4] [The eigenvalues and eigenvectors of finite-rank normal perturbations of large rotationally invariant non-Hermitian matrices](https://arxiv.org/abs/2601.10427)
*Pierre Bousseyroux,Marc Potters*

Main category: cond-mat.dis-nn

TL;DR: Finite-rank normal deformations of rotationally invariant non-Hermitian random matrices: extension of BBP framework to characterize outlier eigenvalues and eigenvectors.


<details>
  <summary>Details</summary>
Motivation: Extend classical Baik-Ben Arous-Péché (BBP) framework to non-Hermitian random matrices with finite-rank normal perturbations, providing unified theory for outlier eigenvalue emergence and fluctuations.

Method: Study models of form A + T, where A is large rotationally invariant non-Hermitian random matrix and T is finite-rank normal perturbation. Characterize emergence and fluctuations of outlier eigenvalues and eigenvector behavior.

Result: Provides unified framework encompassing both Hermitian and non-Hermitian settings, generalizing several known cases. Characterizes conditions for outlier emergence, their fluctuations, and corresponding eigenvector behavior.

Conclusion: Successfully extends BBP framework to non-Hermitian random matrices with finite-rank normal perturbations, offering comprehensive theory for outlier eigenvalues and eigenvectors in this generalized setting.

Abstract: We study finite-rank normal deformations of rotationally invariant non-Hermitian random matrices. Extending the classical Baik-Ben Arous-Péché (BBP) framework, we characterize the emergence and fluctuations of outlier eigenvalues in models of the form $\mathbf{A} + \mathbf{T}$, where $\mathbf{A}$ is a large rotationally invariant non-Hermitian random matrix and $\mathbf{T}$ is a finite-rank normal perturbation. We also describe the corresponding eigenvector behavior. Our results provide a unified framework encompassing both Hermitian and non-Hermitian settings, thereby generalizing several known cases.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [5] [Limits of Rank Recovery in Bilinear Observation Problems](https://arxiv.org/abs/2601.09754)
*Seungbeom Choi*

Main category: quant-ph

TL;DR: Bilinear observation problems exhibit stable rank plateaus that persist across tolerance variations, indicating dimensional deficits not resolved by numerical refinement alone but requiring structural problem modification.


<details>
  <summary>Details</summary>
Motivation: To examine the assumption that rank deficiency in bilinear observation problems can be resolved through numerical refinement, and to understand the structural limitations of rank recovery in such problems.

Method: Analyzed the rank and nullity of bilinear observation operators under systematic tolerance variation, studied the operator directly rather than specific reconstruction algorithms, resolved nullspace into algebraic sectors defined by variable block structure, and compared refinement with explicit problem modification.

Result: Identified extended rank plateaus persisting across broad tolerance ranges, revealing stable dimensional deficits. Nullspace exhibits pronounced but nonexclusive concentration in specific algebraic sectors, showing organized internal structure. Rank recovery requires changing the bilinear observation structure itself, not just numerical refinements.

Conclusion: Bilinear observation problems have inherent dimensional limitations that cannot be overcome by numerical refinement alone; structural modifications to the observation problem formulation are necessary for rank recovery, clarifying the distinction between refinement and problem modification.

Abstract: Bilinear observation problems arise in many physical and information-theoretic settings, where observables and states enter multiplicatively. Rank-based diagnostics are commonly used in such problems to assess the effective dimensionality accessible to observation, often under the implicit assumption that rank deficiency can be resolved through numerical refinement. Here we examine this assumption by analyzing the rank and nullity of a bilinear observation operator under systematic tolerance variation. Rather than focusing on a specific reconstruction algorithm, we study the operator directly and identify extended rank plateaus that persist across broad tolerance ranges. These plateaus indicate stable dimensional deficits that are not removed by refinement procedures applied within a fixed problem definition. To investigate the origin of this behavior, we resolve the nullspace into algebraic sectors defined by the block structure of the variables. The nullspace exhibits a pronounced but nonexclusive concentration in specific sectors, revealing an organized internal structure rather than uniform dimensional loss. Comparing refinement with explicit modification of the problem formulation further shows that rank recovery in the reported setting requires a change in the structure of the observation problem itself. Here, "problem modification" refers to changes that alter the bilinear observation structure (e.g., admissible operator/state families or coupling constraints), in contrast to refinements that preserve the original formulation such as tolerance adjustment and numerical reparameterizations. Together, these results delineate limits of rank recovery in bilinear observation problems and clarify the distinction between numerical refinement and problem modification in accessing effective dimensional structure.

</details>


### [6] [Fractional Revival Dynamics in Kerr-Type Systems: Angular Momentum Moments and Classical Analogs](https://arxiv.org/abs/2601.09763)
*Ashish Kumar Patra,Saikumar Krithivasan*

Main category: quant-ph

TL;DR: Analysis of quantum revival phenomena in nonlinear systems, focusing on angular momentum observables and classical analogs using Kerr-type Hamiltonians.


<details>
  <summary>Details</summary>
Motivation: To extend the study of quantum revival phenomena beyond previous work by investigating fractional revivals in angular momentum observables and establishing connections between quantum revival phenomena and classical recurrence behavior.

Method: Using the Kerr-type nonlinear Hamiltonian as a paradigmatic model, analyzing autocorrelation functions, moment dynamics, and phase-space structures with visualization techniques including quantum carpets.

Result: Higher-order angular momentum moments provide clear and selective signatures of fractional revivals, and structural similarities are established between quantum fractional revivals and recurrence behavior in classical systems.

Conclusion: The study broadens experimental diagnostics for fractional revivals and provides a unified perspective on revival phenomena across quantum and classical dynamical systems.

Abstract: Wave packet revivals and fractional revivals are hallmark quantum interference phenomena that arise in systems with nonlinear energy spectra, and their signatures in expectation values of observables have been studied extensively in earlier work. In this article, we build on these studies and extend the analysis in two important directions. First, we investigate fractional revival dynamics in angular momentum observables, deriving explicit expressions for the time evolution of their moments and demonstrating that higher-order angular momentum moments provide clear and selective signatures of fractional revivals. Second, we examine classical analogs of quantum revival phenomena and elucidate structural similarities between quantum fractional revivals and recurrence behavior in representative classical systems. Using the Kerr-type nonlinear Hamiltonian as a paradigmatic model, we analyze the autocorrelation function, moment dynamics, and phase-space structures, supported by visualizations such as quantum carpets. Our results broaden the range of experimentally accessible diagnostics of fractional revivals and provide a unified perspective on revival phenomena across quantum and classical dynamical systems.

</details>


### [7] [Three questions on the future of quantum science and technology](https://arxiv.org/abs/2601.09769)
*S. Radenkovic,M. Dugic,I. Radojevic*

Main category: quant-ph

TL;DR: Analysis of current status and future development of Quantum Science and Technology


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the current state and future trajectory of Quantum Science and Technology, addressing its significance and potential impact across various domains.

Method: Analysis and synthesis of existing knowledge, trends, and developments in quantum science and technology, likely involving literature review, expert insights, and trend analysis.

Result: Presentation of current status assessment and future development projections for Quantum Science and Technology across key areas including quantum computing, communication, sensing, and materials.

Conclusion: Quantum Science and Technology represents a transformative field with significant current progress and promising future developments that will impact computing, communication, sensing, and fundamental science.

Abstract: The answers on the current status and future development of Quantum Science and Technology are presented.

</details>


### [8] [Hierarchical time crystals](https://arxiv.org/abs/2601.09779)
*Jan Carlo Schumann,Igor Lesanovsky,Parvinder Solanki*

Main category: quant-ph

TL;DR: A hierarchical time crystal phase emerges from coupling discrete and continuous time crystals, inducing simultaneous two-fold temporal symmetry breaking with an emergent discrete symmetry.


<details>
  <summary>Details</summary>
Motivation: To explore the intriguing effects arising from mutual interactions between distinct types of time crystals (discrete and continuous), which have previously been studied as standalone systems.

Method: Using a time-independent coupled system of discrete and continuous time crystals to induce simultaneous two-fold temporal symmetry breaking, with one subsystem breaking an emergent discrete temporal symmetry that arises dynamically rather than existing in the dynamical generator.

Result: Demonstrated that hierarchical time crystals are robust, emerging for fundamentally different coupling schemes and persisting across wide ranges of system parameters.

Conclusion: Coupling discrete and continuous time crystals creates a hierarchical time crystal phase with simultaneous two-fold temporal symmetry breaking, representing a novel convoluted non-equilibrium phase with emergent discrete symmetry.

Abstract: Spontaneous symmetry breaking is one of the central organizing principles in physics. Time crystals have emerged as an exotic phase of matter, spontaneously breaking the time translational symmetry, and are mainly categorized as discrete or continuous. While these distinct types of time crystals have been extensively explored as standalone systems, intriguing effects can arise from their mutual interaction. Here, we demonstrate that a time-independent coupled system of discrete and continuous time crystals induces a simultaneous two-fold temporal symmetry breaking, resulting in a hierarchical time crystal phase. Interestingly, one of the subsystems breaks an emergent discrete temporal symmetry that does not exist in the dynamical generator but rather emerges dynamically, leading to a convoluted non-equilibrium phase. We demonstrate that hierarchical time crystals are robust, emerging for fundamentally different coupling schemes and persisting across wide ranges of system parameters.

</details>


### [9] [Zero-Error List Decoding for Classical-Quantum Channels](https://arxiv.org/abs/2601.09786)
*Marco Dalai,Filippo Girardi,Ludovico Lami*

Main category: quant-ph

TL;DR: Study of zero-error capacity for pure-state classical-quantum channels with list decoding, providing achievability for list-size 2 and general converse bounds, identifying special cases where bounds match, and revealing quantum-specific divergence phenomena.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limits of zero-error communication over classical-quantum channels when using list decoding, particularly investigating how quantum properties affect achievable rates compared to classical channels.

Method: Develop achievability bound for list-size two and general converse bound for any fixed list size; analyze channels where pairwise absolute state overlaps form positive semi-definite matrices; examine divergence behavior of sphere-packing bound.

Result: Achievability and converse bounds coincide for channels with positive semi-definite pairwise overlap matrices; quantum channels exhibit unique behavior where sphere-packing bound divergence rate may not be achievable even with arbitrarily large list sizes, unlike classical case.

Conclusion: Zero-error capacity of classical-quantum channels with list decoding shows quantum-specific phenomena, with bounds matching under certain structural conditions, and reveals fundamental differences from classical information theory regarding achievable divergence rates.

Abstract: The aim of this work is to study the zero-error capacity of pure-state classical-quantum channels in the setting of list decoding. We provide an achievability bound for list-size two and a converse bound holding for every fixed list size. The two bounds coincide for channels whose pairwise absolute state overlaps form a positive semi-definite matrix. Finally, we discuss a remarkable peculiarity of the classical-quantum case: differently from the fully classical setting, the rate at which the sphere-packing bound diverges might not be achievable by zero-error list codes, even when we take the limit of fixed but arbitrarily large list size.

</details>


### [10] [Background cancellation for frequency-selective quantum sensing](https://arxiv.org/abs/2601.09792)
*Ricard Puig,Nathan Constantinides,Bharath Hebbe Madhusudhana,Daniel Bowring,C. Huerta Alderete,Andrew T. Sornborger*

Main category: quant-ph

TL;DR: A quantum sensor using time-independent interactions and entanglement as a passive, tunable frequency filter that responds only to target frequencies above a threshold, eliminating complex control and heavy post-processing.


<details>
  <summary>Details</summary>
Motivation: Current quantum sensing methods for detecting weak time-dependent signals require complex dynamical control of quantum sensors and extensive classical post-processing, creating practical implementation challenges.

Method: The proposed quantum sensor leverages time-independent interactions and entanglement to function as a passive, tunable, thresholded frequency filter. It encodes frequency selectivity and thresholding behavior directly into the sensor's dynamics, making it responsive only to target frequencies whose amplitude exceeds a predetermined threshold.

Result: The sensor operates as a passive frequency filter that responds selectively to target frequencies above a threshold, circumventing the need for complex control schemes and reducing post-processing overhead.

Conclusion: This approach provides a simplified quantum sensing framework for detecting weak time-dependent signals by embedding frequency selectivity and thresholding directly into the sensor's quantum dynamics, offering a practical alternative to conventional methods requiring complex control and heavy classical processing.

Abstract: A key challenge in quantum sensing is the detection of weak time dependent signals, particularly those that arise as specific frequency perturbations over a background field. Conventional methods usually demand complex dynamical control of the quantum sensor and heavy classical post-processing. We propose a quantum sensor that leverages time independent interactions and entanglement to function as a passive, tunable, thresholded frequency filter. By encoding the frequency selectivity and thresholding behavior directly into the dynamics, the sensor is responsive only to a target frequency of choice whose amplitude is above a threshold. This approach circumvents the need for complex control schemes and reduces the post-processing overhead.

</details>


### [11] [Localization of quantum states within subspaces](https://arxiv.org/abs/2601.09817)
*L. L. Salcedo*

Main category: quant-ph

TL;DR: The paper proposes a precise definition for quantum state localization probability within subspaces, identifies localized components, establishes mathematical properties, and discusses quantum information applications.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical framework for quantifying and analyzing quantum state localization within specific subspaces of a Hilbert space, addressing fundamental questions about quantum state confinement and its implications for quantum information theory.

Method: Proposes a precise mathematical definition for localization probability, explicitly identifies the localized component of quantum states, and establishes formal mathematical properties of this localization framework.

Result: Develops a complete mathematical formalism for quantum state localization, including definitions, component identification, and proven properties, with demonstrated applications to quantum information contexts.

Conclusion: The paper establishes a rigorous foundation for analyzing quantum state localization within subspaces, providing tools for quantum information applications and opening avenues for further theoretical development in quantum state analysis.

Abstract: A precise definition is proposed for the localization probability of a quantum state within a given subspace of the full Hilbert space of a quantum system. The corresponding localized component of the state is explicitly identified, and several mathematical properties are established. Applications and interpretations in the context of quantum information are also discussed.

</details>


### [12] [Fragmented Topological Excitations in Generalized Hypergraph Product Codes](https://arxiv.org/abs/2601.09850)
*Meng-Yuan Li,Yue Wu*

Main category: quant-ph

TL;DR: The paper investigates fracton topological orders in a family of generalized hypergraph product codes called orthoplex models, discovering novel properties including non-monotonic ground state degeneracy, non-Abelian lattice defects, and fragmented topological excitations in 4D.


<details>
  <summary>Details</summary>
Motivation: To explore fracton topological orders in stabilizer codes constructed via generalized hypergraph product methods, leveraging the connection between quantum codes and exactly solvable spin models to study many-body physics.

Method: Analysis of a family of generalized hypergraph product codes termed "orthoplex models" based on their stabilizer geometry, examining properties in 3D and 4D systems including ground state degeneracy, lattice defects, and excitation patterns.

Result: In 3D orthoplex models: non-monotonic ground state degeneracy as function of system size and non-Abelian lattice defects. In 4D: discovery of fragmented topological excitations - point-like in real space but projecting to connected objects (loops) in lower dimensions, representing intermediate class between point-like and extended excitations.

Conclusion: Generalized hypergraph product codes provide a versatile, analytically tractable platform for studying fracton orders, with orthoplex models revealing rich physics including fragmented excitations that bridge point-like and spatially extended topological excitations.

Abstract: Product code construction is a powerful tool for constructing quantum stabilizer codes, which serve as a promising paradigm for realizing fault-tolerant quantum computation. Furthermore, the natural mapping between stabilizer codes and the ground states of exactly solvable spin models also motivates the exploration of many-body orders in the stabilizer codes. In this work, we investigate the fracton topological orders in a family of codes obtained by a recently proposed general construction. More specifically, this code family can be regarded as a class of generalized hypergraph product (HGP) codes. We term the corresponding exactly solvable spin models \textit{orthoplex models}, based on the geometry of the stabilizers. In the 3D orthoplex model, we identify a series of intriguing properties within this model family, including non-monotonic ground state degeneracy (GSD) as a function of system size and non-Abelian lattice defects. Most remarkably, in 4D we discover \textit{fragmented topological excitations}: while such excitations manifest as discrete, isolated points in real space, their projections onto lower-dimensional subsystems form connected objects such as loops, revealing the intrinsic topological nature of these excitations. Therefore, fragmented excitations constitute an intriguing intermediate class between point-like and spatially extended topological excitations. In addition, these rich features establish the generalized HGP codes as a versatile and analytically tractable platform for studying the physics of fracton orders.

</details>


### [13] [Multi-level quantum emitter in an optical waveguide: paradoxes and resolutions](https://arxiv.org/abs/2601.09854)
*Ben Lang*

Main category: quant-ph

TL;DR: Theoretical investigation of optical dipole interactions between multi-level quantum systems and single-mode optical waveguides, revealing paradoxical behaviors like opposite-direction photon fluxes from non-orthogonal states and isotropic emitters switching between 100% transmission and reflection with infinitesimal polarization changes.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental quantum mechanical principles governing interactions between multi-level quantum systems and optical waveguides with arbitrary local polarization, particularly exploring seemingly paradoxical behaviors that challenge intuitive understanding of quantum optics.

Method: Theoretical analysis using quantum optical models to investigate dipole interactions between multi-level quantum systems and single-mode waveguides. The study examines paradoxical situations mathematically, including analysis of non-orthogonal quantum states producing opposite photon fluxes and isotropic emitters' transmission/reflection properties dependent on waveguide polarization.

Result: Found that non-orthogonal quantum states can produce photon fluxes in opposite directions without violating quantum unitarity. Discovered that isotropic quantum emitters can switch between 100% transmission and 100% reflection with infinitesimal polarization rotations in the zero-loss limit. Demonstrated a four-level system capable of performing non-destructive parity measurements of photon number.

Conclusion: The study reveals counterintuitive quantum optical phenomena that appear paradoxical but remain consistent with quantum mechanical principles. These findings provide deeper understanding of waveguide-emitter interactions and enable novel quantum information processing applications, such as non-destructive photon number parity measurements.

Abstract: We theoretically investigate the optical dipole interaction between a multi-level quantum system and a single-mode optical waveguide of any local polarisation. We investigate several paradoxical seeming situations, for example we find a situation in which there exist two non-orthogonal quantum states, each of which results in a photon flux in the opposite direction to the other. We show how, despite appearances, this does not break the unitary requirements of quantum mechanics. We also find that an isotropic quantum emitter can be either reflective or transmissive to light depending on the waveguide polarisation at the emitter location, indeed in the zero loss limit such a system changes from 100% transmission to 100% reflection due to an infinitesimal polarisation rotation. An example case for a four level system is also considered, which is found to operate as a non-destructive parity measurement of the photon number.

</details>


### [14] [Time-Dynamic Circuits for Fault-Tolerant Shift Automorphisms in Quantum LDPC Codes](https://arxiv.org/abs/2601.09911)
*Younghun Kim,Spiro Gicev,Martin Sevior,Muhammad Usman*

Main category: quant-ph

TL;DR: Dynamic syndrome measurement circuits enable shift automorphisms in qLDPC codes without distance reduction, achieving logical error rates comparable to idle operations and orders of magnitude improvement over SWAP-based approaches.


<details>
  <summary>Details</summary>
Motivation: Shift automorphisms are essential for universal gate sets in qLDPC codes, but existing SWAP-based implementations suffer from high logical error rates that are orders of magnitude worse than fault-tolerant idle operations, creating a practical bottleneck for implementing logical operations.

Method: Developed time-dynamic syndrome measurement circuits that vary dynamically to implement shift automorphisms without reducing circuit distance. Benchmarked on twisted and untwisted weight-6 generalized toric codes (including gross code family) using circuit-level noise model (SI1000) and BP-OSD decoder.

Result: Dynamic circuits achieve logical error rates comparable to idle operations, with more than an order of magnitude reduction relative to SWAP-based scheme for gross code at physical error rate 10^-3. Performance improvements apply to both twisted and untwisted weight-6 generalized toric codes.

Conclusion: Dynamic circuit approach significantly improves both error resilience and time overhead of shift automorphisms in qLDPC codes, providing a practical pathway for implementing logical operations and enabling alternative syndrome extraction circuit designs like leakage removal protocols.

Abstract: Quantum low-density parity-check (qLDPC) codes have emerged as a promising approach for realizing low-overhead logical quantum memories. Recent theoretical developments have established shift automorphisms as a fundamental building block for completing the universal set of logical gates for qLDPC codes. However, practical challenges remain because the existing SWAP-based shift automorphism yields logical error rates that are orders of magnitude higher than those for fault-tolerant idle operations. In this work, we address this issue by dynamically varying the syndrome measurement circuits to implement the shift automorphisms without reducing the circuit distance. We benchmark our approach on both twisted and untwisted weight-6 generalized toric codes, including the gross code family. Our time-dynamic circuits for shift automorphisms achieve performance comparable to the idle operations under the circuit-level noise model (SI1000). Specifically, the dynamic circuits achieve more than an order of magnitude reduction in logical error rates relative to the SWAP-based scheme for the gross code at a physical error rate of $10^{-3}$, employing the BP-OSD decoder. Our findings improve both the error resilience and the time overhead of the shift automorphisms in qLDPC codes. Furthermore, our work can lead to alternative syndrome extraction circuit designs, such as leakage removal protocols, providing a practical pathway to utilizing dynamic circuits that extend beyond surface codes towards qLDPC codes.

</details>


### [15] [Learning to Decode in Parallel: Self-Coordinating Neural Network for Real-Time Quantum Error Correction](https://arxiv.org/abs/2601.09921)
*Kai Zhang,Zhengzhong Yi,Shaojun Guo,Linghang Kong,Situ Wang,Xiaoyu Zhan,Tan He,Weiping Lin,Tao Jiang,Dongxin Gao,Yiming Zhang,Fangming Liu,Fang Zhang,Zhengfeng Ji,Fusheng Chen,Jianxin Chen*

Main category: quant-ph

TL;DR: A neural network decoder for quantum error correction that enables real-time parallel decoding of surface codes, overcoming throughput bottlenecks while maintaining state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing neural network decoders like AlphaQubit lack the parallelism needed for real-time decoding of syndrome streams from superconducting logical qubits, and cannot be easily integrated with sliding window-based parallel decoding schemes due to their training for global logical corrections rather than local physical corrections.

Method: Train a recurrent, transformer-based neural network specifically for parallel window decoding. While still outputting a single bit, derive training labels from consistent local corrections and train on various decoding window types simultaneously, enabling self-coordination across neighboring windows.

Result: Overcomes throughput bottleneck preventing AlphaQubit-type decoders in FTQC; achieves SOTA accuracy with required throughput for real-time QEC; benchmarks on Zuchongzhi 3.2 processor with surface codes up to distance 7 show superior accuracy; single TPU v6e can decode surface codes up to distance 25 within 1μs per round.

Conclusion: Presents the first scalable neural-network-based parallel decoding framework that simultaneously achieves state-of-the-art accuracy and the stringent throughput required for real-time quantum error correction, enabling practical fault-tolerant quantum computation.

Abstract: Fast, reliable decoders are pivotal components for enabling fault-tolerant quantum computation (FTQC). Neural network decoders like AlphaQubit have demonstrated potential, achieving higher accuracy than traditional human-designed decoding algorithms. However, existing implementations of neural network decoders lack the parallelism required to decode the syndrome stream generated by a superconducting logical qubit in real time. Moreover, integrating AlphaQubit with sliding window-based parallel decoding schemes presents non-trivial challenges: AlphaQubit is trained solely to output a single bit corresponding to the global logical correction for an entire memory experiment, rather than local physical corrections that can be easily integrated. We address this issue by training a recurrent, transformer-based neural network specifically tailored for parallel window decoding. While it still outputs a single bit, we derive training labels from a consistent set of local corrections and train on various types of decoding windows simultaneously. This approach enables the network to self-coordinate across neighboring windows, facilitating high-accuracy parallel decoding of arbitrarily long memory experiments.
  As a result, we overcome the throughput bottleneck that previously precluded the use of AlphaQubit-type decoders in FTQC. Our work presents the first scalable, neural-network-based parallel decoding framework that simultaneously achieves SOTA accuracy and the stringent throughput required for real-time quantum error correction. Using an end-to-end experimental workflow, we benchmark our decoder on the Zuchongzhi 3.2 superconducting quantum processor on surface codes with distances up to 7, demonstrating its superior accuracy. Moreover, we demonstrate that, using our approach, a single TPU v6e is capable of decoding surface codes with distances up to 25 within 1us per decoding round.

</details>


### [16] [Beyond Optimization: Harnessing Quantum Annealer Dynamics for Machine Learning](https://arxiv.org/abs/2601.09938)
*Akitada Sakurai,Aoi Hayashi,Tadayoshi Matumori,Daisuke Kaji,Tadashi Kadowaki,Kae Nemoto*

Main category: quant-ph

TL;DR: Quantum annealing used for machine learning by encoding data into Ising Hamiltonians, evolving on quantum annealer, and using resulting probability distributions as feature maps for classification.


<details>
  <summary>Details</summary>
Motivation: Quantum annealing is typically viewed as a combinatorial optimization tool, but its coherent dynamics offer potential for machine learning applications beyond optimization.

Method: Encode classical data into Ising Hamiltonian, evolve it on a quantum annealer, use resulting probability distributions as feature maps for classification. Experiments on quantum annealer with Digits dataset and simulations on MNIST.

Result: Short annealing times yield higher classification accuracy, while longer times reduce accuracy but lower sampling costs. Participation ratio introduced as measure of effective model size and shows strong correlation with generalization.

Conclusion: Quantum annealing can be effectively used for machine learning tasks beyond optimization, with annealing time trade-offs between accuracy and sampling efficiency, and participation ratio serves as useful metric for generalization performance.

Abstract: Quantum annealing is typically regarded as a tool for combinatorial optimization, but its coherent dynamics also offer potential for machine learning. We present a model that encodes classical data into an Ising Hamiltonian, evolves it on a quantum annealer, and uses the resulting probability distributions as feature maps for classification. Experiments on the quantum annealer machine with the Digits dataset, together with simulations on MNIST, demonstrate that short annealing times yield higher classification accuracy, while longer times reduce accuracy but lower sampling costs. We introduce the participation ratio as a measure of the effective model size and show its strong correlation with generalization.

</details>


### [17] [Three Months in the Life of Cloud Quantum Computing](https://arxiv.org/abs/2601.09943)
*Darrell Teegarden,Allison Casey,F. Gino Serpa,Patrick Becker,Asmita Brahme,Saanvi Kataria,Paul Lopata*

Main category: quant-ph

TL;DR: Systematic analysis of cloud quantum computing environments over three months, documenting connection metrics, algorithm execution, qubit scaling effects, simulation comparisons, execution times, and costs using out-of-the-box settings across multiple platforms.


<details>
  <summary>Details</summary>
Motivation: Quantum computing has transitioned from custom hardware to commercial cloud-accessible systems requiring complex toolchains. Understanding the trade-offs in these evolving environments is essential for evaluating quantum computing's practical utility, but systematic empirical data on real-world usage is lacking.

Method: Three-month systematic investigation using out-of-the-box quantum programming environments across multiple cloud platforms and machines. Executed a single algorithm consistently across different systems, collecting metadata on connection metrics, algorithm execution, qubit scaling effects, simulation comparisons, execution times, and costs without algorithm optimization or platform-specific tuning.

Result: Generated comprehensive metadata from quantum computing environments including connection metrics to various services, algorithm execution data, effects of varying qubit counts, comparisons to simulations, execution time measurements, and cost analyses across different platforms and time periods.

Conclusion: Provides concrete empirical data and insights for quantum computing exploration, offering a consistent baseline comparison across platforms using standard settings. The work focuses on documenting the current state of cloud quantum computing accessibility and performance rather than algorithmic innovation or platform-specific optimization.

Abstract: Quantum Computing (QC) has evolved from a few custom quantum computers, which were only accessible to their creators, to an array of commercial quantum computers that can be accessed on the cloud by anyone. Accessing these cloud quantum computers requires a complex chain of tools that facilitate connecting, programming, simulating algorithms, estimating resources, submitting quantum computing jobs, retrieving results, and more. Some steps in the chain are hardware dependent and subject to change as both hardware and software tools, such as available gate sets and optimizing compilers, evolve. Understanding the trade-offs inherent in this process is essential for evaluating the power and utility of quantum computers. ARLIS has been systematically investigating these environments to understand these complexities. The work presented here is a detailed summary of three months of using such quantum programming environments. We show metadata obtained from these environments, including the connection metrics to the different services, the execution of algorithms, the testing of the effects of varying the number of qubits, comparisons to simulations, execution times, and cost. Our objective is to provide concrete data and insights for those who are exploring the potential of quantum computing. It is not our objective to present any new algorithms or optimize performance on any particular machine or cloud platform; rather, this work is focused on providing a consistent view of a single algorithm executed using out-of-the-box settings and tools across machines, cloud platforms, and time. We present insights only available from these carefully curated data.

</details>


### [18] [Parallelizing the Variational Quantum Eigensolver: From JIT Compilation to Multi-GPU Scaling](https://arxiv.org/abs/2601.09951)
*Rylan Malarchick,Ashton Steed*

Main category: quant-ph

TL;DR: VQE implementation for H₂ potential energy surface achieves 117× speedup through multi-level optimization on HPC cluster with 4× NVIDIA H100 GPUs, reducing runtime from ~10 minutes to 5 seconds.


<details>
  <summary>Details</summary>
Motivation: To accelerate variational quantum eigensolver (VQE) calculations for quantum chemistry applications by leveraging HPC resources and GPU acceleration, enabling interactive exploration of molecular systems.

Method: Implemented VQE for H₂ molecule across 100 bond lengths using PennyLane framework on HPC cluster with 4× NVIDIA H100 GPUs. Applied four-phase optimization: (1) Optimizer + JIT compilation, (2) GPU device acceleration, (3) MPI parallelization, and (4) Multi-GPU scaling.

Result: Achieved 117× total speedup (593.95s → 5.04s) for H₂ potential energy surface. Individual speedups: 4.13× from optimizer+JIT, 3.60-80.5× from GPU acceleration (4-26 qubits), 28.5× from MPI, and 3.98× from multi-GPU with 99.4% parallel efficiency. Single H100 can simulate up to 29 qubits before memory limits.

Conclusion: Multi-level parallelization on HPC clusters with modern GPUs dramatically accelerates VQE calculations, enabling interactive quantum chemistry exploration and establishing GPU advantage across all qubit scales studied (4-26 qubits).

Abstract: The Variational Quantum Eigensolver (VQE) is a hybrid quantum-classical algorithm for computing ground state energies of molecular systems. We implement VQE to calculate the potential energy surface of the hydrogen molecule (H$_2$) across 100 bond lengths using the PennyLane quantum computing framework on an HPC cluster featuring 4$\times$ NVIDIA H100 GPUs (80GB each). We present a comprehensive parallelization study with four phases: (1) Optimizer + JIT compilation achieving 4.13$\times$ speedup, (2) GPU device acceleration achieving 3.60$\times$ speedup at 4 qubits scaling to 80.5$\times$ at 26 qubits, (3) MPI parallelization achieving 28.5$\times$ speedup, and (4) Multi-GPU scaling achieving 3.98$\times$ speedup with 99.4% parallel efficiency across 4 H100 GPUs. The combined effect yields 117$\times$ total speedup for the H$_2$ potential energy surface (593.95s $\rightarrow$ 5.04s). We conduct a CPU vs GPU scaling study from 4--26 qubits, finding GPU advantage at all scales with speedups ranging from 10.5$\times$ to 80.5$\times$. Multi-GPU benchmarks demonstrate near-perfect scaling with 99.4% efficiency and establish that a single H100 can simulate up to 29 qubits before hitting memory limits. The optimized implementation reduces runtime from nearly 10 minutes to 5 seconds, enabling interactive quantum chemistry exploration.

</details>


### [19] [Statistical-noise-enhanced multi-photon interference](https://arxiv.org/abs/2601.09977)
*Rikizo Ikuta*

Main category: quant-ph

TL;DR: Three-photon interference in symmetric circuits shows non-monotonic visibility dependence on intensity correlations, unlike two-photon HOM interference. Engineered super-Poissonian statistics can maximize visibility, surpassing single-photon signatures, revealing statistical complementarity between quantum and classical advantages.


<details>
  <summary>Details</summary>
Motivation: The paper investigates how photon statistics governs multi-photon interference. While two-photon Hong-Ou-Mandel interference shows monotonic visibility degradation with higher intensity correlations, the authors explore whether this monotonicity holds for three-photon interference in symmetric circuits, aiming to understand fundamental statistical relationships in multi-photon interference.

Method: The study uses symmetric circuits, specifically the discrete Fourier transform circuit, to analyze three-photon interference. Engineered super-Poissonian photon-number fluctuations are created using a modulated laser to manipulate photon statistics. The researchers tune symmetric circuit parameters to examine visibility behavior relative to Poissonian statistics benchmarks.

Result: The key findings are: 1) Three-photon interference in symmetric circuits does not follow the monotonic visibility degradation seen in two-photon HOM interference; 2) Engineered super-Poissonian photon-number fluctuations can maximize interference visibility, even surpassing the magnitude of single-photon signatures; 3) By tuning circuit parameters, the visibility hierarchy inverts relative to Poissonian statistics benchmarks; 4) This reveals a trade-off where quantum and classical advantages are mutually exclusive resources for interference, indicating statistical complementarity.

Conclusion: The research demonstrates that photon statistics plays a more complex role in three-photon interference than in two-photon cases, revealing non-monotonic relationships and the possibility of optimizing interference through engineered statistics. The trade-off between quantum and classical advantages suggests a form of statistical complementarity that governs multi-photon interference phenomena in symmetric circuits.

Abstract: Photon statistics plays a governing role in multi-photon interference. While interference visibility in the standard two-photon case, known as Hong-Ou-Mandel interference, monotonically degrades with higher intensity correlation functions, we show that this monotonicity does not hold for three-photon interference in symmetric circuits. We reveal that, in the discrete Fourier transform circuit, engineered super-Poissonian photon-number fluctuations, realized using a modulated laser, maximize the visibility, surpassing the magnitude of the single-photon signature. In addition, by tuning the symmetric circuit parameters, we demonstrate that the visibility hierarchy inverts relative to the benchmark of Poissonian statistics. This trade-off implies that quantum and classical advantages are mutually exclusive resources for interference, indicating a form of statistical complementarity.

</details>


### [20] [Double Markovity for quantum systems](https://arxiv.org/abs/2601.09995)
*Masahito Hayashi,Jinpei Zhao*

Main category: quant-ph

TL;DR: Quantum analogues of double Markovity are established, enabling extension of SDR technique to quantum systems by characterizing simultaneous Markov conditions for tripartite and four-party states.


<details>
  <summary>Details</summary>
Motivation: The SDR technique is powerful for Gaussian optimality in classical information theory but relies on double Markovity. Extending SDR-type arguments to quantum systems requires establishing quantum analogues of double Markovity, which has been a key bottleneck.

Method: For tripartite states, characterize simultaneous Markov conditions A-B-C and A-C-B via compatible projective measurements on B and C that induce a common classical label J yielding A-J-(BC). For strictly positive four-party states, show that A-(BD)-C and A-(CD)-B hold if and only if A-D-(BC) holds.

Result: Established quantum analogues of double Markovity: (1) For tripartite states, simultaneous Markov conditions are equivalent to existence of compatible projective measurements inducing a common classical label; (2) For four-party states, two conditional Markov chains are equivalent to a single Markov chain through the fourth party.

Conclusion: These results remove the key bottleneck in extending SDR-type arguments to quantum systems by providing the necessary quantum analogues of double Markovity, enabling application of the powerful SDR technique to quantum information theory problems.

Abstract: The subadditivity-doubling-rotation (SDR) technique is a powerful route to Gaussian optimality in classical information theory and relies on strict subadditivity and its equality-case analysis, where double Markovity is a standard tool. We establish quantum analogues of double Markovity. For tripartite states, we characterize the simultaneous Markov conditions A-B-C and A-C-B via compatible projective measurements on B and C that induce a common classical label J yielding A-J-(BC). For strictly positive four-party states, we show that A-(BD)-C and A-(CD)-B hold if and only if A-D-(BC) holds. These results remove a key bottleneck in extending SDR-type arguments to quantum systems.

</details>


### [21] [Reentrant topological phases and entanglement scalings in moiré-modulated extended Su-Schrieffer-Heeger Model](https://arxiv.org/abs/2601.09997)
*Guo-Qing Zhang,L. F. Quezada,Shi-Hai Dong*

Main category: quant-ph

TL;DR: The paper studies reentrant phase transitions in moiré-modulated 1D SSH models, analyzing universality classes, bulk-boundary correspondence, and entanglement properties.


<details>
  <summary>Details</summary>
Motivation: While moiré physics offers opportunities for quantum phase transitions, reentrant phase transitions driven by moiré strength are poorly understood, particularly their universal characteristics and bulk-boundary correspondence in 1D systems.

Method: For the simplified case (w=0), analytical derivation of renormalization relations; for general cases, numerical calculation of phase boundaries in thermodynamic limit; analysis of zero-energy edge modes, entanglement spectrum degeneracy, and correspondence between central charge from entanglement entropy and winding number changes.

Result: Analytical explanation of reentrant phenomenon via renormalization relations, numerical phase boundaries, revelation of bulk-boundary correspondence through degeneracy of zero-energy edge modes and entanglement spectrum, and establishment of correspondence between central charge and winding number changes during phase transitions.

Conclusion: The results provide insights into universal characteristics and bulk-boundary correspondence for moiré-induced reentrant phase transitions in 1D condensed-matter systems, advancing understanding of moiré physics in quantum phase transitions.

Abstract: Recent studies of moiré physics have unveiled a wealth of opportunities for significantly advancing the field of quantum phase transitions. However, properties of reentrant phase transitions driven by moiré strength are poorly understood. Here, we investigate the reentrant sequence of phase transitions and the invariant of universality class in moiré-modulated extended Su-Schrieffer-Heeger (SSH) model. For the simplified case with intercell hopping $w=0$, we analytically derive renormalization relations of Hamiltonian parameters to explain the reentrant phenomenon. For the general case, numerical phase boundaries are calculated in the thermodynamic limit. The bulk boundary correspondence between zero-energy edge modes and entanglement spectrum is revealed from the degeneracy of both quantities. We also address the correspondence between the central charge obtained from entanglement entropy and the change in winding number during the phase transition. Our results shed light on the understanding of universal characteristics and bulk-boundary correspondence for moiré induced reentrant phase transitions in 1D condensed-matter systems.

</details>


### [22] [Contextuality Derived from Minimal Decision Dynamics: Quantum Tug-of-War Decision Making](https://arxiv.org/abs/2601.10034)
*Song-Ju Kim*

Main category: quant-ph

TL;DR: Quantum probability emerges as necessary for modeling context-dependent decision making, not just as convenient assumption, through conservation-based learning dynamics.


<details>
  <summary>Details</summary>
Motivation: To determine whether quantum probability in cognition is merely a convenient modeling assumption or a necessary consequence of decision dynamics, addressing the fundamental question of why quantum formalisms work in cognitive modeling.

Method: Developed a quantum extension of the Tug-of-War (TOW) model, incorporating conservation-based internal state updates and measurement-induced disturbance, then analyzed whether this system admits non-contextual classical descriptions with unified internal states.

Result: The framework shows that conservation constraints and measurement disturbance preclude any non-contextual classical description with a single unified internal state, making contextuality a structural consequence of adaptive learning dynamics. The system exhibits KCBS-type contextuality witnesses in minimal single-system settings.

Conclusion: Quantum probability is not merely a descriptive convenience but an unavoidable effective theory for adaptive decision dynamics, emerging naturally from physically grounded constraints on decision making.

Abstract: Decision making often exhibits context dependence that challenges classical probability theory. While quantum cognition has successfully modeled such phenomena, it remains unclear whether quantum probability is merely a convenient assumption or a necessary consequence of decision dynamics. Here we present a theoretical framework in which contextuality arises generatively from physically grounded constraints on decision making. By developing a quantum extension of the Tug-of-War (TOW) model, we show that conservation-based internal state updates and measurement-induced disturbance preclude any non-contextual classical description with a single, unified internal state. Contextuality therefore emerges as a structural consequence of adaptive learning dynamics. We further show that the resulting measurement structure admits Klyachko-Can-Binicioglu-Shumovsky (KCBS)-type contextuality witnesses in a minimal single-system setting. These results indicate that quantum probability is not merely a descriptive convenience, but an unavoidable effective theory for adaptive decision dynamics.

</details>


### [23] [Towards Minimal Fault-tolerant Error-Correction Sequence with Quantum Hamming Codes](https://arxiv.org/abs/2601.10042)
*Sha Shi,Xiao-Yang Xu,Min-Quan Cheng,Dong-Sheng Wang,Yun-Jiang Wang*

Main category: quant-ph

TL;DR: Constructs efficient fault-tolerant measurement sequences for quantum Hamming codes with length reduced to exactly 2r+1, achieving minimal overhead through cyclic matrix transformations and hardware-efficient circuit reuse.


<details>
  <summary>Details</summary>
Motivation: The high overhead of fault-tolerant measurement sequences (FTMSs) poses a major challenge for implementing quantum stabilizer codes, particularly for quantum Hamming codes, creating a need for more efficient designs.

Method: Uses cyclic matrix transformations to systematically combine rows of the initial stabilizer matrix while preserving a self-dual CSS-like symmetry. This symmetry enables hardware-efficient circuit reuse where measurement circuits for the first r stabilizers are transformed into circuits for the remaining r stabilizers by toggling boundary Hadamard gates.

Result: Demonstrates that sequence length can be reduced to exactly 2r+1 - only one additional measurement beyond the original non-fault-tolerant sequence, establishing a tight lower bound. The approach simultaneously reduces time overhead via shortened FTMS length and hardware overhead through symmetry-enabled circuit multiplexing.

Conclusion: Provides an important advance toward designing minimal FTMSs for quantum Hamming codes and may shed light on similar challenges in other quantum stabilizer codes, offering both time and hardware efficiency improvements for distance-3 fault-tolerant error correction.

Abstract: The high overhead of fault-tolerant measurement sequences (FTMSs) poses a major challenge for implementing quantum stabilizer codes. Here, we address this problem by constructing efficient FTMSs for the class of quantum Hamming codes $[\![2^r-1, 2^r-1-2r, 3]\!]$ with $r=3k+1$ ($k \in \mathbb{Z}^+$). Our key result demonstrates that the sequence length can be reduced to exactly $2r+1$-only one additional measurement beyond the original non-fault-tolerant sequence, establishing a tight lower bound. The proposed method leverages cyclic matrix transformations to systematically combine rows of the initial stabilizer matrix and preserving a self-dual CSS-like symmetry analogous to that of the original quantum Hamming codes. This induced symmetry enables hardware-efficient circuit reuse: the measurement circuits for the first $r$ stabilizers are transformed into circuits for the remaining $r$ stabilizers simply by toggling boundary Hadamard gates, eliminating redundant hardware. For distance-3 fault-tolerant error correction, our approach simultaneously reduces the time overhead via shorting the FTMS length and the hardware overhead through symmetry-enabled circuit multiplexing. These results provide an important advance towards the important open problem regarding the design of minimal FTMSs for quantum Hamming codes and may shed light on similar challenges in other quantum stabilizer codes.

</details>


### [24] [Optimal qudit overlapping tomography and optimal measurement order](https://arxiv.org/abs/2601.10059)
*Shuowei Ma,Qianfan Wang,Lvzhou Li,Fei Shi*

Main category: quant-ph

TL;DR: Optimal overlapping tomography for qudit systems using generalized Gell-Mann matrices and combinatorial covering arrays, with explicit constructions for qutrits and efficient scheduling to minimize experimental switching overhead.


<details>
  <summary>Details</summary>
Motivation: Quantum state tomography becomes infeasible for large systems due to exponential scaling. While overlapping tomography addresses this for qubits, extension to higher-dimensional qudit systems remains unexplored, limiting efficient characterization of qudit-based quantum systems.

Method: Construct local measurement settings from generalized Gell-Mann matrices, establish correspondence with combinatorial covering arrays, present two explicit constructions of optimal measurement schemes, and develop efficient algorithm to optimize measurement ordering to minimize switching overhead.

Result: For n-qutrit systems, pairwise tomography requires at most 8 + 56⌈log₈ n⌉ measurement settings with explicit scheme achieving this bound. Optimized scheduling reduces switching costs by ~50% compared to worst-case ordering.

Conclusion: Provides practical pathway for efficient characterization of qudit systems using optimal overlapping tomography, facilitating applications in quantum communication and computation by overcoming exponential scaling limitations.

Abstract: Quantum state tomography is essential for characterizing quantum systems, but it becomes infeasible for large systems due to exponential resource scaling. Overlapping tomography addresses this challenge by reconstructing all $k$-body marginals using few measurement settings, enabling the efficient extraction of key information for many quantum tasks. While optimal schemes are known for qubits, the extension to higher-dimensional qudit systems remains largely unexplored. Here, we investigate optimal qudit overlapping tomography, constructing local measurement settings from generalized Gell-Mann matrices. By establishing a correspondence with combinatorial covering arrays, we present two explicit constructions of optimal measurement schemes. For $n$-qutrit systems, we prove that pairwise tomography requires at most $8 + 56\left\lceil \log_{8} n \right\rceil$ measurement settings, and provide an explicit scheme achieving this bound. Furthermore, we develop an efficient algorithm to determine the optimal order of these measurement settings, minimizing the experimental overhead associated with switching configurations. Compared to the worst-case ordering, our optimized schedule reduces switching costs by approximately 50\%. These results provide a practical pathway for efficient characterization of qudit systems, facilitating their application in quantum communication and computation.

</details>


### [25] [Geometric Criteria for Complete Mode Conversion in Detuned Systems via Piecewise-Coherent Modulation](https://arxiv.org/abs/2601.10066)
*Awanish Pandey*

Main category: quant-ph

TL;DR: Geometric Bloch-sphere framework enables complete mode conversion in detuned systems via piecewise-coherent modulation, breaking time-reversal symmetry for optical isolation and establishing universal bounds on switching events.


<details>
  <summary>Details</summary>
Motivation: Static phase detuning fundamentally limits coherent state transfer in asymmetric classical and quantum systems, creating a need for control strategies that overcome this constraint.

Method: Introduces a Bloch-sphere formulation for piecewise-coherent modulation that recasts coupled-mode dynamics as geometric trajectories, transforming algebraic control into path optimization. The approach reveals a cone of inaccessibility at the target pole and yields exact geodesic criteria for complete mode conversion.

Result: The framework enables breaking time-reversal symmetry to realize a magnet-free optical isolator with near-unity contrast. For detuning larger than coupling, a recursive multi-step protocol enables deterministic transfer for arbitrary detunings, with derivation of a universal geometric lower bound on required coupling-switching events.

Conclusion: Geometric control via Bloch-sphere trajectories provides a powerful framework for overcoming detuning constraints in coupled-mode systems, enabling complete state transfer and nonreciprocal devices with fundamental performance bounds.

Abstract: Static phase detuning fundamentally constrains coherent state transfer in asymmetric classical and quantum systems. We introduce a Bloch-sphere formulation for piecewise-coherent modulation that recasts coupled-mode dynamics as geometric trajectories, transforming algebraic control into path optimization. The approach reveals a cone of inaccessibility at the target pole and yields exact geodesic criteria for complete mode conversion in detuned systems. Leveraging this framework, we break time-reversal symmetry to realize a magnet-free optical isolator with near-unity contrast. Furthermore, for detuning larger than coupling between modes, we develop a recursive multi-step protocol enabling deterministic transfer for arbitrary detunings and derive a universal geometric lower bound on the required number of coupling-switching events.

</details>


### [26] [Pseudomode approach to Fano effect in dissipative cavity quantum electrodynamics](https://arxiv.org/abs/2601.10087)
*Kazuki Kobayashi,Tatsuro Yuge*

Main category: quant-ph

TL;DR: The paper establishes a unified framework for the Fano effect in dissipative cavity QED, showing how Fano interference emerges from system-environment interactions and clarifying its non-Markovian origin through spectral function analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the Fano effect in dissipative cavity quantum electrodynamics systems, particularly how Fano interference between direct radiation and cavity-mediated radiation emerges from system-environment interactions and its non-Markovian characteristics.

Method: 1) Derive quantum master equation using Born-Markov approximation for two-level system coupled to structured reservoir; 2) Use pseudomode approach with single auxiliary mode; 3) Identify spectral function of system-environment interaction; 4) Apply Fano diagonalization to common-environment setup with explicit cavity mode in strongest-interference regime.

Result: The spectral function consists of constant and non-Lorentzian contributions forming Fano profile; constant term is essential for Lindblad master equation and directly related to Fano interference rate; same spectral function derived independently via Fano diagonalization in strongest-interference regime.

Conclusion: Establishes unified framework for describing Fano effect in single-mode cavity QED systems and clarifies its non-Markovian origin encoded in the spectral function, connecting pseudomode approach with Fano diagonalization methodology.

Abstract: We study the Fano effect in dissipative cavity quantum electrodynamics, which originates from the interference between the emitter's direct radiation and that mediated by a cavity mode. Starting from a two-level system coupled to a structured reservoir, we show that a quantum master equation previously derived within the Born-Markov approximation can be rederived by introducing a single auxiliary mode via pseudomode approach. We identify the corresponding spectral function of the system--environment interaction and demonstrate that it consists of a constant and a non-Lorentzian contribution forming the Fano profile. The constant term is shown to be essential for obtaining a Lindblad master equation and is directly related to the rate associated with this Fano interference. Furthermore, by applying Fano diagonalization to a common-environment setup including an explicit cavity mode, we independently derive the same spectral function in the strongest-interference regime. Our results establish a unified framework for describing the Fano effect in single-mode cavity QED systems and clarify its non-Markovian origin encoded in the spectral function.

</details>


### [27] [Classical simulation of a quantum circuit with noisy magic inputs](https://arxiv.org/abs/2601.10111)
*Jiwon Heo,Sojeong Park,Changhun Oh*

Main category: quant-ph

TL;DR: Noisy magic states transition quantum circuits from classically intractable to efficiently simulable; explicit noise thresholds determine when polynomial-time classical simulation becomes possible.


<details>
  <summary>Details</summary>
Motivation: Magic states enable universal quantum computation but are inevitably noisy in realistic devices; understanding how this noise affects classical simulability reveals when quantum advantage is lost.

Method: Resource-centric noise model where only injected magic components are noisy; development of approximate classical sampling algorithm with controlled error; analysis of explicit noise-dependent conditions for polynomial-time simulation; framework applies to qubit circuits with Clifford baselines and fermionic circuits with matchgate baselines.

Result: Derived explicit noise thresholds determining transition from classically intractable to efficiently simulable behavior; provided numerical estimates of simulation cost, concrete thresholds, and runtime scaling across practical parameter regimes; algorithm works for representative noise channels like dephasing and particle loss.

Conclusion: Noise on magic resources fundamentally changes classical simulability of quantum circuits; explicit noise thresholds exist where quantum advantage disappears and efficient classical simulation becomes possible, providing practical benchmarks for quantum advantage experiments.

Abstract: Magic states are essential for universal quantum computation and are widely viewed as a key source of quantum advantage, yet in realistic devices they are inevitably noisy. In this work, we characterize how noise on injected magic resources changes the classical simulability of quantum circuits and when it induces a transition from classically intractable behavior to efficient classical simulation. We adopt a resource-centric noise model in which only the injected magic components are noisy, while the baseline states, operations, and measurements belong to an efficiently simulable family. Within this setting, we develop an approximate classical sampling algorithm with controlled error and prove explicit noise-dependent conditions under which the algorithm runs in polynomial time. Our framework applies to both qubit circuits with Clifford baselines and fermionic circuits with matchgate baselines, covering representative noise channels such as dephasing and particle loss. We complement the analysis with numerical estimates of the simulation cost, providing concrete thresholds and runtime scaling across practically relevant parameter regimes.

</details>


### [28] [Casimir interactions as a probe of broadband optical response](https://arxiv.org/abs/2601.10118)
*Calum F. Shelden,Jeremy N. Munday*

Main category: quant-ph

TL;DR: Machine learning inversion of Lifshitz theory enables broadband optical characterization from Casimir force measurements, transforming Casimir interactions into a spectroscopic tool.


<details>
  <summary>Details</summary>
Motivation: The connection between Casimir forces and real-frequency optical properties has been obscured by Lifshitz theory's formulation in terms of imaginary frequencies, limiting Casimir interactions as a materials probe.

Method: Using supervised machine learning to invert Lifshitz theory, determining complex permittivity from a single force-distance curve, with measurements at different separations constraining distinct frequency ranges.

Result: Successfully reconstructed a material's broadband optical response over more than seven orders of magnitude in frequency from Casimir force measurements, demonstrating how quantum fluctuations sample different electromagnetic spectrum regions.

Conclusion: Establishes Casimir interactions as a physically constrained, broadband spectroscopic tool that enables optical characterization in regimes inaccessible to conventional techniques.

Abstract: Casimir forces arise from quantum electromagnetic fluctuations and depend on the dielectric response of interacting materials across the entire frequency spectrum. Although this dependence is central to Lifshitz theory of the Casimir effect, the formulation of the force in terms of dielectric functions evaluated at imaginary frequencies has largely obscured its connection to real-frequency optical properties, limiting the use of Casimir interactions as a probe of materials. Here we demonstrate that Casimir force measurements encode sufficient information to reconstruct a material's broadband optical response. Using supervised machine learning to invert Lifshitz theory, we determine the complex permittivity of a material over more than seven orders of magnitude in frequency from a single force-distance curve. We show that measurements at different separations selectively constrain distinct frequency ranges of the dielectric response, providing direct physical insight into how quantum fluctuations sample the electromagnetic spectrum. These results establish Casimir interactions as a physically constrained, broadband spectroscopic tool and open new opportunities for optical characterization in regimes inaccessible to conventional techniques.

</details>


### [29] [Bridging Superconducting and Neutral-Atom Platforms for Efficient Fault-Tolerant Quantum Architectures](https://arxiv.org/abs/2601.10144)
*Xiang Fang,Jixuan Ruan,Sharanya Prabhu,Ang Li,Travis Humble,Dean Tullsen,Yufei Ding*

Main category: quant-ph

TL;DR: Heterogeneous quantum architectures combining superconducting and neutral atom platforms achieve 752× speedup over neutral atom-only systems and 10× physical qubit reduction versus superconducting-only systems.


<details>
  <summary>Details</summary>
Motivation: Homogeneous quantum systems have limitations where no single qubit modality simultaneously offers optimal operation speed, connectivity, and scalability. The transition to fault-tolerant quantum computing exposes these limitations, necessitating strategic approaches that leverage complementary strengths of different hardware platforms.

Method: Proposes Heterogeneous Quantum Architectures (HQA) synthesizing superconducting (SC) and neutral atom (NA) platforms. Two architectural strategies: (1) MagicAcc - offloads latency-critical Magic State Factory to fast SC devices while performing computation on scalable NA arrays; (2) Memory-Compute Separation (MCSep) - uses NA arrays for high-density qLDPC memory storage and SC devices for fast surface-code processing. Evaluation based on comprehensive end-to-end cost model.

Result: Principled heterogeneity yields significant performance gains: designs achieve 752× speedup over NA-only baselines on average and reduce physical qubit footprint by over 10× compared to SC-only systems.

Conclusion: Heterogeneous quantum architectures leveraging cross-modality interconnects provide a clear pathway for optimizing space-time efficiency of future fault-tolerant quantum computers by strategically combining complementary strengths of different qubit modalities.

Abstract: The transition to the fault-tolerant era exposes the limitations of homogeneous quantum systems, where no single qubit modality simultaneously offers optimal operation speed, connectivity, and scalability. In this work, we propose a strategic approach to Heterogeneous Quantum Architectures (HQA) that synthesizes the distinct advantages of the superconducting (SC) and neutral atom (NA) platforms. We explore two architectural role assignment strategies based on hardware characteristics: (1) We offload the latency-critical Magic State Factory (MSF) to fast SC devices while performing computation on scalable NA arrays, a design we term MagicAcc, which effectively mitigates the resource-preparation bottleneck. (2) We explore a Memory-Compute Separation (MCSep) paradigm that utilizes NA arrays for high-density qLDPC memory storage and SC devices for fast surface-code processing. Our evaluation, based on a comprehensive end-to-end cost model, demonstrates that principled heterogeneity yields significant performance gains. Specifically, our designs achieve $752\times$ speedup over NA-only baselines on average and reduce the physical qubit footprint by over $10\times$ compared to SC-only systems. These results chart a clear pathway for leveraging cross-modality interconnects to optimize the space-time efficiency of future fault-tolerant quantum computers.

</details>


### [30] [Fluctuation-induced quenching of chaos in quantum optics](https://arxiv.org/abs/2601.10147)
*Mei-Qi Gao,Song-hai Li,Xun Li,Xingli Li,Jiong Cheng,Wenlin Li*

Main category: quant-ph

TL;DR: Quantum chaos suppression by thermal fluctuations in optical systems: room-temperature noise prevents chaotic behavior in expectation values, with nonlinearity reducing the noise threshold needed for suppression.


<details>
  <summary>Details</summary>
Motivation: Mean-field approximations in quantum optics ignore fluctuations, but chaos is sensitive to initial conditions, questioning whether such approximations remain valid when accounting for quantum fluctuations.

Method: Analyze chaotic effects using stochastic Langevin equations and Lindblad master equation for systems at 10^5-10^7 Hz frequencies, examining room-temperature thermal fluctuations and nonlinearity effects.

Result: Room-temperature thermal fluctuations suppress chaos at expectation value level even with weak nonlinearity; nonlinearity induces non-Gaussian phase-space distributions with attractor-like Wigner functions; increasing nonlinearity lowers noise threshold for chaos suppression toward vacuum fluctuation scale.

Conclusion: Quantum mechanical suppression of chaos is validated bidirectionally: thermal fluctuations prevent chaos in expectation values, while nonlinearity reveals quantum signatures and reduces the noise threshold needed for suppression.

Abstract: Recent studies have extensively explored chaotic dynamics in quantum optical systems through the mean-field approximation, which corresponds to an ideal, fluctuation-free scenario. However, the inherent sensitivity of chaos to initial conditions implies that even minute fluctuations can be amplified, thereby questioning the applicability of this approximation. Here, we analyze these chaotic effects using stochastic Langevin equations or the Lindblad master equation. For systems operating at frequencies of $10^5$ to $10^7$ Hz, we demonstrate that room-temperature thermal fluctuations are sufficient to suppress chaos at the level of expectation values, even under weak nonlinearity. Furthermore, nonlinearity induces deviations from Gaussian phase-space distributions of the quantum state, revealing attractor-like features in the Wigner function. With increasing nonlinearity, the noise threshold for chaos suppression decreases, approaching the scale of vacuum fluctuations. These results provide a bidirectional validation of the quantum mechanical suppression of chaos.

</details>


### [31] [Computing Statistical Properties of Velocity Fields on Current Quantum Hardware](https://arxiv.org/abs/2601.10166)
*Miriam Goldack,Yosi Atia,Ori Alberton,Karl Jansen*

Main category: quant-ph

TL;DR: Quantum methods for extracting statistical properties of velocity fields in CFD without full state tomography, demonstrated on 1D cases with 16 spatial points using 4 qubits on IBMQ hardware.


<details>
  <summary>Details</summary>
Motivation: Quantum CFD offers favorable scaling but faces challenges in efficient readout of simulation results, which has received limited attention in literature. The need for methods to extract statistical properties without costly full quantum state tomography.

Method: Develop methods to extract statistical properties (central moments and structure functions) directly from parameterized ansatz circuits. Implement for 1D velocity fields encoding 16 spatial points with 4 qubits, analyzing sine wave signals and Burgers' equation snapshots. Use Qedma's error mitigation software QESEM on IBMQ's Heron2 system (ibm_fez).

Result: Demonstrated that such computations achieve high accuracy on current quantum devices (IBMQ's Heron2 system) using error mitigation techniques, showing practical feasibility of quantum CFD statistical analysis.

Conclusion: The presented methods enable efficient extraction of statistical properties from quantum CFD simulations without full state tomography, demonstrating practical implementation on current quantum hardware with error mitigation.

Abstract: Quantum algorithms are gaining attention in Computational Fluid Dynamics (CFD) for their favorable scaling, as encoding physical fields into quantum probability amplitudes enables representation of two to the power of n spatial points with only n qubits. A key challenge in Quantum CFD is the efficient readout of simulation results, a topic that has received limited attention in literature. This work presents methods to extract statistical properties of spatial velocity fields, such as central moments and structure functions, directly from parameterized ansatz circuits, avoiding full quantum state tomography. As a proof of concept, we implement our approach for 1D velocity fields, encoding 16 spatial points with 4 qubits, and analyze both a sine wave signal and four snapshots from Burgers' equation evolution. Using Qedma's error mitigation software QESEM, we demonstrate that such computations achieve high accuracy on current quantum devices, specifically IBMQ's Heron2 system ibm_fez.

</details>


### [32] [Exponential Analysis for Entanglement Distillation](https://arxiv.org/abs/2601.10190)
*Zhiwen Lin,Ke Li,Kun Fang*

Main category: quant-ph

TL;DR: The paper studies the reliability function of entanglement distillation, extending from known states to black-box settings, characterizing it via regularized quantum Hoeffding divergence, and analyzing various free operation classes.


<details>
  <summary>Details</summary>
Motivation: Traditional entanglement distillation focuses on distillable entanglement assuming complete state knowledge; this work aims to study the reliability function (error decay exponent) for rates below distillable entanglement and extend to more operational black-box settings where the initial state is unknown but belongs to a set.

Method: Establishes exact finite blocklength results connecting to composite correlated hypothesis testing without redundant correction terms. Characterizes the reliability function via regularized quantum Hoeffding divergence. Constructs optimal distillation protocols for known states and analyzes strong converse exponents. Investigates various free operation classes including non-entangling, PPT-preserving, dually non-entangling, and dually PPT-preserving operations.

Result: The reliability function of entanglement distillation is characterized by the regularized quantum Hoeffding divergence. For pure initial states, the result reduces to Hayashi et al.'s 2003 entanglement concentration error exponent. Optimal distillation protocols are constructed for known states, and strong converse exponents are analyzed. Results are extended across different free operation classes.

Conclusion: The paper provides a comprehensive characterization of entanglement distillation reliability functions, bridging finite blocklength analysis with operational black-box settings, and generalizing results across various resource-theoretic operation classes, offering both theoretical insights and practical protocol constructions.

Abstract: Historically, the focus in entanglement distillation has predominantly been on the distillable entanglement, and the framework assumes complete knowledge of the initial state. In this paper, we study the reliability function of entanglement distillation, which specifies the optimal exponent of the decay of the distillation error when the distillation rate is below the distillable entanglement. Furthermore, to capture greater operational significance, we extend the framework from the standard setting of known states to a black-box setting, where distillation is performed from a set of possible states. We establish an exact finite blocklength result connecting to composite correlated hypothesis testing without any redundant correction terms. Based on this, the reliability function of entanglement distillation is characterized by the regularized quantum Hoeffding divergence. In the special case of a pure initial state, our result reduces to the error exponent for entanglement concentration derived by Hayashi et al. in 2003. Given full prior knowledge of the state, we construct a concrete optimal distillation protocol. Additionally, we analyze the strong converse exponent of entanglement distillation. While all the above results assume the free operations to be non-entangling, we also investigate other free operation classes, including PPT-preserving, dually non-entangling, and dually PPT-preserving operations.

</details>


### [33] [Autonomous Quantum Simulation through Large Language Model Agents](https://arxiv.org/abs/2601.10194)
*Weitang Li,Jiajun Ren,Lixue Cheng,Cunxi Gong*

Main category: quant-ph

TL;DR: LLM agents can autonomously perform tensor network simulations of quantum many-body systems with ~90% success rate, using in-context learning and multi-agent decomposition to overcome the expertise barrier typically requiring years of graduate training.


<details>
  <summary>Details</summary>
Motivation: Tensor network methods are powerful for quantum simulation but require specialized expertise typically acquired through years of graduate training, creating a significant barrier to their effective use.

Method: Combined in-context learning with curated documentation and multi-agent decomposition to create autonomous AI agents trainable in specialized computational domains within minutes. Benchmarked three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on problems spanning quantum phase transitions, open quantum system dynamics, and photochemical reactions.

Result: Achieved approximately 90% success rate across representative benchmark tasks. Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrated that both in-context learning and multi-agent architecture are essential. Multi-agent configuration substantially reduced implementation errors and hallucinations compared to simpler architectures.

Conclusion: LLM agents can autonomously perform complex tensor network simulations with high success rates, with multi-agent architecture and in-context learning being critical components for reducing errors and hallucinations in specialized computational domains.

Abstract: We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Tensor network methods are powerful tools for quantum simulation, but their effective use requires expertise typically acquired through years of graduate training. By combining in-context learning with curated documentation and multi-agent decomposition, we create autonomous AI agents that can be trained in specialized computational domains within minutes. We benchmark three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on problems spanning quantum phase transitions, open quantum system dynamics, and photochemical reactions. Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.

</details>


### [34] [On the average-case complexity of learning states from the circular and Gaussian ensembles](https://arxiv.org/abs/2601.10197)
*Maxwell West*

Main category: quant-ph

TL;DR: The paper establishes average-case hardness of learning Born distributions from quantum states sampled from circular and Gaussian ensembles, complementing previous results for classical compact groups.


<details>
  <summary>Details</summary>
Motivation: To understand the complexity of states sampled from various quantum ensembles, particularly the computational hardness of learning their Born distributions, which is central to quantum information theory.

Method: Uses statistical query model analysis and an unconventional approach to integrating over compact groups, focusing on uniform measures on compact symmetric spaces of type AI, AII, and DIII (circular and fermionic Gaussian ensembles).

Result: Established average-case hardness of learning Born distributions for states from circular and Gaussian ensembles, and exactly evaluated total variation distances between output distributions of Haar random unitary/orthogonal circuits and constant distribution.

Conclusion: The findings complement analogous results for classical compact groups and demonstrate computational hardness for learning quantum states from these ensembles, with technical contributions in group integration methods.

Abstract: Studying the complexity of states sampled from various ensembles is a central component of quantum information theory. In this work we establish the average-case hardness of learning, in the statistical query model, the Born distributions of states sampled uniformly from the circular and (fermionic) Gaussian ensembles. These ensembles of states are induced variously by the uniform measures on the compact symmetric spaces of type AI, AII, and DIII. This finding complements analogous recent results for states sampled from the classical compact groups. On the technical side, we employ a somewhat unconventional approach to integrating over the compact groups which may be of some independent interest. For example, our approach allows us to exactly evaluate the total variation distances between the output distributions of Haar random unitary and orthogonal circuits and the constant distribution, which were previously known only approximately.

</details>


### [35] [Topology-Aware Block Coordinate Descent for Qubit Frequency Calibration of Superconducting Quantum Processors](https://arxiv.org/abs/2601.10203)
*Zheng Zhao,Weifeng Zhuang,Yanwu Gu,Peng Qian,Xiao Xiao,Dong E. Liu*

Main category: quant-ph

TL;DR: The paper establishes that the Snake optimizer for qubit frequency calibration is equivalent to Block Coordinate Descent (BCD), develops a topology-aware block ordering via SD-TSP with nearest-neighbor heuristic, achieves linear complexity per epoch while maintaining calibration quality, and demonstrates superior runtime efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Pre-execution calibration is a major bottleneck for superconducting quantum processors, with qubit frequency allocation being particularly challenging due to crosstalk-coupled objectives. The widely-used Snake optimizer lacks rigorous theoretical foundation, and there's a need for scalable, efficient calibration methods for NISQ-era processors.

Method: Formalizes Snake optimizer as Block Coordinate Descent (BCD), casts block ordering selection as a Sequence-Dependent Traveling Salesman Problem (SD-TSP), solves it efficiently with nearest-neighbor heuristic (BCD-NNA), formalizes calibration objective and reduced-experiment equivalence, and analyzes convergence of inexact BCD with noisy measurements.

Result: Under local crosstalk/bounded-degree assumptions, the method achieves linear complexity in qubit count per epoch while retaining calibration quality. Simulations show BCD-NNA ordering attains same optimization accuracy at markedly lower runtime than graph-based heuristics (BFS, DFS) and random orders, and is robust to measurement noise and tolerant to moderate non-local crosstalk.

Conclusion: The work provides a rigorous theoretical foundation for frequency calibration, develops a scalable topology-aware ordering method with linear complexity, and delivers an implementation-ready workflow for NISQ-era quantum processors that significantly reduces calibration runtime while maintaining quality.

Abstract: Pre-execution calibration is a major bottleneck for operating superconducting quantum processors, and qubit frequency allocation is especially challenging due to crosstalk-coupled objectives. We establish that the widely-used Snake optimizer is mathematically equivalent to Block Coordinate Descent (BCD), providing a rigorous theoretical foundation for this calibration strategy. Building on this formalization, we present a topology-aware block ordering obtained by casting order selection as a Sequence-Dependent Traveling Salesman Problem (SD-TSP) and solving it efficiently with a nearest-neighbor heuristic. The SD-TSP cost reflects how a given block choice expands the reduced-circuit footprint required to evaluate the block-local objective, enabling orders that minimize per-epoch evaluation time. Under local crosstalk/bounded-degree assumptions, the method achieves linear complexity in qubit count per epoch, while retaining calibration quality. We formalize the calibration objective, clarify when reduced experiments are equivalent or approximate to the full objective, and analyze convergence of the resulting inexact BCD with noisy measurements. Simulations on multi-qubit models show that the proposed BCD-NNA ordering attains the same optimization accuracy at markedly lower runtime than graph-based heuristics (BFS, DFS) and random orders, and is robust to measurement noise and tolerant to moderate non-local crosstalk. These results provide a scalable, implementation-ready workflow for frequency calibration on NISQ-era processors.

</details>


### [36] [Noise-Resilient Quantum Evolution in Open Systems through Error-Correcting Frameworks](https://arxiv.org/abs/2601.10206)
*Nirupam Basak,Goutam Paul,Pritam Chattopadhyay*

Main category: quant-ph

TL;DR: Quantum error correction codes embedded in microscopic system-bath models show five-qubit code outperforms Steane and toric codes, with QEC effectiveness depending on temperature regime and entanglement levels.


<details>
  <summary>Details</summary>
Motivation: To move beyond abstract quantum channel analysis and evaluate quantum error-correcting codes in realistic open quantum systems with explicit microscopic system-bath models, providing quantitative guidance for noise-resilient quantum architectures in near-term technologies.

Method: Embed QEC codes (five-qubit, Steane, toric) into multi-qubit registers coupled to bosonic thermal environments, derive second-order master equation for reduced dynamics, compute logical qubit state fidelities as functions of coupling strength, bath temperature, and correction cycles, analyze two-qubit Werner states.

Result: Five-qubit code strongly suppresses decoherence/relaxation in low-temperature regime; thermal excitations reduce QEC benefit in high-temperature regime but five-qubit still outperforms others. Critical evolution time exists before which QEC doesn't improve fidelity for Werner states, increasing with entanglement. Five-qubit code consistently offers higher fidelities than topological and concatenated architectures.

Conclusion: Establishes quantitative framework for evaluating QEC under realistic noise environments, demonstrates five-qubit code's superior performance in open-system settings, provides guidance for developing noise-resilient quantum architectures in near-term quantum technologies.

Abstract: We analyze quantum state preservation in open quantum systems using quantum error-correcting (QEC) codes that are explicitly embedded into microscopic system-bath models. Instead of abstract quantum channels, we consider multi-qubit registers coupled to bosonic thermal environments, derive a second-order master equation for the reduced dynamics, and use it to benchmark the five-qubit, Steane, and toric codes under local and collective noise. We compute state fidelities for logical qubits as functions of coupling strength, bath temperature, and the number of correction cycles. In the low-temperature regime, we find that repeated error-correction with the five-qubit code strongly suppresses decoherence and relaxation, while in the high-temperature regime, thermal excitations dominate the dynamics and reduce the benefit of all codes, though the five-qubit code still outperforms the Steane and toric codes. For two-qubit Werner states, we identify a critical evolution time before which QEC does not improve fidelity, and this time increases as entanglement grows. After this critical time, QEC does improve fidelity. Comparative analysis further reveals that the five-qubit code (the smallest perfect code) offers consistently higher fidelities than topological and concatenated architectures in these open-system settings. These findings establish a quantitative framework for evaluating QEC under realistic noise environments and provide guidance for developing noise-resilient quantum architectures in near-term quantum technologies.

</details>


### [37] [Coherence Limits in Interference-Based cos(2$\varphi$) Qubits](https://arxiv.org/abs/2601.10209)
*S. Messelot,A. Leblanc,J. -S. Tettekpoe,F. Lefloch,Q. Ficheux,J. Renard,É. Dumur*

Main category: quant-ph

TL;DR: Parity-protected cos(2φ) qubits face fundamental trade-off between charge and flux noise dephasing, limiting T_φ to microseconds despite millisecond T_1 lifetimes.


<details>
  <summary>Details</summary>
Motivation: To investigate coherence properties of parity-protected cos(2φ) qubits based on interference between Josephson elements, and understand practical limits of this approach for quantum computing applications.

Method: Show that various qubit implementations (semiconducting junctions, rhombus circuits, flowermon, KITE structures) can be described by same Hamiltonian as two multi-harmonic Josephson junctions in SQUID geometry. Use numerical simulations to examine how relaxation and dephasing rates depend on external flux and circuit parameters.

Result: Despite parity protection suppressing single Cooper pair tunneling, fundamental trade-off exists between charge and flux noise dephasing channels. With current circuit parameters, T_1 can exceed milliseconds but T_φ limited to few microseconds due to either flux or charge noise. Best compromise for maximum coherence identified through parameter optimization.

Conclusion: Findings establish practical limits on coherence of parity-protected cos(2φ) qubits and raise questions about long-term potential of this approach for quantum computing applications.

Abstract: We investigate the coherence properties of parity-protected $\cos(2\varphi)$ qubits based on interferences between two Josephson elements in a superconducting loop. We show that qubit implementations of a $\cos(2\varphi)$ potential using a single loop, such as those employing semiconducting junctions, rhombus circuits, flowermon and KITE structures, can be described by the same Hamiltonian as two multi-harmonic Josephson junctions in a SQUID geometry. We find that, despite the parity protection arising from the suppression of single Cooper pair tunneling, there exists a fundamental trade-off between charge and flux noise dephasing channels. Using numerical simulations, we examine how relaxation and dephasing rates depend on external flux and circuit parameters, and we identify the best compromise for maximum coherence. With currently existing circuit parameters, the qubit lifetime $T_1$ can exceed milliseconds while the dephasing time $T_\varphi$ remains limited to only a few microseconds due to either flux or charge noise. Our findings establish practical limits on the coherence of this class of qubits and raise questions about the long-term potential of this approach.

</details>


### [38] [Quantitative approach for the Dicke-Ising chain with an effective self-consistent matter Hamiltonian](https://arxiv.org/abs/2601.10210)
*J. Leibig,M. Hörmann,A. Langheld,A. Schellenberger,K. P. Schmidt*

Main category: quant-ph

TL;DR: The Dicke-Ising chain maps to an effective self-consistent matter Hamiltonian, enabling precise quantum phase diagram determination using NLCE+DMRG without needing photon-spin quantum correlations.


<details>
  <summary>Details</summary>
Motivation: To understand the quantum phase diagram of the Dicke-Ising chain in the thermodynamic limit without requiring quantum correlations between photons and spins, and to achieve higher accuracy in determining phase boundaries compared to previous estimates.

Method: Map the Dicke-Ising chain to an effective self-consistent matter Hamiltonian where the photon field acts as a self-consistent effective field, then solve this Hamiltonian using numerical linked-cluster expansions combined with density matrix renormalization group calculations (NLCE+DMRG).

Result: For ferromagnetic Ising couplings: refined location of multicritical point governing superradiant phase transition order with relative accuracy of 10⁻⁴. For antiferromagnetic Ising couplings: confirmed existence of narrow antiferromagnetic superradiant phase in thermodynamic limit, identified as many-body ground state of antiferromagnetic transverse-field Ising model with longitudinal field.

Conclusion: The effective matter Hamiltonian framework enables precise determination of the Dicke-Ising phase diagram in one dimension using NLCE+DMRG, revealing continuous polariton condensation followed by first-order transitions between different phases.

Abstract: In the thermodynamic limit, the Dicke-Ising chain maps exactly onto an effective self-consistent matter Hamiltonian with the photon field acting solely as a self-consistent effective field. As a consequence, no quantum correlations between photons and spins are needed to understand the quantum phase diagram. This enables us to determine the quantum phase diagram in the thermodynamic limit using numerical linked-cluster expansions combined with density matrix renormalization group calculations (NLCE+DMRG) to solve the resulting self-consistent matter Hamiltonian. This includes magnetically ordered phases with significantly improved accuracy compared to previous estimates. For ferromagnetic Ising couplings, we refine the location of the multicritical point governing the change in the order of the superradiant phase transition, reaching a relative accuracy of $10^{-4}$. For antiferromagnetic Ising couplings, we confirm the existence of the narrow antiferromagnetic superradiant phase in the thermodynamic limit. The effective matter Hamiltonian framework identifies the antiferromagnetic superradiant phase as the many-body ground state of an antiferromagnetic transverse-field Ising model with longitudinal field. This phase emerges through continuous Dicke-type polariton condensation from the antiferromagnetic normal phase, followed by a first-order transition to the paramagnetic superradiant phase. Thus, NLCE+DMRG provides a precise determination of the Dicke-Ising phase diagram in one dimension by solving the self-consistent effective matter Hamiltonian.

</details>


### [39] [Optimal control of a dissipative micromaser quantum battery in the ultrastrong coupling regime](https://arxiv.org/abs/2601.10281)
*Maristella Crotti,Luca Razzoli,Luigi Giannelli,Giuseppe A. Falci,Giuliano Benenti*

Main category: quant-ph

TL;DR: Micromaser quantum battery in ultrastrong coupling regime achieves enhanced charging and stability through optimized control and dissipation management.


<details>
  <summary>Details</summary>
Motivation: Investigate open system dynamics of micromaser quantum batteries operating in ultrastrong coupling regime under environmental dissipation, addressing challenges of unbounded energy growth and mixed states in USC systems.

Method: Single-mode cavity sequentially interacts with qubit chargers via Rabi Hamiltonian in USC regime; dissipative effects from weak coupling to thermal bath; optimized control of qubit preparation and interaction times; measurement-based passive-feedback strategy.

Result: Counter-rotating terms in USC regime improve charging speed but cause unbounded energy growth; dissipation mitigates these effects yielding finite energy steady-state; optimized protocols maximize ergotropy and stabilize against losses.

Conclusion: Interplay of ultrastrong light-matter coupling, controlled dissipation, and optimized control enables micromaser quantum batteries to achieve enhanced charging performance and long-term stability under realistic conditions.

Abstract: We investigate the open system dynamics of a micromaser quantum battery operating in the ultrastrong coupling (USC) regime under environmental dissipation. The battery consists of a single-mode electromagnetic cavity sequentially interacting, via the Rabi Hamiltonian, with a stream of qubits acting as chargers. Dissipative effects arise from the weak coupling of the qubit-cavity system to a thermal bath. Non-negligible in the USC regime, the counter-rotating terms substantially improve the charging speed, but also lead, in the absence of dissipation, to unbounded energy growth and highly mixed cavity states. Dissipation during each qubit-cavity interaction mitigates these detrimental effects, yielding steady-state of finite energy and ergotropy. Optimal control on qubit preparation and interaction times enhances battery's performance in: (i) Maximizing the stored ergotropy trhough an optimized charging protocol; (ii) Stabilizing the stored ergotropy against dissipative losses through an optimized measurement-based passive-feedback strategy. Overall, our numerical results demonstrate that the interplay of ultrastrong light-matter coupling, controlled dissipation, and optimized control strategies enables micromaser quantum batteries to achieve both enhanced charging performance and long-term stability under realistic conditions.

</details>


### [40] [Exponential improvement in benchmarking multiphoton interference](https://arxiv.org/abs/2601.10289)
*Rodrigo M. Sanz,Emilio Annoni,Stephen C. Wein,Carmen G. Almudever,Shane Mansfield,Ellen Derbyshire,Rawad Mezher*

Main category: quant-ph

TL;DR: New protocol using quantum Fourier transform achieves constant sample complexity for estimating genuine n-photon indistinguishability, representing exponential improvement over previous exponential-scaling methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for benchmarking multi-photon indistinguishability have exponential sample complexity scaling with photon number, making them impractical for scalability in photonic quantum technologies that require multiple indistinguishable photons.

Method: Introduces new theorems linking distinguishability to suppression laws of quantum Fourier transform (QFT) interferometer, then proposes protocol using QFT to benchmark genuine n-photon indistinguishability (GI) with constant sample complexity for prime photon numbers and sub-polynomial scaling otherwise.

Result: Protocol achieves exponential improvement over state-of-the-art, with constant sample complexity for prime photon numbers and sub-polynomial scaling otherwise. Experimental validation on Quandela's reconfigurable photonic quantum processor shows clear runtime and precision advantages. Protocol proven optimal in many relevant scenarios.

Conclusion: Establishes first scalable method for computing multi-photon indistinguishability applicable to current and near-term photonic quantum hardware, solving the exponential scaling problem of previous approaches.

Abstract: Several photonic quantum technologies rely on the ability to generate multiple indistinguishable photons. Benchmarking the level of indistinguishability of these photons is essential for scalability. The Hong-Ou-Mandel dip provides a benchmark for the indistinguishability between two photons, and extending this test to the multi-photon setting has so far resulted in a protocol that computes the genuine n-photon indistinguishability (GI). However, this protocol has a sample complexity that increases exponentially with the number of input photons for an estimation of GI up to a given additive error. To address this problem, we introduce new theorems that strengthen our understanding of the relationship between distinguishability and the suppression laws of the quantum Fourier transform interferometer (QFT). Building on this, we propose a protocol using the QFT for benchmarking GI that achieves constant sample complexity for the estimation of GI up to a given additive error for prime photon numbers, and sub-polynomial scaling otherwise, representing an exponential improvement over the state of the art. We prove the optimality of our protocol in many relevant scenarios and validate our approach experimentally on Quandela's reconfigurable photonic quantum processor, where we observe a clear advantage in runtime and precision over the state of the art. We therefore establish the first scalable method for computing multi-photon indistinguishability, which applies naturally to current and near-term photonic quantum hardware.

</details>


### [41] [Complex scalar relativistic field as a probability amplitude](https://arxiv.org/abs/2601.10302)
*Yu. M. Poluektov*

Main category: quant-ph

TL;DR: Proposes relativistic equation for neutral complex field as probability amplitude, obtains continuity equation, identifies two excitation types with positive energy and different dispersion laws, derives conservation laws via Lagrangian formalism, considers secondary quantization.


<details>
  <summary>Details</summary>
Motivation: To develop a relativistic quantum field theory framework for neutral complex fields that can serve as probability amplitudes, establishing proper continuity equations and identifying the physical excitations of such fields.

Method: Proposes relativistic equation for neutral complex field, derives continuity equation for probability density, analyzes field excitations to identify two types with positive energy and different dispersion laws, applies Lagrangian formalism to obtain conservation laws, and considers transition to secondary quantization.

Result: Successfully obtains continuity equation for probability density, identifies two distinct excitation types with positive energy but different dispersion laws, derives conservation laws from Lagrangian formalism, and establishes framework for secondary quantization.

Conclusion: The proposed relativistic equation provides a consistent framework for neutral complex fields as probability amplitudes, with well-defined excitations and conservation laws, enabling transition to quantum field theory through secondary quantization.

Abstract: A relativistic equation for a neutral complex field as a probability amplitude is proposed. The continuity equation for the probability density is obtained. It is shown that there are two types of excitations of this field, which describe particles with positive energy and different dispersion laws. Based on the Lagrangian formalism, conservation laws are obtained. The transition to secondary quantization is considered.

</details>


### [42] [Addition to the dynamic Stark shift of the coherent population trapping resonance](https://arxiv.org/abs/2601.10319)
*Gavriil Voloshin,Konstantin Barantsev,Andrey Litvinov*

Main category: quant-ph

TL;DR: Theoretical study of light-induced shift in coherent population trapping resonance, showing additional shift beyond conventional Stark shift due to off-resonant transitions in Λ-scheme atomic systems.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize the additional light-induced shift in coherent population trapping resonances beyond the conventional dynamic Stark shift, particularly for precision atomic devices like quantum frequency standards.

Method: Developed analytical model describing interaction of two radiation components with Λ-scheme atomic system including additional excited state level. Analyzed both weak and strong coupling regimes with off-resonant transitions.

Result: Derived analytical expression for additional shift in weak-coupling limit, showing significant impact on resonance shape and sensitivity to laser intensities. Found that under strong coupling, shift deviates substantially from linear intensity dependence.

Conclusion: The additional shift due to off-resonant transitions offers new opportunities for controlling light shifts in precision atomic devices, potentially improving quantum frequency standards.

Abstract: This paper presents a theoretical study of the light-induced shift of the coherent population trapping resonance. An analytical model is proposed that describes the interaction of two radiation components with an atomic system using a $Λ$ scheme and takes into account an additional level of excited state. Both weak and strong coupling regimes with off-resonant transitions are considered. It is shown that, in addition to the conventional dynamic Stark shift, an extra shift arises due to the distortion of the resonance line shape when bichromatic laser radiation interacts with off-resonant atomic transitions. An analytical expression for this additional shift is derived in the weak-coupling limit, and its significant impact on the resonance shape and sensitivity to the intensities of the laser field components is demonstrated. It is found that under strong coupling conditions, the additional shift can deviate substantially from a linear dependence on light intensity, suggesting new opportunities for controlling light shifts in precision atomic devices such as quantum frequency standards.

</details>


### [43] [Principles of Optics in the Fock Space: Scalable Manipulation of Giant Quantum States](https://arxiv.org/abs/2601.10325)
*Yifang Xu,Yilong Zhou,Ziyue Hua,Lida Sun,Jie Zhou,Weiting Wang,Weizhou Cai,Hongwei Huang,Lintao Xiao,Guangming Xue,Haifeng Yu,Ming Li,Chang-Ling Zou,Luyan Sun*

Main category: quant-ph

TL;DR: Fock-space optics establishes wave propagation in quantum domain by treating photon number as synthetic dimension, enabling optical analogies (propagation, refraction, lensing, dispersion, interference) with up to 180 photons in superconducting microwave resonator.


<details>
  <summary>Details</summary>
Motivation: While classical optics provides scalable control over spatial/temporal degrees of freedom, quantum state engineering in Fock space has been limited to few-photon regimes due to computational and experimental challenges of large Hilbert spaces. There's a need to bridge classical optical concepts with quantum domain for scalable quantum information processing.

Method: Introduces "Fock-space optics" framework treating photon number as synthetic dimension. Uses superconducting microwave resonator to experimentally demonstrate Fock-space analogues of optical phenomena: propagation, refraction, lensing, dispersion, and interference with up to 180 photons.

Result: Successfully demonstrates Fock-space optical phenomena with up to 180 photons, establishing fundamental correspondence between Schrödinger evolution in single bosonic mode and classical paraxial wave propagation. Shows quantum state engineering in high-dimensional Hilbert space.

Conclusion: Establishes conceptual bridge between classical optics and quantum domain, enabling intuitive optical concepts to map onto high-dimensional quantum state engineering. Opens path toward scalable control of large-scale quantum systems with thousands of photons and advanced bosonic information processing.

Abstract: The manipulation of distinct degrees of freedom of photons plays a critical role in both classical and quantum information processing. While the principles of wave optics provide elegant and scalable control over classical light in spatial and temporal domains, engineering quantum states in Fock space has been largely restricted to few-photon regimes, hindered by the computational and experimental challenges of large Hilbert spaces. Here, we introduce ``Fock-space optics", establishing a conceptual framework of wave propagation in the quantum domain by treating photon number as a synthetic dimension. Using a superconducting microwave resonator, we experimentally demonstrate Fock-space analogues of optical propagation, refraction, lensing, dispersion, and interference with up to 180 photons. These results establish a fundamental correspondence between Schrödinger evolution in a single bosonic mode and classical paraxial wave propagation. By mapping intuitive optical concepts onto high-dimensional quantum state engineering, our work opens a path toward scalable control of large-scale quantum systems with thousands of photons and advanced bosonic information processing.

</details>


### [44] [Realistic prospects for testing a relativistic local quantum measurement inequality](https://arxiv.org/abs/2601.10354)
*Riccardo Falcone,Claudio Conti*

Main category: quant-ph

TL;DR: Derivation of a relativistic quantum measurement inequality quantifying trade-off between vacuum insensitivity and excitation responsiveness for finite-size detectors, with explicit bound for coherent states and numerical verification in realistic photodetection scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop experimentally testable bounds on quantum measurement performance in relativistic settings, specifically addressing the fundamental trade-off between minimizing false positives (vacuum insensitivity) and maximizing detection probability for actual excitations.

Method: Builds on Reeh-Schlieder approximation for coherent states to derive explicit inequality; models detection region as square prism with finite time window; analyzes normally incident single-mode coherent state; performs numerical simulations.

Result: Derived explicit, practically applicable bound for arbitrary coherent states; numerical results confirm expected qualitative behavior: suppressing dark counts (vacuum insensitivity) necessarily tightens achievable click probability for excitations.

Conclusion: Established experimentally testable relativistic quantum measurement inequality that quantifies fundamental performance trade-off, providing concrete bounds for realistic photodetection scenarios with finite-size detectors.

Abstract: We investigate the experimental prospects for testing a relativistic local quantum measurement inequality that quantifies the trade-off between vacuum insensitivity and responsiveness to excitations for finite-size detectors. Building on the Reeh--Schlieder approximation for coherent states, we derive an explicit and practically applicable bound for arbitrary coherent states. To connect with realistic photodetection scenarios, we model the detection region as a square prism operating over a finite time window and consider a normally incident single-mode coherent state. Numerical results exhibit the expected qualitative behavior: suppressing dark counts necessarily tightens the achievable click probability.

</details>


### [45] [Learning Hamiltonians in the Heisenberg limit with static single-qubit fields](https://arxiv.org/abs/2601.10380)
*Shrigyan Brahmachari,Shuchen Zhu,Iman Marvian,Yu Tong*

Main category: quant-ph

TL;DR: A Hamiltonian learning protocol achieves Heisenberg-limited scaling using only static single-qubit control fields with strength independent of target precision, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Heisenberg-limited Hamiltonian learning protocols require either multi-qubit operations prone to noise or single-qubit operations whose frequency/strength increases with desired precision, limiting applicability on near-term quantum platforms.

Method: Protocol learns quantum Hamiltonian with optimal Heisenberg-limited scaling using only single-qubit control in the form of static fields with strengths independent of target precision; robust against SPAM errors.

Result: Method achieves Heisenberg-limited scaling through rigorous mathematical proof and numerical experiments; proves information-theoretic lower bound showing non-vanishing static field strength is necessary for Heisenberg limit without extensive discrete control operations.

Conclusion: Protocol overcomes limitations of existing methods, provides new tools for device characterization and quantum sensing on near-term quantum platforms.

Abstract: Learning the Hamiltonian governing a quantum system is a central task in quantum metrology, sensing, and device characterization. Existing Heisenberg-limited Hamiltonian learning protocols either require multi-qubit operations that are prone to noise, or single-qubit operations whose frequency or strength increases with the desired precision. These two requirements limit the applicability of Hamiltonian learning on near-term quantum platforms. We present a protocol that learns a quantum Hamiltonian with the optimal Heisenberg-limited scaling using only single-qubit control in the form of static fields with strengths that are independent of the target precision. Our protocol is robust against the state preparation and measurement (SPAM) error. By overcoming these limitations, our protocol provides new tools for device characterization and quantum sensing. We demonstrate that our method achieves the Heisenberg-limited scaling through rigorous mathematical proof and numerical experiments. We also prove an information-theoretic lower bound showing that a non-vanishing static field strength is necessary for achieving the Heisenberg limit unless one employs an extensive number of discrete control operations.

</details>


### [46] [Experimental Realization of Rabi-Driven Reset for Fast Cooling of a High-Q Cavity](https://arxiv.org/abs/2601.10385)
*Eliya Blumenthal,Natan Karaev,Shay Hacohen-Gourgy*

Main category: quant-ph

TL;DR: Rabi-Driven Reset (RDR) enables fast, measurement-free cooling of superconducting cavity modes by converting dispersive interactions into effective Jaynes-Cummings couplings via strong Rabi drives, achieving photon decay 100× faster than intrinsic lifetime.


<details>
  <summary>Details</summary>
Motivation: High-quality bosonic memories are essential for hardware-efficient quantum error correction, but their isolation creates a persistent bottleneck for fast, high-fidelity reset. Existing methods either use weak intermode cross-Kerr conversion or measurement-based sequences with substantial latency.

Method: Rabi-Driven Reset uses a strong resonant Rabi drive on a transmon qubit combined with sideband drives on memory and readout modes detuned by the Rabi frequency. This converts the dispersive interaction into an effective Jaynes-Cummings coupling between qubit dressed states and each mode, creating a tunable dissipation channel from memory to the cold readout bath.

Result: Demonstrated single photon decay time of 1.2 μs (more than 100× faster than intrinsic lifetime) and reset of ~30 thermal photons to steady-state average photon number of 0.045 ± 0.025 in about 80 μs.

Conclusion: RDR provides a hardware-efficient, measurement-free approach for fast cavity reset that scales with dispersive interaction strength and drive amplitude rather than intermode cross-Kerr, enabling rapid cooling even in weakly coupled architectures designed to suppress direct mode-mode coupling.

Abstract: High-Q bosonic memories are central to hardware-efficient quantum error correction, but their isolation makes fast, high-fidelity reset a persistent bottleneck. Existing approaches either rely on weak intermode cross-Kerr conversion or on measurement-based sequences with substantial latency. Here we demonstrate a hardware-efficient Rabi-Driven Reset (RDR) that implements continuous, measurement-free cooling of a superconducting cavity mode. A strong resonant Rabi drive on a transmon, together with sideband drives on the memory and readout modes detuned by the Rabi frequency, converts the dispersive interaction into an effective Jaynes-Cummings coupling between the qubit dressed states and each mode. This realizes a tunable dissipation channel from the memory to the cold readout bath. Crucially, the engineered coupling scales with the qubit-mode dispersive interaction and the drive amplitude, rather than with the intermode cross-Kerr, enabling fast cooling even in very weakly coupled architectures that deliberately suppress direct mode-mode coupling. We demonstrate RDR of a single photon with a decay time of $1.2 μs$, more than two orders of magnitude faster than the intrinsic lifetime. Furthermore, we reset about 30 thermal photons in about $80 μs$ to a steady-state average photon number of $\bar{n} = 0.045 \pm 0.025$.

</details>


### [47] [A Collection of Pinsker-type Inequalities for Quantum Divergences](https://arxiv.org/abs/2601.10395)
*Kläre Wienecke,Gereon Koßmann,René Schwonnek*

Main category: quant-ph

TL;DR: The paper establishes lower bounds on various quantum and classical divergences in terms of trace distance, extending Pinsker's inequality to multiple divergence types including f-divergences and Rényi divergences, with applications to smoothed divergences.


<details>
  <summary>Details</summary>
Motivation: Pinsker's inequality provides a fundamental relationship between Umegaki divergence (quantum relative entropy) and trace distance, but similar bounds for other important divergences are lacking. The authors aim to establish analogous inequalities for a broader class of quantum and classical divergences to enable better theoretical analysis and applications in quantum information theory.

Method: The authors formulate mathematical estimates (lower bounds) connecting various divergences to trace distance. They cover f-divergences (including Hellinger and χ² divergences), Rényi divergences (including Umegaki, collision, and max divergences as special cases), and provide a systematic strategy to adapt these bounds to smoothed divergences.

Result: The paper establishes comprehensive lower bounds on multiple quantum and classical divergences in terms of trace distance, extending the classical Pinsker inequality framework. The results cover both standard and smoothed divergence measures, providing a unified approach to relating different distance measures in quantum information theory.

Conclusion: The work successfully generalizes Pinsker's inequality to a wide range of quantum and classical divergences, establishing fundamental relationships between different distance measures. The extension to smoothed divergences provides practical tools for applications in quantum information processing, error analysis, and theoretical studies of quantum systems.

Abstract: Pinsker's inequality sets a lower bound on the Umegaki divergence of two quantum states in terms of their trace distance. In this work, we formulate corresponding estimates for a variety of quantum and classical divergences including $f$-divergences like Hellinger and $χ^2$-divergences as well as Rényi divergences and special cases thereof like the Umegaki divergence, collision divergence, max divergence. We further provide a strategy on how to adapt these bounds to smoothed divergences.

</details>


### [48] [Bounding many-body properties under partial information and finite measurement statistics](https://arxiv.org/abs/2601.10408)
*Luke Mortimer,Leonardo Zambrano,Antonio Acín,Donato Farina*

Main category: quant-ph

TL;DR: Scalable semidefinite programming approach for certifying quantum system properties using moment-matrix relaxations and finite-shot measurements


<details>
  <summary>Details</summary>
Motivation: Need for scalable methods to calculate bounds on many-body quantum system properties that complement estimation methods and work with practical experimental constraints like finite-shot measurements

Method: Uses moment-matrix relaxations within semidefinite programming framework to make certification scalable in qubit number; adapts approach for specific system knowledge (ground states, symmetries, steady states)

Result: Develops a scalable real-world certification scheme that leverages semidefinite programming relaxations and accounts for experimental shot noise

Conclusion: Provides practical, scalable certification method for quantum systems that works with incomplete observables and experimental noise, enabling real-world quantum property verification

Abstract: Calculating bounds of properties of many-body quantum systems is of paramount importance, since they guide our understanding of emergent quantum phenomena and complement the insights obtained from estimation methods. Recent semidefinite programming approaches enable probabilistic bounds from finite-shot measurements of easily accessible, yet informationally incomplete, observables. Here we render these methods scalable in the number of qubits by instead utilizing moment-matrix relaxations. After introducing the general formalism, we show how the approach can be adapted with specific knowledge of the system, such as it being the ground state of a given Hamiltonian, possessing specific symmetries or being the steady state of a given Lindbladian. Our approach defines a scalable real-world certification scheme leveraging semidefinite programming relaxations and experimental estimations which, unavoidably, contain shot noise.

</details>


### [49] [Tight bounds on recurrence time in closed quantum systems](https://arxiv.org/abs/2601.10409)
*Marcin Kotowski,Michał Oszmaniec*

Main category: quant-ph

TL;DR: The paper establishes rigorous upper bounds on quantum recurrence times, relating them to escape times from initial state neighborhoods and providing partial solutions to inverse quantum speed limit problems.


<details>
  <summary>Details</summary>
Motivation: Despite the fundamental nature of quantum recurrence (Poincaré recurrence theorem), there has been a lack of rigorous quantitative understanding of recurrence times in isolated quantum systems.

Method: The authors establish upper bounds on recurrence time using escape time from initial state neighborhoods, formulate an inverse quantum speed limit problem to estimate escape times, and analyze the impact of initial state coherence in the Hamiltonian eigenbasis.

Result: Upper bound on recurrence time: t_rec ≲ t_exit(ε)(1/ε)^d, where d is Hilbert-space dimension. For pure states: t_exit(ε) ≈ ε/√Δ(H^2), with Δ(H^2) being Hamiltonian variance. The bound is generically saturated for random Hamiltonians.

Conclusion: The work provides rigorous quantitative bounds on quantum recurrence times, connects recurrence to escape times and inverse quantum speed limits, and shows these bounds are tight for generic Hamiltonians, advancing fundamental understanding of quantum dynamics.

Abstract: The evolution of an isolated quantum system inevitably exhibits recurrence: the state returns to the vicinity of its initial condition after finite time. Despite its fundamental nature, a rigorous quantitative understanding of recurrence has been lacking. We establish upper bounds on the recurrence time, $t_{\mathrm{rec}} \lesssim t_{\mathrm{exit}}(ε)(1/ε)^d$, where $d$ is the Hilbert-space dimension, $ε$ the neighborhood size, and $t_{\mathrm{exit}}(ε)$ the escape time from this neighborhood. For pure states evolving under a Hamiltonian $H$, estimating $t_{\mathrm{exit}}$ is equivalent to an inverse quantum speed limit problem: finding upper bounds on the time a time-evolved state $ψ_t$ needs to depart from the $ε$-vicinity of the initial state $ψ_0$. We provide a partial solution, showing that under mild assumptions $t_{\mathrm{exit}}(ε) \approx ε/\sqrt{ Δ(H^2)}$, with $Δ(H^2)$ the Hamiltonian variance in $ψ_0$. We show that our upper bound on $t_{\mathrm{rec}}$ is generically saturated for random Hamiltonians. Finally, we analyze the impact of coherence of the initial state in the eigenbasis of $H$ on recurrence behavior.

</details>


### [50] [Unifying Quantum and Classical Dynamics](https://arxiv.org/abs/2601.10423)
*Abdul Rahaman Shaikh,Tabish Qureshi*

Main category: quant-ph

TL;DR: Quantum and classical dynamics are shown to be exactly equivalent when Heisenberg equations are reformulated to match Newton's equations without ħ, unifying both theories under identical mathematical structure.


<details>
  <summary>Details</summary>
Motivation: Despite quantum physics being fundamental, the emergence of classical mechanics from quantum mechanics remains challenging. The paper aims to unify classical and quantum dynamics rather than showing classical behavior emerges from quantum mechanics.

Method: Reformulating Heisenberg equations of motion into a form identical to Newton's equations of motion, eliminating ħ from the formulation, and demonstrating exact equivalence between quantum observables and classical counterparts.

Result: Heisenberg equations can be cast in a form identical to Newton's equations, with ħ absent, showing both quantum and classical dynamics are governed by the same equations with Heisenberg operators replacing classical observables.

Conclusion: Quantum and classical dynamics are exactly equivalent when properly formulated, providing a unified framework where both theories share identical mathematical structure without requiring classical behavior to emerge from quantum mechanics.

Abstract: Classical and quantum physics represent two distinct theories; however, quantum physics is regarded as the more fundamental of the two. It is posited that classical mechanics should arise from quantum mechanics under certain limiting conditions. Nevertheless, this remains a challenging objective. In this work, we explore the potential for unifying the dynamics of classical and quantum physics. This discussion does not suggest that classical behavior emerges from quantum mechanics; rather, it demonstrates the exact equivalence between the dynamics of quantum observables and their classical counterparts. It is shown that the Heisenberg equations of motion can be cast in a form that is identical to Newton's equations of motion, with $\hbar$ being absent from the formulation. This implies that both quantum and classical dynamics are governed by the same equations, with the Heisenberg operators substituting the classical observables.

</details>


### [51] [Reduction of thermodynamic uncertainty by a virtual qubit](https://arxiv.org/abs/2601.10429)
*Yang Li,Fu-Lin Zhang*

Main category: quant-ph

TL;DR: Quantum thermal machines with coherent coupling can violate classical thermodynamic uncertainty relations through purely quantum corrections to current fluctuations originating from steady-state coherences.


<details>
  <summary>Details</summary>
Motivation: To understand how quantum violations of classical thermodynamic uncertainty relations (TUR) reveal genuinely quantum thermodynamic effects that are essential for improving performance and enabling optimization in quantum technologies, specifically in quantum thermal-machine models.

Method: Analysis of TUR in paradigmatic quantum thermal-machine models where operation is enabled by coherent coupling between two energy levels forming a virtual qubit. Steady-state coherences are confined to this virtual-qubit subspace, while without coherent coupling the system satisfies detailed balance with thermal reservoirs and supports no steady-state heat currents.

Result: Steady-state currents and entropy production can be fully reproduced by an effective classical Markov process, but current fluctuations acquire an additional purely quantum correction originating from coherence. The thermodynamic uncertainty naturally decomposes into classical (diagonal) and coherent contributions, with the latter becoming negative under resonant conditions and reaching its minimum at the coupling strength that maximizes steady-state coherence.

Conclusion: Quantum thermal machines with coherent coupling can surpass classical TUR bounds in the vicinity of the reversible limit, with optimization conditions and criteria identified for achieving quantum advantages in thermodynamic uncertainty relations.

Abstract: The thermodynamic uncertainty relation (TUR) imposes a fundamental constraint between current fluctuations and entropy production, providing a refined formulation of the second law for micro- and nanoscale systems. Quantum violations of the classical TUR reveal genuinely quantum thermodynamic effects, which are essential for improving performance and enabling optimization in quantum technologies. In this work, we analyze the TUR in a class of paradigmatic quantum thermal-machine models whose operation is enabled by coherent coupling between two energy levels forming a virtual qubit. Steady-state coherences are confined to this virtual-qubit subspace, while in the absence of coherent coupling the system satisfies detailed balance with the thermal reservoirs and supports no steady-state heat currents. We show that the steady-state currents and entropy production can be fully reproduced by an effective classical Markov process, whereas current fluctuations acquire an additional purely quantum correction originating from coherence. As a result, the thermodynamic uncertainty naturally decomposes into a classical (diagonal) contribution and a coherent contribution. The latter becomes negative under resonant conditions and reaches its minimum at the coupling strength that maximizes steady-state coherence. We further identify the optimization conditions and the criteria for surpassing the classical TUR bound in the vicinity of the reversible limit.

</details>


### [52] [The SpinPulse library for transpilation and noise-accurate simulation of spin qubit quantum computers](https://arxiv.org/abs/2601.10435)
*Benoît Vermersch,Oscar Gravier,Nathan Miscopein,Julia Guignon,Carlos Ramos Marimón,Jonathan Durandau,Matthieu Dartiailh,Tristan Meunier,Valentin Savin*

Main category: quant-ph

TL;DR: SpinPulse is an open-source Python package for pulse-level simulation of spin qubit quantum computers with realistic noise modeling.


<details>
  <summary>Details</summary>
Motivation: To provide realistic pulse-level simulations of spin qubit quantum computers that incorporate classical non-Markovian noise, supporting hardware development and quantum circuit optimization.

Method: SpinPulse transpiles quantum circuits into native gate sets, converts them to pulse sequences, then numerically integrates these sequences in simulated noisy experimental environments with classical non-Markovian noise modeling.

Result: The package enables workflows including transpilation, pulse-level compilation, hardware benchmarking, quantum error mitigation, and large-scale simulations via integration with the tensor-network library quimb.

Conclusion: SpinPulse is expected to be a valuable open-source tool for the quantum computing community, fostering development of high-fidelity quantum circuits and improved error mitigation/correction strategies.

Abstract: We introduce SpinPulse, an open-source python package for simulating spin qubit-based quantum computers at the pulse-level. SpinPulse models the specific physics of spin qubits, particularly through the inclusion of classical non-Markovian noise. This enables realistic simulations of native gates and quantum circuits, in order to support hardware development. In SpinPulse, a quantum circuit is first transpiled into the native gate set of our model and then converted to a pulse sequence. This pulse sequence is subsequently integrated numerically in the presence of a simulated noisy experimental environment. We showcase workflows including transpilation, pulse-level compilation, hardware benchmarking, quantum error mitigation, and large-scale simulations via integration with the tensor-network library quimb. We expect SpinPulse to be a valuable open-source tool for the quantum computing community, fostering efforts to devise high-fidelity quantum circuits and improved strategies for quantum error mitigation and correction.

</details>


### [53] [Minimal-Energy Optimal Control of Tunable Two-Qubit Gates in Superconducting Platforms Using Continuous Dynamical Decoupling](https://arxiv.org/abs/2601.10446)
*Adonai Hilário da Silva,Octávio da Motta,Leonardo Kleber Castelano,Reginaldo de Jesus Napolitano*

Main category: quant-ph

TL;DR: A unified scheme for generating high-fidelity entangling gates in superconducting platforms using continuous dynamical decoupling combined with variational minimal-energy optimal control.


<details>
  <summary>Details</summary>
Motivation: To develop a practical and noise-resilient scheme for designing superconducting entangling gates that can suppress residual couplings, calibration drifting, and quasistatic noise while achieving virtually unit fidelity.

Method: Combines continuous dynamical decoupling (CDD) to suppress noise and create a stable effective Hamiltonian, followed by variational geodesic optimization to calculate smooth low-energy single-qubit control functions that directly minimize gate infidelity.

Result: Achieves virtually unit fidelity and robustness for CZ, CX, and generic entangling gates under restricted single-qubit action with experimentally realistic control fields.

Conclusion: CDD-enhanced variational geometric optimal control establishes a practical and noise-resilient scheme for designing superconducting entangling gates.

Abstract: We present a unified scheme for generating high-fidelity entangling gates in superconducting platforms by continuous dynamical decoupling (CDD) combined with variational minimal-energy optimal control. During the CDD stage, we suppress residual couplings, calibration drifting, and quasistatic noise, resulting in a stable effective Hamiltonian that preserves the designed ZZ interaction intended for producing tunable couplers. In this stable $\mathrm{SU}(4)$ manifold, we calculate smooth low-energy single-quibt control functions using a variational geodesic optimization process that directly minimizes gate infidelity. We illustrate the methodology by applying it to CZ, CX, and generic engangling gates, achieving virtually unit fidelity and robustness under restricted single-qubit action, with experimentally realistic control fields. These results establish CDD-enhanced variational geometric optimal control as a practical and noise-resilient scheme for designing superconducting entangling gates.

</details>


### [54] [Localization Landscape in Non-Hermitian and Floquet quantum systems](https://arxiv.org/abs/2601.10451)
*David Guéry-Odelin,François Impens*

Main category: quant-ph

TL;DR: Generalization of localization landscape theory to non-Hermitian, Floquet, and topological systems using H†H operator, enabling eigenstate-free prediction of localization phenomena.


<details>
  <summary>Details</summary>
Motivation: Extend the Filoche-Mayboroda localization landscape beyond static, elliptic, and Hermitian settings while preserving geometric interpretability, to address non-Hermitian, Floquet, and topological systems where traditional eigenstate-based methods are limited.

Method: Use the positive operator H†H to construct a generalized localization landscape that predicts localization without computing eigenstates. Apply singular-value analysis to reveal spectral instabilities and skin effects, use Sambe formulation for driven systems, and extract topological zero modes directly from the landscape.

Result: The generalized landscape successfully predicts localization in Hatano-Nelson chains, driven two-level systems, and driven Aubry-André-Harper models with quantitative accuracy. It captures singular-value collapse indicating spectral instabilities, skin effects, coherent destruction of tunneling, and topological zero modes.

Conclusion: The approach establishes a unified predictor for localization in equilibrium and driven quantum matter, extending landscape theory to non-Hermitian, Floquet, and topological regimes while maintaining geometric interpretability and eigenstate-free computation.

Abstract: We propose a generalization of the Filoche--Mayboroda localization landscape that extends the theory well beyond the static, elliptic and Hermitian settings while preserving its geometric interpretability. Using the positive operator $H^\dagger H$, we obtain a landscape that predicts localization across non-Hermitian, Floquet, and topological systems without computing eigenstates. Singular-value collapse reveals spectral instabilities and skin effects, the Sambe formulation captures coherent destruction of tunneling, and topological zero modes emerge directly from the landscape. Applications to Hatano--Nelson chains, driven two-level systems, and driven Aubry--André--Harper models confirm quantitative accuracy, establishing a unified predictor for localization in equilibrium and driven quantum matter.

</details>


### [55] [Erasure conversion for singlet-triplet spin qubits enables high-performance shuttling-based quantum error correction](https://arxiv.org/abs/2601.10461)
*Adam Siegel,Simon Benjamin*

Main category: quant-ph

TL;DR: The paper presents a fault-tolerant quantum error correction framework using singlet-triplet (dual-spin) qubits as natural erasure qubits in semiconductor quantum dot devices, featuring hardware-efficient leakage detection and achieving significant improvements in error correction performance.


<details>
  <summary>Details</summary>
Motivation: To establish singlet-triplet qubits as optimal for high-fidelity shuttling in semiconductor quantum dot devices and develop a practical fault-tolerant quantum error correction framework that leverages their natural erasure properties.

Method: Introduces a hardware-efficient leakage-detection protocol that automatically projects leaked qubits back to the computational subspace without measurement feedback or increased classical control overheads. Combines this with the XZZX surface code and leakage-aware decoding.

Result: Demonstrates a twofold increase in error correction threshold and achieves orders-of-magnitude reductions in logical error rates compared to conventional approaches.

Conclusion: Establishes the singlet-triplet encoding as a practical route toward high-fidelity shuttling and erasure-based, fault-tolerant quantum computation in semiconductor devices.

Abstract: Fast and high fidelity shuttling of spin qubits has been demonstrated in semiconductor quantum dot devices. Several architectures based on shuttling have been proposed; it has been suggested that singlet-triplet (dual-spin) qubits could be optimal for the highest shuttling fidelities. Here we present a fault-tolerant framework for quantum error correction based on such dual-spin qubits, establishing them as a natural realisation of erasure qubits within semiconductor architectures. We introduce a hardware-efficient leakage-detection protocol that automatically projects leaked qubits back onto the computational subspace, without the need for measurement feedback or increased classical control overheads. When combined with the XZZX surface code and leakage-aware decoding, we demonstrate a twofold increase in the error correction threshold and achieve orders-of-magnitude reductions in logical error rates. This establishes the singlet-triplet encoding as a practical route toward high-fidelity shuttling and erasure-based, fault-tolerant quantum computation in semiconductor devices.

</details>


### [56] [Nonlinear quantum Kibble-Zurek ramps in open systems at finite temperature](https://arxiv.org/abs/2601.10465)
*Johannes N. Kriel,Emma C. King,Michael Kastner*

Main category: quant-ph

TL;DR: The paper analyzes quantum systems undergoing simultaneous temperature and Hamiltonian parameter ramps toward quantum critical points, showing these protocols can probe universality classes at finite temperature despite being out-of-equilibrium.


<details>
  <summary>Details</summary>
Motivation: To develop protocols that allow probing of quantum critical exponents (ν and z) in experimentally realistic finite-temperature situations, overcoming limitations of fixed-temperature protocols that cannot access universality classes at finite temperature.

Method: Analyzes quantum systems under protocols where temperature and Hamiltonian control parameter are simultaneously ramped in nonlinear fashion toward quantum critical points. Uses open-system version of Kitaev quantum wire as example, identifying ramps where both coherent and incoherent dynamics affect excitation density non-negligibly.

Result: Shows these protocols enable probing of universality classes (characterized by critical exponents ν and z) in out-of-equilibrium situations at finite temperature. Identifies specific ramps that suppress subleading corrections to asymptotic scaling laws, providing guidance for experimental quantum critical exponent measurement.

Conclusion: Simultaneous temperature-Hamiltonian ramps provide a viable approach to dynamically probe quantum critical exponents in realistic finite-temperature experimental conditions, overcoming limitations of conventional protocols.

Abstract: We analyze quantum systems under a broad class of protocols in which the temperature and a Hamiltonian control parameter are ramped simultaneously and, in general, in a nonlinear fashion toward a quantum critical point. Using an open-system version of a Kitaev quantum wire as an example, we show that, unlike finite-temperature protocols at fixed temperature, these protocols allow us to probe, in an out-of-equilibrium situation and at finite temperature, the universality class (characterized by the critical exponents $ν$ and $z$) of an equilibrium quantum phase transition at zero temperature. Key to this is the identification of ramps in which both coherent and incoherent parts of the open-system dynamics affect the excitation density in a non-negligible way. We also identify the specific ramps for which subleading corrections to the asymptotic scaling laws are suppressed, which serves as a guide to dynamically probing quantum critical exponents in experimentally realistic finite-temperature situations.

</details>


### [57] [Analysis and Experimental Demonstration of Amplitude Amplification for Combinatorial Optimization](https://arxiv.org/abs/2601.10473)
*Daniel Koch,Brian Pardo,Kip Nieman*

Main category: quant-ph

TL;DR: The paper extends Quantum Amplitude Amplification (QAA) to handle cost functions like QUBO, develops exact formulas for linear cost functions, demonstrates performance via simulations up to 40 qubits, and validates experimentally on IBMQ and IonQ quantum hardware.


<details>
  <summary>Details</summary>
Motivation: To extend Quantum Amplitude Amplification beyond the conventional 2-dimensional Grover representation to handle more realistic combinatorial optimization problems with cost functions like QUBO, and to provide practical guidance for parameter settings in quantum implementations.

Method: Extends QAA's mathematical framework to encode cost functions in oracles, derives exact formulas for linear cost functions, performs simulations up to 40 qubits to analyze algorithmic performance across all solutions, and conducts experimental demonstrations on IBMQ (superconducting) and IonQ (trapped ion) quantum processors.

Result: Shows that linear cost functions allow exact formulas for optimal oracle parameter settings, demonstrates QAA's performance across all solutions with emphasis on near-optimal solutions, and validates that experimental probabilities match theoretical predictions as oracle and diffusion parameters vary.

Conclusion: Successfully extends QAA to practical cost functions, provides analytical tools for parameter optimization, demonstrates scalability through simulations, and validates the approach on current quantum hardware, showing promise for solving combinatorial optimization problems with quantum amplitude amplification.

Abstract: Quantum Amplitude Amplification (QAA), the generalization of Grover's algorithm, is capable of yielding optimal solutions to combinatorial optimization problems with high probabilities. In this work we extend the conventional 2-dimensional representation of Grover's (orthogonal collective states) to oracles which encode cost functions such as QUBO, and show that linear cost functions are a special case whereby an exact formula exists for determining optimal oracle parameter settings. Using simulations of problem sizes up to 40 qubits we demonstrate QAA's algorithmic performance across all possible solutions, with an emphasis on the closeness in Grover-like performance for solutions near the global optimum. We conclude with experimental demonstrations of generalized QAA on both IBMQ (superconducting) and IonQ (trapped ion) qubits, showing that the observed probabilities of each basis state match our equations as a function of varying the free parameters in the oracle and diffusion operators.

</details>


### [58] [H-EFT-VA: An Effective-Field-Theory Variational Ansatz with Provable Barren Plateau Avoidance](https://arxiv.org/abs/2601.10479)
*Eyad I. B Hamid*

Main category: quant-ph

TL;DR: H-EFT-VA ansatz prevents barren plateaus via hierarchical UV-cutoff initialization while maintaining volume-law entanglement, achieving 109x better energy convergence and 10.7x higher ground-state fidelity over standard HEA.


<details>
  <summary>Details</summary>
Motivation: Variational Quantum Algorithms (VQAs) are critically threatened by the Barren Plateau (BP) phenomenon, which causes exponentially vanishing gradients and makes optimization infeasible for large systems. Existing approaches that avoid BPs often sacrifice entanglement or expressibility, limiting their ability to represent complex quantum states.

Method: Introduces H-EFT Variational Ansatz (H-EFT-VA), an architecture inspired by Effective Field Theory (EFT). The method enforces a hierarchical "UV-cutoff" on initialization to restrict the circuit's state exploration, preventing the formation of approximate unitary 2-designs. This localization theoretically guarantees an inverse-polynomial lower bound on gradient variance.

Result: Theoretical proof shows $Var[\partial θ] \in Ω(1/poly(N))$, guaranteeing non-vanishing gradients. Extensive benchmarking across 16 experiments (including Transverse Field Ising and Heisenberg XXZ models) demonstrates: 109x improvement in energy convergence, 10.7x increase in ground-state fidelity over standard Hardware-Efficient Ansatze (HEA), with statistical significance $p < 10^{-88}$. Crucially, H-EFT-VA maintains volume-law entanglement and near-Haar purity.

Conclusion: H-EFT-VA provides a principled solution to the barren plateau problem by combining hierarchical initialization constraints with maintained expressibility. Unlike previous approaches that sacrifice entanglement, it preserves volume-law entanglement while guaranteeing polynomial lower bounds on gradients, making VQAs scalable for complex quantum systems.

Abstract: Variational Quantum Algorithms (VQAs) are critically threatened by the Barren Plateau (BP) phenomenon. In this work, we introduce the H-EFT Variational Ansatz (H-EFT-VA), an architecture inspired by Effective Field Theory (EFT). By enforcing a hierarchical "UV-cutoff" on initialization, we theoretically restrict the circuit's state exploration, preventing the formation of approximate unitary 2-designs. We provide a rigorous proof that this localization guarantees an inverse-polynomial lower bound on the gradient variance: $Var[\partial θ] \in Ω(1/poly(N))$. Crucially, unlike approaches that avoid BPs by limiting entanglement, we demonstrate that H-EFT-VA maintains volume-law entanglement and near-Haar purity, ensuring sufficient expressibility for complex quantum states. Extensive benchmarking across 16 experiments -- including Transverse Field Ising and Heisenberg XXZ models -- confirms a 109x improvement in energy convergence and a 10.7x increase in ground-state fidelity over standard Hardware-Efficient Ansatze (HEA), with a statistical significance of $p < 10^{-88}$.

</details>


### [59] [Optimized readout strategies for neutral atom quantum processors](https://arxiv.org/abs/2601.10492)
*Liang Chen,Wen-Yi Zhu,Zi-Jie Chen,Zhu-Bo Wang,Ya-Dong Hu,Qing-Xuan Jie,Guang-Can Guo,Chang-Ling Zou*

Main category: quant-ph

TL;DR: Theoretical framework for optimizing readout in neutral atom quantum processors by balancing fidelity and atomic retention, achieving quantum circuit iteration rates up to 197.2Hz.


<details>
  <summary>Details</summary>
Motivation: Neutral atom quantum processors offer scalability but face challenges in efficiently extracting readout outcomes while maintaining high system throughput for practical applications.

Method: Developed theoretical framework quantifying trade-off between readout fidelity and atomic retention, introduced quantum circuit iteration rate (qCIR) metric, used normalized quantum Fisher information to characterize performance, and demonstrated optimized readout strategy.

Result: Achieved qCIRs of 197.2Hz with single photon detectors and 154.5Hz with cameras using experimentally feasible parameters for 87Rb atoms.

Conclusion: Provides practical guidance for constructing scalable, high-throughput neutral atom quantum processors suitable for sensing, simulation, and near-term algorithm implementation.

Abstract: Neutral atom quantum processors have emerged as a promising platform for scalable quantum information processing, offering high-fidelity operations and exceptional qubit scalability. A key challenge in realizing practical applications is efficiently extracting readout outcomes while maintaining high system throughput, i.e., the rate of quantum task executions. In this work, we develop a theoretical framework to quantify the trade-off between readout fidelity and atomic retention. Moreover, we introduce a metric of quantum circuit iteration rate (qCIR) and employ normalized quantum Fisher information to characterize system overall performance. Further, by carefully balancing fidelity and retention, we demonstrate a readout strategy for optimizing information acquisition efficiency. Considering the experimentally feasible parameters for 87Rb atoms, we demonstrate that qCIRs of 197.2Hz and 154.5Hz are achievable using single photon detectors and cameras, respectively. These results provide practical guidance for constructing scalable and high-throughput neutral atom quantum processors for applications in sensing, simulation, and near-term algorithm implementation.

</details>


### [60] [A Mirror-Descent Algorithm for Computing the Petz-Rényi Capacity of Classical-Quantum Channels](https://arxiv.org/abs/2601.10558)
*Yu-Hong Lai,Hao-Chung Cheng*

Main category: quant-ph

TL;DR: Exponentiated-gradient algorithm for computing α-Rényi capacity of classical-quantum channels with convergence guarantees: global sublinear convergence and local linear convergence under certain conditions.


<details>
  <summary>Details</summary>
Motivation: Need to compute α-Rényi capacity of classical-quantum channels for α∈(0,1), requiring generalization of classical Blahut-Arimoto algorithm to quantum setting with provable convergence guarantees.

Method: Propose exponentiated-gradient (mirror descent) iteration generalizing Blahut-Arimoto algorithm, analyzed using relative smoothness with respect to entropy geometry, with convergence analysis under tangent-space nondegeneracy conditions.

Result: Established global sublinear convergence of objective values, and under natural conditions, proved local linear (geometric) convergence in Kullback-Leibler divergence on truncated probability simplex with explicit contraction factor.

Conclusion: The proposed algorithm provides efficient computation of α-Rényi capacity for classical-quantum channels with strong convergence guarantees, extending classical information theory results to quantum domain.

Abstract: We study the computation of the $α$-Rényi capacity of a classical-quantum (c-q) channel for $α\in(0,1)$. We propose an exponentiated-gradient (mirror descent) iteration that generalizes the Blahut-Arimoto algorithm. Our analysis establishes relative smoothness with respect to the entropy geometry, guaranteeing a global sublinear convergence of the objective values. Furthermore, under a natural tangent-space nondegeneracy condition (and a mild spectral lower bound in one regime), we prove local linear (geometric) convergence in Kullback-Leibler divergence on a truncated probability simplex, with an explicit contraction factor once the local curvature constants are bounded.

</details>


### [61] [Deterministic and scalable generation of large Fock states](https://arxiv.org/abs/2601.10559)
*Mo Xiong,Jize Han,Chuanzhen Cao,Jinbin Li,Qi Liu,Zhiguo Huang,Ming Xue*

Main category: quant-ph

TL;DR: Scalable protocol generates large Fock states (up to ~100 photons) with >0.9 fidelity using hybrid Genetic-Adam optimization of native control operations.


<details>
  <summary>Details</summary>
Motivation: Large Fock-number states are crucial for quantum metrology, communication, and simulation, but generating them with high fidelity at scale remains challenging despite progress with small states.

Method: Hybrid Genetic-Adam optimization framework combines global search of genetic algorithms with adaptive convergence of Adam to optimize multi-pulse control sequences using native Jaynes-Cummings interactions and displacement operations.

Result: Achieves fidelities exceeding 0.9 for Fock states up to photon numbers on the order of 100, with shallow circuit depths and strong robustness against parameter variations.

Conclusion: Establishes efficient and scalable pathway for high-fidelity non-classical state generation, enabling applications in precision metrology and fault-tolerant quantum technologies.

Abstract: The scalable and deterministic preparation of large Fock-number states represents a long-standing frontier in quantum science, with direct implications for quantum metrology, communication, and simulation. Despite significant progress in small-scale implementations, extending such state generation to large excitation numbers while maintaining high fidelity remains a formidable challenge. Here, we present a scalable protocol for generating large Fock states with fidelities exceeding 0.9 up to photon numbers on the order of 100, achieved using only native control operations and, when desired, further enhanced by an optional post-selection step. Our method employs a hybrid Genetic-Adam optimization framework that combines the global search efficiency of genetic algorithms with the adaptive convergence of Adam to optimize multi-pulse control sequences comprising Jaynes-Cummings interactions and displacement operations, both of which are native to leading experimental platforms. The resulting control protocols achieve high fidelities with shallow circuit depths and strong robustness against parameter variations. These results establish an efficient and scalable pathway toward high-fidelity non-classical state generation for precision metrology and fault-tolerant quantum technologies.

</details>


### [62] [Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders](https://arxiv.org/abs/2601.10588)
*I. K. Kominis,C. Xie,S. Li,M. Skotiniotis,G. P. Tsironis*

Main category: quant-ph

TL;DR: Proposes an information-theoretic test for nonclassicality in neural processing using autoencoders and Bell-type consistency tests in latent space.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved question of whether neural information processing involves quantum-mechanical elements, moving beyond microscopic assumptions to test the structure of neural representations directly.

Method: Uses autoencoders as a transparent model system and introduces a Bell-type consistency test in latent space, examining whether decoding statistics under multiple readout contexts can be explained by a single positive latent-variable distribution.

Result: The approach provides a model-agnostic, information-theoretic test that bypasses microscopic assumptions and probes neural representation structure directly.

Conclusion: Shifts the search for quantum-like signatures from microscopic dynamics to experimentally testable constraints on information processing, opening a new route for probing the fundamental physics of neural computation.

Abstract: Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts can be jointly explained by a single positive latent-variable distribution. By shifting the search for quantum-like signatures in neural systems from microscopic dynamics to experimentally testable constraints on information processing, this work opens a new route for probing the fundamental physics of neural computation.

</details>


### [63] [Quantum solver for single-impurity Anderson models with particle-hole symmetry](https://arxiv.org/abs/2601.10594)
*Mariia Karabin,Tanvir Sohail,Dmytro Bykov,Eduardo Antonio Coello Pérez,Swarnava Ghosh,Murali Gopalakrishnan Meena,Seongmin Kim,Amir Shehata,In-Saeng Suh,Hanna Terletska,Markus Eisenbach*

Main category: quant-ph

TL;DR: Quantum-classical hybrid solver using VQE to solve Anderson impurity models for DMFT applications, enabling Green's function reconstruction on near-term quantum devices under noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Solving the Anderson impurity model (AIM) is a computational bottleneck in dynamical mean-field theory (DMFT) for strongly correlated materials, with exact solutions being classically intractable for large bath sizes. Quantum computers offer potential advantages for this problem.

Method: Developed a quantum-classical hybrid solver using variational quantum eigensolver (VQE) with shallow circuits to prepare AIM ground state. Used unified ansatz framework to generate particle/hole excitations via parameter-shifted circuits, enabling Green's function reconstruction through continued-fraction expansion. Evaluated three optimization routines (COBYLA, Adam, L-BFGS-B) and assessed quantum-computed moment correction to variational energies.

Result: Demonstrated feasibility of Green's function reconstruction on near-term devices under noisy, shot-limited conditions. Compared reconstructed density of states against classical pipeline benchmarks. Established practical benchmarks for quantum impurity solvers in self-consistent DMFT loops.

Conclusion: The approach shows promise for quantum embedding methods in strongly correlated materials research, providing a pathway for quantum impurity solvers to be integrated into DMFT workflows on near-term quantum hardware.

Abstract: Quantum embedding methods, such as dynamical mean-field theory (DMFT), provide a powerful framework for investigating strongly correlated materials. A central computational bottleneck in DMFT is in solving the Anderson impurity model (AIM), whose exact solution is classically intractable for large bath sizes. In this work, we develop and benchmark a quantum-classical hybrid solver tailored for DMFT applications, using the variational quantum eigensolver (VQE) to prepare the ground state of the AIM with shallow quantum circuits. The solver uses a unified ansatz framework to prepare the particle and hole excitations of the ground-state from parameter-shifted circuits, enabling the reconstruction of the impurity Green's function through a continued-fraction expansion. We evaluate the performance of this approach across a few bath sizes and interaction strengths under noisy, shot-limited conditions. We compare three optimization routines (COBYLA, Adam, and L-BFGS-B) in terms of convergence and fidelity, assess the benefits of estimating a quantum-computed moment (QCM) correction to the variational energies, and benchmark the approach by comparing the reconstructed density of states (DOS) against that obtained using a classical pipeline. Our results demonstrate the feasibility of Green's function reconstruction on near-term devices and establish practical benchmarks for quantum impurity solvers embedded within self-consistent DMFT loops.

</details>


### [64] [Quantifying the properties of evolutionary quantum states of the XXZ spin model using quantum computing](https://arxiv.org/abs/2601.10650)
*M. P. Tonne,Kh. P. Gnatenko*

Main category: quant-ph

TL;DR: Study of entanglement distance and evolution speed in two-spin XXZ model systems using both analytical methods and quantum computing, showing agreement between theoretical predictions and quantum computations.


<details>
  <summary>Details</summary>
Motivation: To investigate entanglement properties and evolution dynamics in quantum two-spin systems with XXZ interactions, bridging theoretical analysis with practical quantum computing implementations.

Method: Combined analytical approach and quantum computing simulations to study entanglement distance and evolution speed in two-spin XXZ model systems, examining dependence on coupling constants and initial state parameters.

Result: Obtained explicit analytical dependencies of entanglement distance and evolution speed on coupling constants and initial state parameters, with quantum computing results showing good agreement with theoretical predictions.

Conclusion: The study successfully demonstrates the relationship between entanglement dynamics, evolution speed, and system parameters in two-spin XXZ systems, validating analytical models with quantum computing implementations.

Abstract: The entanglement distance of evolutionary quantum states of a two-spin system with the XXZ model has been studied. The analysis has been conducted both analytically and using quantum computing. An analytical dependence of the entanglement distance on the values of the model coupling constants and the parameters of the initial states has been obtained. The speed of evolution of a two-spin system has been investigated. The analysis has been performed analytically and using quantum computing. An explicit dependence of the speed of evolution on the coupling constants and on the parameters of the initial state has been obtained. The results of quantum computations are in good agreement with the theoretical predictions.

</details>


### [65] [Symmetry-based Perspectives on Hamiltonian Quantum Search Algorithms and Schrodinger's Dynamics between Orthogonal States](https://arxiv.org/abs/2601.10655)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: Constant Hamiltonians cannot achieve time-optimal evolution between orthogonal states when confined to the 2D subspace spanned by initial and final states; time-dependent Hamiltonians or higher-dimensional subspaces are required.


<details>
  <summary>Details</summary>
Motivation: To understand why continuous-time Grover's search fails when source and target states are orthogonal, and to investigate fundamental limitations of time-optimal evolution between orthogonal states under constant Hamiltonians.

Method: Employ normalization, orthogonality, and energy constraints to analyze quantum evolution constraints; demonstrate impossibility of breaching time-optimality between orthogonal states with constant Hamiltonians in 2D subspaces.

Result: Time-optimality violations for unitary evolutions between orthogonal states require either time-dependent Hamiltonians or constant Hamiltonians operating in higher-dimensional subspaces; failure is linked to inherent system symmetry.

Conclusion: The failure of analog quantum search with orthogonal states and the impossibility of sub-optimal transitions between orthogonal states with constant Hamiltonians both stem from inherent system symmetry, providing insights into time-optimal evolutions and quantum search limitations.

Abstract: It is known that the continuous-time variant of Grover's search algorithm is characterized by quantum search frameworks that are governed by stationary Hamiltonians, which result in search trajectories confined to the two-dimensional subspace of the complete Hilbert space formed by the source and target states. Specifically, the search approach is ineffective when the source and target states are orthogonal. In this paper, we employ normalization, orthogonality, and energy limitations to demonstrate that it is unfeasible to breach time-optimality between orthogonal states with constant Hamiltonians when the evolution is limited to the two-dimensional space spanned by the initial and final states. Deviations from time-optimality for unitary evolutions between orthogonal states can only occur with time-dependent Hamiltonian evolutions or, alternatively, with constant Hamiltonian evolutions in higher-dimensional subspaces of the entire Hilbert space. Ultimately, we employ our quantitative analysis to provide meaningful insights regarding the relationship between time-optimal evolutions and analog quantum search methods. We determine that the challenge of transitioning between orthogonal states with a constant Hamiltonian in a sub-optimal time is closely linked to the shortcomings of analog quantum search when the source and target states are orthogonal and not interconnected by the search Hamiltonian. In both scenarios, the fundamental cause of the failure lies in the existence of an inherent symmetry within the system.

</details>


### [66] [Counterdiabatic driving for random-gap Landau-Zener transitions](https://arxiv.org/abs/2601.10659)
*Georgios Theologou,Mikkel F. Andersen,Sandro Wimberger*

Main category: quant-ph

TL;DR: The paper develops a single control field that statistically minimizes average transition probability for an ensemble of Landau-Zener systems with distributed energy gaps, revealing a trade-off between instantaneous adiabaticity and final transition probability.


<details>
  <summary>Details</summary>
Motivation: While counterdiabatic driving (H_CD) can achieve perfect adiabaticity for individual Landau-Zener systems, the authors aim to develop a single control field that works statistically for an ensemble of LZ-type Hamiltonians with varying energy gaps, addressing practical scenarios where system parameters are not precisely known.

Method: The authors restrict attention to a special class of control fields motivated by counterdiabatic driving (H_CD). They minimize average transition probability across the ensemble, analyze limiting cases with linear sweeps analytically (including Dirac δ(t) function cases), and conduct comprehensive numerical simulations to support and extend analytical findings.

Result: The study reveals a systematic trade-off between instantaneous adiabaticity and final transition probability. The control field works best statistically rather than perfectly for individual systems. Analytical results for linear sweep limiting cases are obtained, with comprehensive numerical simulations confirming and extending these findings.

Conclusion: A single control field can be designed to statistically drive an ensemble of Landau-Zener systems with distributed energy gaps, though with inherent trade-offs between instantaneous adiabaticity and final transition probability, providing practical solutions for systems with parameter uncertainties.

Abstract: The Landau--Zener (LZ) model describes a two-level quantum system that undergoes an avoided crossing. In the adiabatic limit, the transition probability vanishes. An auxiliary control field $H_\text{CD}$ can be reverse-engineered so that the full Hamiltonian $H_0 + H_\text{CD}$ reproduces adiabaticity for all parameter values. Our aim is to construct a single control field $H_1$ that drives an ensemble of LZ-type Hamiltonians with a distribution of energy gaps. $H_1$ works best statistically, minimizing the average transition probability. We restrict our attention to a special class of $H_1$ controls, motivated by $H_\text{CD}$. We found a systematic trade-off between instantaneous adiabaticity and the final transition probability. Certain limiting cases with a linear sweep can be treated analytically; one of them being the LZ system with Dirac $δ(t)$ function. Comprehensive and systematic numerical simulations support and extend the analytic results.

</details>


### [67] [Geometric Aspects of Entanglement Generating Hamiltonian Evolutions](https://arxiv.org/abs/2601.10662)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: The paper analyzes geometric and entanglement characteristics of Hamiltonian evolutions from separable to maximally entangled two-qubit states, finding time-optimal evolutions exhibit high geodesic efficiency, zero curvature, and less average path entanglement than suboptimal ones, with nonlocality patterns differing between orthogonal and nonorthogonal state evolutions.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric properties of entanglement generation in two-qubit systems under Hamiltonian evolution, specifically examining how time-optimal versus time-suboptimal trajectories differ in terms of geometric efficiency, curvature, and entanglement metrics, and how these relate to nonlocality in unitary propagators.

Method: Analyzes stationary Hamiltonian evolutions from separable to maximally entangled two-qubit states using geometric measures (geodesic efficiency, speed efficiency, curvature coefficient) and entanglement metrics (concurrence, entanglement power, entangling capability), comparing time-optimal and time-suboptimal trajectories for both orthogonal and nonorthogonal state transitions.

Result: Time-optimal evolutions show high geodesic efficiency, zero curvature, no energy wastage, and lower average path entanglement than suboptimal evolutions. For nonorthogonal states, time-optimal evolutions exhibit greater short-time nonlocality, while for orthogonal states, suboptimal evolutions show greater nonlocality due to longer paths with smaller curvature and higher energy wastage.

Conclusion: Geometric properties of entanglement generation reveal fundamental trade-offs: time-optimal trajectories maximize efficiency with zero curvature but sacrifice some entanglement along the path, while the relationship between nonlocality and optimality depends on whether initial and final states are orthogonal or nonorthogonal, suggesting different mechanisms for achieving maximal entanglement.

Abstract: We examine the pertinent geometric characteristics of entanglement that arise from stationary Hamiltonian evolutions transitioning from separable to maximally entangled two-qubit quantum states. From a geometric perspective, each evolution is characterized by means of geodesic efficiency, speed efficiency, and curvature coefficient. Conversely, from the standpoint of entanglement, these evolutions are quantified using various metrics, such as concurrence, entanglement power, and entangling capability. Overall, our findings indicate that time-optimal evolution trajectories are marked by high geodesic efficiency, with no energy resource wastage, no curvature (i.e., zero bending), and an average path entanglement that is less than that observed in time-suboptimal evolutions. Additionally, when analyzing separable-to-maximally entangled evolutions between nonorthogonal states, time-optimal evolutions demonstrate a greater short-time degree of nonlocality compared to time-suboptimal evolutions between the same initial and final states. Interestingly, the reverse is generally true for separable-to-maximally entangled evolutions involving orthogonal states. Our investigation suggests that this phenomenon arises because suboptimal trajectories between orthogonal states are characterized by longer path lengths with smaller curvature, which are traversed with a higher energy resource wastage compared to suboptimal trajectories between nonorthogonal states. Consequently, a higher initial degree of nonlocality in the unitary time propagators appears to be essential for achieving the maximally entangled state from a separable state. Furthermore, when assessing optimal and suboptimal evolutions...

</details>


### [68] [Efficiency, Curvature, and Complexity of Quantum Evolutions for Qubits in Nonstationary Magnetic Fields](https://arxiv.org/abs/2601.10672)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: Exact analytical expression for curvature in two-level quantum systems under time-dependent magnetic fields, relating curvature to geodesic efficiency, speed efficiency, and complexity metrics.


<details>
  <summary>Details</summary>
Motivation: Realistic quantum evolutions often deviate from ideal optimal paths, exhibiting suboptimal efficiency, nonzero curvature, and high complexity. Understanding the curvature of quantum evolutions in two-level systems can provide insights into the relationship between geometric properties and efficiency/complexity metrics.

Method: Derived exact analytical expression for curvature of quantum evolution in two-level systems subjected to various time-dependent magnetic fields. Analyzed dynamics produced by two-parameter nonstationary Hermitian Hamiltonian with unit speed efficiency. Examined curvature behavior in relation to geodesic efficiency, speed efficiency, and complexity (ratio of difference between accessible and accessed Bloch-sphere volumes to accessible volume).

Result: Efficient quantum evolutions generally exhibit lower complexity compared to inefficient ones. However, complexity transcends mere path length: longer paths with sufficient curvature can demonstrate lower complexity than shorter paths with lower curvature coefficients. The curvature coefficient provides physical insights into the relationship between geometric properties and efficiency metrics.

Conclusion: Curvature analysis reveals nuanced relationships between geometric properties, efficiency, and complexity in quantum evolutions. While efficiency generally correlates with lower complexity, the geometric curvature of paths plays a crucial role in determining complexity, sometimes allowing longer but sufficiently curved paths to be less complex than shorter, less curved alternatives.

Abstract: In optimal quantum-mechanical evolutions, motion can take place along paths of minimal length within an optimal time frame. Alternatively, optimal evolutions may occur along established paths without any waste of energy resources and achieving 100% speed efficiency. Unfortunately, realistic physical scenarios often lead to less-than-ideal evolutions that demonstrate suboptimal efficiency, nonzero curvature, and a high level of complexity. In this paper, we provide an exact analytical expression for the curvature of a quantum evolution pertaining to a two-level quantum system subjected to various time-dependent magnetic fields. Specifically, we examine the dynamics produced by a two-parameter nonstationary Hermitian Hamiltonian with unit speed efficiency. To enhance our understanding of the physical implications of the curvature coefficient, we analyze the curvature behavior in relation to geodesic efficiency, speed efficiency, and the complexity of the quantum evolution (as described by the ratio of the difference between accessible and accessed Bloch-sphere volumes for the evolution from initial to final state to the accessible volume for the given quantum evolution). Our findings indicate that, generally, efficient quantum evolutions exhibit lower complexity compared to inefficient ones. However, we also note that complexity transcends mere length. In fact, longer paths that are sufficiently curved can demonstrate a complexity that is less than that of shorter paths with a lower curvature coefficient.

</details>


### [69] [Optimal lower bound for quantum channel tomography in away-from-boundary regime](https://arxiv.org/abs/2601.10683)
*Kean Chen,Zhicheng Zhang,Nengkun Yu*

Main category: quant-ph

TL;DR: Optimal query lower bound Ω(rd₁d₂/ε²) for quantum channel tomography in the away-from-boundary regime (rd₂ ≥ 2d₁), matching existing upper bound and settling query complexity for equal dimensions d₁=d₂=d with r≥2.


<details>
  <summary>Details</summary>
Motivation: To establish fundamental limits on quantum channel tomography, particularly contrasting the achievable scaling for general channels (r≥2) versus unitary channels (r=1) where Heisenberg scaling is possible.

Method: Theoretical analysis establishing query lower bounds using information-theoretic techniques for quantum channel tomography in the away-from-boundary regime where rd₂ ≥ 2d₁.

Result: Proves optimal lower bound Ω(rd₁d₂/ε²) matching existing upper bound, fully settling query complexity for equal dimensions d₁=d₂=d with r≥2, showing sharp contrast to unitary case with Heisenberg scaling Θ(d²/ε).

Conclusion: Quantum channel tomography for general channels (r≥2) requires Ω(rd₁d₂/ε²) queries, fundamentally different from unitary tomography, establishing optimal scaling in away-from-boundary regime.

Abstract: Consider quantum channels with input dimension $d_1$, output dimension $d_2$ and Kraus rank at most $r$. Any such channel must satisfy the constraint $rd_2\geq d_1$, and the parameter regime $rd_2=d_1$ is called the boundary regime. In this paper, we show an optimal query lower bound $Ω(rd_1d_2/\varepsilon^2)$ for quantum channel tomography to within diamond norm error $\varepsilon$ in the away-from-boundary regime $rd_2\geq 2d_1$, matching the existing upper bound $O(rd_1d_2/\varepsilon^2)$. In particular, this lower bound fully settles the query complexity for the commonly studied case of equal input and output dimensions $d_1=d_2=d$ with $r\geq 2$, in sharp contrast to the unitary case $r=1$ where Heisenberg scaling $Θ(d^2/\varepsilon)$ is achievable.

</details>


### [70] [Mitigating nonlinear transduction noise in high-cooperativity cavity optomechanics](https://arxiv.org/abs/2601.10689)
*Daniel Allepuz-Requena,Zohran Ali,Dennis Høj,Yingxuan Chen,Luiz Couto Correa Pinto Filho,Alexander Huck,Ulrik L. Andersen*

Main category: quant-ph

TL;DR: A nonlinear transform technique is developed to remove all orders of thermal intermodulation noise in high-cooperativity room-temperature cavity optomechanical systems, improving mechanical SNR by nearly 10 dB.


<details>
  <summary>Details</summary>
Motivation: Thermal intermodulation noise (TIN) from nonlinear mixing of thermomechanical motion increases measurement imprecision above the standard quantum limit in high-cooperativity optomechanical systems, and existing techniques only cancel TIN up to second order.

Method: Record output of a membrane-in-the-middle microcavity system operating at room temperature with high cooperativity (C > nth), then apply a nonlinear transform that removes all orders of thermal intermodulation noise.

Result: The nonlinear transform technique improves mechanical signal-to-noise ratio by nearly 10 dB by effectively removing all orders of TIN, addressing third-order TIN which dominates in high-cooperativity room-temperature systems.

Conclusion: The developed nonlinear transform method successfully eliminates all orders of thermal intermodulation noise, enabling improved displacement measurements in high-cooperativity room-temperature cavity optomechanical systems where third-order TIN is the dominant intrinsic noise source.

Abstract: Coupling mechanical motion to an optical resonator enables displacement measurements approaching the standard quantum limit (SQL). However, increasing the optomechanical coupling strength will inevitably lead to probing of the nonlinear response of the optical resonator. Thermal intermodulation noise (TIN) arising from the nonlinear mixing of thermomechanical motion can further increase the imprecision well above the SQL and has hitherto been canceled up to second order of nonlinearity via operation at the "magic detuning". In this work, we record the output of a membrane-in-the-middle microcavity system operating at room temperature and achieving high cooperativity, $C>n_\text{th}$, and apply a nonlinear transform that removes all orders of TIN, improving the mechanical signal-to-noise ratio by nearly 10 dB. Our results can be applied to experiments affected by third-order TIN, which we expect to be the dominating intrinsic source of noise in high-cooperativity room-temperature cavity optomechanical systems.

</details>


### [71] [Constant-Depth Unitary Preparation of Dicke States](https://arxiv.org/abs/2601.10693)
*Francisca Vasconcelos,Malvika Raj Joshi*

Main category: quant-ph

TL;DR: First unitary constant-depth protocols for exact Dicke state preparation using global interactions beyond standard circuit models.


<details>
  <summary>Details</summary>
Motivation: Dicke states are crucial for quantum applications, but existing preparation methods are limited to logarithmic depth in standard circuits or require measurement and feed-forward, creating a need for more efficient unitary preparation.

Method: Overcome logarithmic-depth barrier by moving beyond standard circuit model and leveraging global interactions native to architectures like neutral atoms and trapped ions. Use unbounded CZ gates (QAC⁰ circuit class) for constant-weight Dicke states and quantum FAN-OUT operation (QAC_f⁰ circuit class) for arbitrary-weight Dicke states.

Result: Achieve exact computation of constant-weight Dicke states using polynomial ancillae, approximation of weight-1 Dicke states (W states) using constant ancillae, and exact preparation of arbitrary-weight Dicke states with polynomial ancillae when granted quantum FAN-OUT operation.

Conclusion: These protocols demonstrate constant-depth capabilities of quantum architectures based on connectivity and offer a novel approach to resolving a long-standing quantum complexity conjecture.

Abstract: Dicke states serve as a critical resource in quantum metrology, communication, and computation. However, unitary preparation of Dicke states is limited to logarithmic depth in standard circuit models and existing constant-depth protocols require measurement and feed-forward. In this work, we present the first unitary, constant-depth protocols for exact Dicke state preparation. We overcome the logarithmic-depth barrier by moving beyond the standard circuit model and leveraging global interactions (native to architectures such as neutral atoms and trapped ions). Specifically, utilizing unbounded CZ gates (i.e. within the QAC$^0$ circuit class), we offer circuits for exact computation of constant-weight Dicke states, using polynomial ancillae, and approximation of weight-1 Dicke states (i.e. $W$ states), using only constant ancillae. Granted additional access to the quantum FAN-OUT operation (i.e. upgrading to the QAC$_f^0$ circuit class), we also achieve exact preparation of arbitrary-weight Dicke states, with polynomial ancillae. These protocols distinguish the constant-depth capabilities of quantum architectures based on connectivity and offer a novel path toward resolving a long-standing quantum complexity conjecture.

</details>


### [72] [Madelung hydrodynamics of spin-orbit coupling: action principles, currents, and correlations](https://arxiv.org/abs/2601.10698)
*Cesare Tronci*

Main category: quant-ph

TL;DR: The paper analyzes spin-orbit coupling in quantum hydrodynamics by identifying distinct SOC-induced quantum forces on orbital trajectories and revealing two mechanisms for quantum spin-orbit correlations, while also elucidating spin transport features like spin Hall effect and quantum torques.


<details>
  <summary>Details</summary>
Motivation: To understand the correlation and torque mechanisms accompanying spin-orbit coupling in electronic motion by exploiting the variational and Hamiltonian structures of quantum hydrodynamics with spin, and to clarify previously overlooked contributions to spin current and forces.

Method: Using Hamilton's action principle for the Pauli equation to isolate SOC-induced quantum forces on orbital Madelung-Bohm trajectories, distinguishing between spin-hydrodynamic forces related to quantum geometric tensor and SOC-induced orbital forces from a particular current operator. Leveraging Hamiltonian structure to analyze spin transport features.

Result: Identification of distinct SOC-induced quantum forces that complement known spin-hydrodynamic forces, revelation of two fundamentally different mechanisms generating quantum spin-orbit correlations, elucidation of spin transport features including current shift in spin Hall effect and correlation-induced quantum torques, and development of Madelung-Rashba equations for planar SOC configurations with proposed particle-based numerical scheme.

Conclusion: The variational and Hamiltonian approach to quantum hydrodynamics with spin provides a comprehensive framework for understanding spin-orbit coupling mechanisms, revealing previously overlooked contributions to forces and correlations, and enabling analysis of spin transport phenomena with practical implications for numerical implementation.

Abstract: We exploit the variational and Hamiltonian structures of quantum hydrodynamics with spin to unfold the correlation and torque mechanisms accompanying spin-orbit coupling (SOC) in electronic motion. Using Hamilton's action principle for the Pauli equation, we isolate SOC-induced quantum forces that act on the orbital Madelung--Bohm trajectories and complement the usual force terms known to appear in quantum hydrodynamics with spin. While the latter spin-hydrodynamic forces relate to the quantum geometric tensor (QGT), SOC-induced orbital forces originate from a particular current operator that contributes prominently to the spin current and whose contribution was overlooked in the past. The distinction between different force terms reveals two fundamentally different mechanisms generating quantum spin-orbit correlations. Leveraging the Hamiltonian structure of the hydrodynamic system, we also elucidate spin transport features such as the current shift in the spin Hall effect and the correlation-induced quantum torques. Finally, we illustrate the framework via the Madelung--Rashba equations for planar SOC configurations and propose a particle-based scheme for numerical implementation.

</details>


### [73] [Scalable Spin Squeezing in Power-Law Interacting XXZ Models with Disorder](https://arxiv.org/abs/2601.10703)
*Samuel E. Begg,Bishal K. Ghosh,Chong Zu,Chuanwei Zhang,Michael Kolodrubetz*

Main category: quant-ph

TL;DR: Spin squeezing remains scalable in 2D lattices with power-law interactions despite positional disorder, up to a threshold where squeezing advantage is lost.


<details>
  <summary>Details</summary>
Motivation: Recent experiments (Wu et al. Nature 2025) showed that spin squeezing in NV centers is heavily affected by positional disorder, reducing practical squeezing advantages that require scalability. This work explores whether scalable spin squeezing can survive in disordered systems.

Method: Semi-classical modeling of two-dimensional lattices with power-law interacting XXZ models containing a fraction of unoccupied lattice sites (positional disorder).

Result: Demonstrates existence of scalable squeezing up to a disorder threshold, beyond which squeezing is not scalable. Produces a phase diagram for scalable squeezing and explains its absence in the NV experiment.

Conclusion: Identifies maximum disorder tolerance for scalable spin squeezing in quantum simulators, highlights regimes with substantial disorder tolerance, and suggests controlled defect creation as a promising route for scalable squeezing in solid-state systems.

Abstract: While spin squeezing has been traditionally considered in all-to-all interacting models, recent works have shown that spin squeezing can occur in systems with power-law interactions, leading to direct testing in Rydberg atoms, trapped ions, ultracold atoms and nitrogen vacancy (NV) centers in diamond. For the latter, Wu. et al. Nature 646 (2025) demonstrated that spin squeezing is heavily affected by positional disorder, reducing any capacity for a practical squeezing advantage, which requires scalability with the system size. In this Letter we explore the robustness of spin-squeezing in two-dimensional lattices with a fraction of unoccupied lattice sites. Using semi-classical modeling, we demonstrate the existence of scalable squeezing in power-law interacting XXZ models up to a disorder threshold, above which squeezing is not scalable. We produce a phase diagram for scalable squeezing, and explain its absence in the aforementioned NV experiment. Our work illustrates the maximum disorder allowed for realizing scalable spin squeezing in a host of quantum simulators, highlights a regime with substantial tolerance to disorder, and identifies controlled defect creation as a promising route for scalable squeezing in solid-state systems.

</details>


### [74] [Quantum Maxwell Erasure Decoder for qLDPC codes](https://arxiv.org/abs/2601.10713)
*Bruno Costa Alves Freire,François-Marie Le Régent,Anthony Leverrier*

Main category: quant-ph

TL;DR: Quantum Maxwell erasure decoder for CSS qLDPC codes extends peeling with bounded symbolic guessing, offering tunable complexity-performance tradeoff via guessing budget.


<details>
  <summary>Details</summary>
Motivation: Need efficient decoders for CSS quantum LDPC codes that can bridge the gap between linear-time decoding and optimal Maximum-Likelihood performance.

Method: Extends peeling decoder with bounded symbolic guessing; tracks guesses symbolically and eliminates them via restrictive checks; uses tunable guessing budget parameter.

Result: Theoretical asymptotic performance guarantees; strong performance demonstrated on bivariate bicycle and quantum Tanner codes; unconstrained budget recovers ML performance, constant budget yields linear-time decoding approximating ML.

Conclusion: Quantum Maxwell erasure decoder provides flexible tradeoff between complexity and performance for CSS qLDPC codes, enabling practical near-ML decoding with controllable computational cost.

Abstract: We introduce a quantum Maxwell erasure decoder for CSS quantum low-density parity-check (qLDPC) codes that extends peeling with bounded guessing. Guesses are tracked symbolically and can be eliminated by restrictive checks, giving a tunable tradeoff between complexity and performance via a guessing budget: an unconstrained budget recovers Maximum-Likelihood (ML) performance, while a constant budget yields linear-time decoding and approximates ML. We provide theoretical guarantees on asymptotic performance and demonstrate strong performance on bivariate bicycle and quantum Tanner codes.

</details>
