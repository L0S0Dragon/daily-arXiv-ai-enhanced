{"id": "2601.00220", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00220", "abs": "https://arxiv.org/abs/2601.00220", "authors": ["Bibek Saha", "Sthitadhi Roy"], "title": "Anderson localisation in spatially structured random graphs", "comment": "18 pages, 12 figures", "summary": "We study Anderson localisation on high-dimensional graphs with spatial structure induced by long-ranged but distance-dependent hopping. To this end, we introduce a class of models that interpolate between the short-range Anderson model on a random regular graph and fully connected models with statistically uniform hopping, by embedding a random regular graph into a complete graph and allowing hopping amplitudes to decay exponentially with graph distance. The competition between the exponentially growing number of neighbours with graph distance and the exponentially decaying hopping amplitude positions our models effectively as power-law hopping generalisation of the Anderson model on random regular graphs. Using a combination of numerical exact diagonalisation and analytical renormalised perturbation theory, we establish the resulting localisation phase diagram emerging from the interplay of the lengthscale associated to the hopping range and the onsite disorder strength. We find that increasing the hopping range shifts the localisation transition to stronger disorder, and that beyond a critical range the localised phase ceases to exist even at arbitrarily strong disorder. Our results indicate a direct Anderson transition between delocalised and localised phases, with no evidence for an intervening multifractal phase, for both deterministic and random hopping models. A scaling analysis based on inverse participation ratios reveals behaviour consistent with a Kosterlitz-Thouless-like transition with two-parameter scaling, in line with Anderson transitions on high-dimensional graphs. We also observe distinct critical behaviour in average and typical correlation functions, reflecting the different scaling properties of generalised inverse participation ratios.", "AI": {"tldr": "Study of Anderson localization on high-dimensional graphs with distance-dependent hopping, showing that increasing hopping range shifts localization transition to stronger disorder and can eliminate localized phase entirely beyond critical range.", "motivation": "To understand Anderson localization in high-dimensional graphs with spatial structure and long-range hopping, bridging between short-range models on random regular graphs and fully connected models with uniform hopping.", "method": "Introduce models interpolating between short-range Anderson model on random regular graphs and fully connected models by embedding random regular graphs into complete graphs with exponentially decaying hopping amplitudes. Use numerical exact diagonalization and analytical renormalized perturbation theory.", "result": "Increasing hopping range shifts localization transition to stronger disorder; beyond critical range, localized phase ceases to exist even at arbitrarily strong disorder. Direct Anderson transition between delocalized and localized phases with no intervening multifractal phase. Scaling analysis shows Kosterlitz-Thouless-like transition with two-parameter scaling.", "conclusion": "The interplay between hopping range and disorder strength creates rich localization phase diagrams on high-dimensional graphs, with hopping range fundamentally altering localization properties and potentially eliminating localized phases entirely."}}
{"id": "2601.00062", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2601.00062", "abs": "https://arxiv.org/abs/2601.00062", "authors": ["Haowei Fan", "Vladimir Fal'ko", "Xiao Li"], "title": "Classical vs quantum dynamics and the onset of chaos in a macrospin system", "comment": "32 pages, 17 figures. Comments are welcome", "summary": "We study a periodically driven macrospin system with anisotropic long-range interactions and collective dissipation, described by a Lindblad master equation. In the thermodynamic limit ($N\\to\\infty$), a mean-field treatment yields classical equations of motion, whose dynamics are characterized via the maximal Lyapunov exponent (MLE). Focusing on the thermodynamic limit, we map out chaotic, quasiperiodic, and periodic phases via bifurcation diagrams, MLEs, and Fourier spectra of evolved observables, identifying classic period-doubling bifurcations and fractal boundaries in the regions of attractors. Finite-size quantum simulations in the Dicke basis reveal that while both quantum and classical systems exhibit diverse dynamical phases, finite-size effects suppress some behaviors present in the thermodynamic limit. The sign of $\u03bb_{\\mathrm{max}}$ serves as a key indicator of convergence between quantum and classical dynamics, which agree over timescales up to the Lyapunov time. Analysis of the density matrix shows that convergence occurs only when its nonzero elements are sharply localized. However, the nonconvergence does not imply a fundamental difference between quantum and classical dynamics: in chaotic regimes, although the evolution orbits of quantum and classical systems show significant differences, quantum evolution becomes mixed and diffusively explores the Hilbert space, signaling quantum chaos, which can be confirmed by the delocalized nature of the density matrix.", "AI": {"tldr": "Periodically driven macrospin system with anisotropic long-range interactions and collective dissipation shows chaotic, quasiperiodic, and periodic phases in thermodynamic limit; quantum-classical convergence occurs up to Lyapunov time when density matrix is localized, while quantum chaos manifests as diffusive Hilbert space exploration.", "motivation": "To understand the relationship between quantum and classical dynamics in driven dissipative systems with long-range interactions, particularly how finite-size quantum systems compare to their thermodynamic limit classical counterparts, and to identify conditions for quantum-classical convergence.", "method": "Mean-field treatment in thermodynamic limit yields classical equations of motion analyzed via maximal Lyapunov exponent (MLE), bifurcation diagrams, and Fourier spectra. Finite-size quantum simulations performed in Dicke basis with analysis of density matrix localization and Hilbert space exploration.", "result": "Classical system exhibits chaotic, quasiperiodic, and periodic phases with period-doubling bifurcations and fractal attractor boundaries. Quantum-classical convergence occurs up to Lyapunov time when density matrix elements are sharply localized. In chaotic regimes, quantum evolution becomes mixed and diffusively explores Hilbert space, indicating quantum chaos despite orbital differences from classical dynamics.", "conclusion": "Quantum and classical dynamics show agreement over Lyapunov timescales when density matrix is localized, but nonconvergence doesn't imply fundamental difference - quantum chaos manifests as diffusive Hilbert space exploration. Finite-size effects suppress some thermodynamic limit behaviors, and MLE sign serves as key indicator of quantum-classical convergence."}}
{"id": "2601.00010", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00010", "abs": "https://arxiv.org/abs/2601.00010", "authors": ["Luigi E. Picasso"], "title": "On the measurement problem in quantum mechanics: a simple proposal", "comment": "8 pages, 0 figures", "summary": "Some of the problems connected with the interpretation of quantum mechanics are enumerated, in particular those related to some well known paradoxes and, above all, to the measurement process. We then show how the so called \"Physics Laboratory Assumption\" introduced in [1], which considers as \"observables'' only the self-adjoint operators corresponding to existing measuring instruments, can propose a new perspective on the aforementioned problems and can replace the wavefunction collapse postulate.\n  [1] Luigi E. Picasso, \"On the Concept of State in Quantum Mechanics: Another Way to Decoherence?'' Int. J. Theor. Phys. 62 (2), (2023)", "AI": {"tldr": "The paper proposes using the \"Physics Laboratory Assumption\" (only considering as observables the self-adjoint operators corresponding to existing measuring instruments) to address quantum mechanics interpretation problems, particularly measurement paradoxes, potentially replacing the wavefunction collapse postulate.", "motivation": "To address persistent problems in quantum mechanics interpretation, especially those related to well-known paradoxes and the measurement process, by offering an alternative to the problematic wavefunction collapse postulate.", "method": "Introduces and applies the \"Physics Laboratory Assumption\" from a previous paper, which restricts observables to only those self-adjoint operators that correspond to actually existing measuring instruments in physical laboratories.", "result": "The approach provides a new perspective on quantum mechanics interpretation problems and offers a potential replacement for the wavefunction collapse postulate by grounding observables in actual experimental reality.", "conclusion": "The Physics Laboratory Assumption offers a promising framework for resolving quantum measurement paradoxes by connecting mathematical formalism directly to physical measurement apparatus, potentially eliminating the need for the problematic collapse postulate."}}
{"id": "2601.00064", "categories": ["quant-ph", "cond-mat.str-el", "hep-th", "math.QA"], "pdf": "https://arxiv.org/pdf/2601.00064", "abs": "https://arxiv.org/abs/2601.00064", "authors": ["Yitao Feng", "Hanyu Xue", "Ryohei Kobayashi", "Po-Shen Hsin", "Yu-An Chen"], "title": "Pauli stabilizer formalism for topological quantum field theories and generalized statistics", "comment": "48 pages, 2 figures", "summary": "Topological quantum field theory (TQFT) provides a unifying framework for describing topological phases of matter and for constructing quantum error-correcting codes, playing a central role across high-energy physics, condensed matter, and quantum information. A central challenge is to formulate topological order on the lattice and to extract the properties of topological excitations from microscopic Hamiltonians. In this work, we construct new classes of lattice gauge theories as Pauli stabilizer models, realizing a wide range of TQFTs in general spacetime dimensions. We develop a lattice description of the resulting extended excitations and systematically determine their generalized statistics.\n  Our main example is the $(4+1)$D \\emph{fermionic-loop toric code}, obtained by condensing the $e^2 m^2$-loop in the $(4+1)$D $\\mathbb{Z}_4$ toric code. We show that the loop excitation exhibits fermionic loop statistics: the 24-step loop-flipping process yields a phase of $-1$. Our Pauli stabilizer models realize all twisted 2-form gauge theories in $(4+1)$D, the higher-form Dijkgraaf-Witten TQFT classified by $H^{5}(B^{2}G, U(1))$. % Beyond $(4+1)$D, the fermionic-loop toric codes form a family of $\\mathbb{Z}_2$ topological orders in arbitrary dimensions featuring fermionic loop excitations, realized as explicit Pauli stabilizer codes using $\\mathbb{Z}_4$ qudits. % Finally, we develop a Pauli-based framework that defines generalized statistics for extended excitations in any dimension, yielding computable lattice unitary processes to detect nontrivial generalized statistics. For example, we propose anyonic membrane statistics in $(6+1)$D, as well as fermionic membrane and volume statistics in arbitrary dimensions. We construct new families of $\\mathbb{Z}_2$ topological orders: the \\emph{fermionic-membrane toric code} and the \\emph{fermionic-volume toric code}.", "AI": {"tldr": "Construction of new lattice gauge theories as Pauli stabilizer models realizing various TQFTs in general dimensions, featuring fermionic loop, membrane, and volume excitations with computable generalized statistics.", "motivation": "To formulate topological order on the lattice and extract properties of topological excitations from microscopic Hamiltonians, bridging TQFT, topological phases of matter, and quantum error-correcting codes.", "method": "Construct lattice gauge theories as Pauli stabilizer models, develop lattice description of extended excitations, systematically determine generalized statistics using Pauli-based framework with computable lattice unitary processes.", "result": "Realized all twisted 2-form gauge theories in (4+1)D (higher-form Dijkgraaf-Witten TQFTs), constructed fermionic-loop toric code in (4+1)D with fermionic loop statistics (-1 phase from 24-step process), and proposed new families of Z2 topological orders: fermionic-membrane and fermionic-volume toric codes in arbitrary dimensions.", "conclusion": "Developed systematic framework for constructing lattice realizations of TQFTs and characterizing generalized statistics of extended excitations, enabling exploration of exotic topological orders in higher dimensions with potential applications to quantum information and condensed matter physics."}}
{"id": "2601.00077", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00077", "abs": "https://arxiv.org/abs/2601.00077", "authors": ["Tailan S. Sarubi", "Santiago Zamora", "Mois\u00e9s Alves", "Vin\u00edcius F. Alves", "Gandhi Viswanathan", "Rafael Chaves"], "title": "Detection Efficiency Bounds in (Semi-)Device-Independent Scenarios", "comment": null, "summary": "This article provides a comprehensive review of the critical role of detection efficiency in demonstrating non-classicality across various device-independent and semi-device-independent scenarios. The central focus is the detection loophole, a challenge in which imperfect detectors can allow classical hidden variable models to mimic quantum correlations, thus masking genuine non-classicality. As a review, the article revisits the paradigmatic Bell scenario, detailing the efficiency requirements for the CHSH inequality, such as the 2/3 threshold for symmetric efficiencies, and traces the historical trajectory toward the first loophole-free tests. The analysis extends to other causal structures to explore how efficiency requirements are affected in different contexts. These include the instrumental scenario, which for binary variables has recently been shown to follow the same inefficiency bounds as the bipartite dichotomic Bell scenario; the prepare-and-measure scenario, where inefficiencies impact the certification of a quantum system's dimension and create security breaches in protocols such as Quantum Key Distribution (QKD); and the bilocality scenario, which exemplifies how employing multiple independent sources can significantly relax the required efficiencies to certify non-classical correlations.", "AI": {"tldr": "Review of detection efficiency requirements for demonstrating non-classicality across device-independent and semi-device-independent scenarios, focusing on the detection loophole and efficiency thresholds in various causal structures.", "motivation": "To comprehensively analyze how imperfect detectors create the detection loophole, allowing classical models to mimic quantum correlations and mask genuine non-classicality, necessitating specific efficiency requirements for reliable demonstrations.", "method": "Review approach analyzing paradigmatic Bell scenarios (CHSH inequality with 2/3 symmetric efficiency threshold), historical trajectory toward loophole-free tests, and extension to other causal structures including instrumental, prepare-and-measure, and bilocality scenarios.", "result": "Identifies specific efficiency requirements: 2/3 threshold for symmetric efficiencies in CHSH inequality; instrumental scenario follows same bounds as bipartite dichotomic Bell; inefficiencies impact dimension certification and create security breaches in QKD; bilocality scenario shows relaxed efficiency requirements with multiple independent sources.", "conclusion": "Detection efficiency plays a critical role in demonstrating non-classicality across various scenarios, with different causal structures imposing distinct efficiency requirements, and employing multiple independent sources can significantly relax these requirements for certifying non-classical correlations."}}
{"id": "2601.00078", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00078", "abs": "https://arxiv.org/abs/2601.00078", "authors": ["Nicolas Zapata", "Najmeh Etehadi Abari", "Mitchell Field", "Patrick Winkel", "Simon Geisert", "Soeren Ihssen", "Anja Metelmann", "Ioan M. Pop"], "title": "Double-Pumped Kerr Parametric Amplifier Beyond the Gain-Bandwidth Limit", "comment": null, "summary": "Superconducting standing$-$wave parametric amplifiers are crucial for the readout of microwave quantum devices. Despite significant improvements in recent years, the need to operate near an instability point imposes a fundamental constraint: the instantaneous bandwidth decreases with increasing amplifier gain. Here we show that it is possible to obtain parametric amplification without instability by using two simultaneous drives that activate phase-preserving gain and frequency conversion. Realized in a granular aluminum dimer with Kerr nonlinearity, our method demonstrates a sixfold bandwidth increase at 20 dB gain, surpasses the conventional gain$-$bandwidth scaling up to 25 dB, and remains near the quantum limit.", "AI": {"tldr": "Researchers demonstrate a new parametric amplification method using dual drives that avoids instability, achieving 6\u00d7 bandwidth increase at 20 dB gain while maintaining quantum-limited performance.", "motivation": "Conventional superconducting parametric amplifiers face a fundamental limitation: operating near instability points causes bandwidth to decrease with increasing gain, creating a trade-off that limits their performance for microwave quantum device readout.", "method": "The researchers use two simultaneous drives that activate both phase-preserving gain and frequency conversion in a granular aluminum dimer with Kerr nonlinearity, enabling parametric amplification without instability.", "result": "The method achieves a sixfold bandwidth increase at 20 dB gain, surpasses conventional gain-bandwidth scaling up to 25 dB, and maintains operation near the quantum limit.", "conclusion": "This dual-drive approach overcomes the fundamental instability constraint of conventional parametric amplifiers, offering significantly improved bandwidth while preserving quantum-limited performance for microwave quantum device readout."}}
{"id": "2601.00111", "categories": ["quant-ph", "cond-mat.other", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00111", "abs": "https://arxiv.org/abs/2601.00111", "authors": ["J. Eisert"], "title": "A compellingly simple proof of the speed of sound for interacting bosons", "comment": "4 pages, 1 figure", "summary": "On physical grounds, one expects locally interacting quantum many-body systems to feature a finite group velocity. This intuition is rigorously underpinned by Lieb-Robinson bounds that state that locally interacting Hamiltonians with finite-dimensional constituents on suitably regular lattices always exhibit such a finite group velocity. This also implies that causality is always respected by the dynamics of quantum lattice models. It had been a long-standing open question whether interacting bosonic systems also feature finite speeds of sound in information and particle propagation, which was only recently resolved. This work proves a strikingly simple such bound for particle propagation - shown in literally a few elementary, yet not straightforward, lines - for generalized Bose-Hubbard models defined on general lattices, proving that appropriately locally perturbed stationary states feature a finite speed of sound in particle numbers.", "AI": {"tldr": "The paper proves a simple Lieb-Robinson type bound for particle propagation in generalized Bose-Hubbard models, establishing finite speed of sound in locally perturbed stationary states.", "motivation": "To resolve the long-standing open question of whether interacting bosonic systems exhibit finite speeds of sound in information and particle propagation, extending Lieb-Robinson bounds from spin systems to bosonic lattice models.", "method": "Uses a strikingly simple proof approach requiring only a few elementary (though non-straightforward) lines to establish a Lieb-Robinson type bound for generalized Bose-Hubbard models on general lattices.", "result": "Proves that appropriately locally perturbed stationary states in generalized Bose-Hubbard models feature a finite speed of sound in particle numbers, establishing rigorous bounds on particle propagation.", "conclusion": "The work provides a simple yet rigorous proof that bosonic lattice systems respect causality through finite propagation speeds, resolving a fundamental question in quantum many-body physics and extending Lieb-Robinson bounds to bosonic systems."}}
{"id": "2601.00128", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.00128", "abs": "https://arxiv.org/abs/2601.00128", "authors": ["T. Rick Perche"], "title": "(PhD Thesis) The Information Locally Stored in Quantum Fields: From Entanglement to Gravity", "comment": "271 pages + appendices", "summary": "This is an updated version of my PhD thesis, defended at the University of Waterloo on the 2nd of April 2025, uploaded to the ArXiv with the goal of reaching a wider audience. The thesis is divided into 5 chapters, respectively containing (I) a brief introduction to local quantum field theory (QFT), (II) a description of local probes in QFT, (III) a discussion of entanglement in QFT and how to probe it, (IV) a description of the regimes where QFT interactions can be approximated by direct interactions, and (V) a discussion the information about the geometry of spacetime contained in quantum fields. The partial goal of this thesis is to serve as a guide for students aiming to tackle these different research programs. If the reader is interested in pursuing one or more research projects detailed here, they are encouraged to contact me for collaboration in these topics.", "AI": {"tldr": "Updated PhD thesis on quantum field theory covering local probes, entanglement, interaction approximations, and spacetime geometry information, serving as a guide for students interested in these research areas.", "motivation": "To disseminate PhD research to a wider audience and serve as a comprehensive guide for students interested in quantum field theory research programs, particularly in local probes, entanglement, interaction approximations, and spacetime geometry information.", "method": "Thesis organized into 5 chapters: (I) introduction to local QFT, (II) local probes in QFT, (III) entanglement in QFT and probing methods, (IV) regimes where QFT interactions approximate direct interactions, and (V) information about spacetime geometry contained in quantum fields.", "result": "The thesis provides a structured framework for understanding key aspects of quantum field theory including local observables, entanglement properties, interaction approximations, and geometric information extraction from quantum fields.", "conclusion": "The work serves as both a research summary and educational guide, offering students pathways into specific QFT research programs while inviting collaboration opportunities in these advanced topics."}}
{"id": "2601.00157", "categories": ["quant-ph", "physics.app-ph", "physics.atom-ph", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2601.00157", "abs": "https://arxiv.org/abs/2601.00157", "authors": ["Sean Lourette", "Andrey Jarmola", "Jabir Chathanathil", "Victor M. Acosta", "A. Glen Birdwell", "Peter Bl\u00fcmler", "Dmitry Budker", "Sebasti\u00e1n C. Carrasco", "Tony G. Ivanov", "Shimon Kolkowitz", "Vladimir S. Malinovsky"], "title": "Towards a temperature-insensitive composite diamond clock", "comment": "14 pages, 6 figures", "summary": "Frequency references based on solid state spins promise simplicity, compactness, robustness, multifunctionality, ease of integration, and high densities of emitters. Nitrogen-vacancy (NV) centers in diamond are a natural candidate, but the electronic zero-field splitting exhibits a large fractional temperature dependence, which has precluded its use as a stable clock transition. Here we show that this limitation can be overcome by forming a composite frequency reference that combines measurements of the electronic splitting D with the nuclear quadrupole splitting of the $^{14}$N nuclear spin intrinsic to the NV center. We further benchmark this composite approach against alternative strategies for mitigating temperature sensitivity. By implementing a specially designed pulse sequence with an eight-phase control scheme that suppresses pulse imperfections, we interleave measurements of D and Q in a high-density NV ensemble and demonstrate a temperature-compensated composite frequency reference. The stability of this composite diamond clock is characterized over a 10-day period at room temperature through a comparison to a Rb vapor-cell clock, yielding a fractional instability below $5 \\times 10^{-9}$ for an averaging time of $\u03c4= 200$ s and below $1 \\times 10^{-8}$ at $\u03c4= 2 \\times 10^5$ s, corresponding to measured improvements by a factor of 4 and 200, respectively, over a clock based purely on the single frequency D for the same periods. By characterizing the residual sensitivity to magnetic fields, optical power, and radio-frequency drive amplitudes, we find that temperature is no longer the dominant source of instability. These results establish complementary electron- and nuclear-spin transitions in diamond as a viable route to thermally robust frequency metrology, providing a pathway toward compact, multifunctional solid-state clocks and quantum sensors.", "AI": {"tldr": "Composite frequency reference using NV center's electronic splitting D and nuclear quadrupole splitting Q achieves temperature compensation, improving stability by factors of 4-200 over D-only clocks.", "motivation": "NV centers in diamond are promising for compact solid-state frequency references but suffer from large temperature dependence of electronic zero-field splitting D, preventing stable clock use.", "method": "Form composite frequency reference combining electronic splitting D with nuclear quadrupole splitting Q of intrinsic \u00b9\u2074N nuclear spin. Implement eight-phase control pulse sequence to suppress imperfections, interleaving D and Q measurements in high-density NV ensemble.", "result": "Demonstrated temperature-compensated composite clock with fractional instability below 5\u00d710\u207b\u2079 at \u03c4=200s and below 1\u00d710\u207b\u2078 at \u03c4=2\u00d710\u2075s, improving over D-only clock by factors of 4 and 200 respectively. Temperature is no longer dominant instability source.", "conclusion": "Complementary electron- and nuclear-spin transitions in diamond enable thermally robust frequency metrology, providing pathway to compact, multifunctional solid-state clocks and quantum sensors."}}
{"id": "2601.00198", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00198", "abs": "https://arxiv.org/abs/2601.00198", "authors": ["Keyi Huang", "Qi Zhang", "Xiangjing Liu", "Ruiqing Li", "Xinyue Long", "Hongfeng Liu", "Xiangyu Wang", "Yu-ang Fan", "Yuxuan Zheng", "Yufang Feng", "Yu Zhou", "Jack Ng", "Xinfang Nie", "Zhong-Xiao Man", "Dawei Lu"], "title": "Reversing Heat Flow by Coherence in a Multipartite Quantum System", "comment": "6+18 pages, Comments are welcome!", "summary": "The second law of thermodynamics dictates that heat flows spontaneously from a high-temperature entity to a lower-temperature one. Yet, recent advances have demonstrated that quantum correlations between a system and its thermal environment can induce a reversal of heat flow, challenging classical thermodynamic expectations. Here, we experimentally demonstrate that internal quantum coherence in a multipartite spin system can also reverse heat flow, without relying on initial correlations with the environment. Under the collision model with cascade interaction, we verify that both the strength and the phase of the coherence term determine the direction and magnitude of energy transfer. These results enable precise control of heat flow using only local quantum properties.", "AI": {"tldr": "Experimental demonstration that internal quantum coherence in a multipartite spin system can reverse heat flow without initial system-environment correlations, enabling precise control of heat flow using local quantum properties.", "motivation": "The second law of thermodynamics dictates spontaneous heat flow from hot to cold, but recent quantum thermodynamics research shows quantum correlations can reverse this flow. This work investigates whether internal quantum coherence alone (without system-environment correlations) can also induce heat flow reversal.", "method": "Experimental demonstration using a multipartite spin system under a collision model with cascade interaction. The researchers verify that both the strength and phase of the coherence term determine the direction and magnitude of energy transfer.", "result": "Demonstrated that internal quantum coherence can reverse heat flow without relying on initial correlations with the environment. Both coherence strength and phase control the direction and magnitude of energy transfer.", "conclusion": "Internal quantum coherence provides a mechanism for precise control of heat flow using only local quantum properties, expanding the toolbox for quantum thermodynamics beyond system-environment correlations."}}
{"id": "2601.00214", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00214", "abs": "https://arxiv.org/abs/2601.00214", "authors": ["Yecheng Xue", "Rui Yang", "Zhiding Liang", "Tongyang Li"], "title": "DC-MBQC: A Distributed Compilation Framework for Measurement-Based Quantum Computing", "comment": "14 pages, 10 figures. To appear in the IEEE International Symposium on High-Performance Computer Architecture (HPCA) 2026", "summary": "Distributed quantum computing (DQC) is a promising technique for scaling up quantum systems. While significant progress has been made in DQC for quantum circuit models, there exists much less research on DQC for measurement-based quantum computing (MBQC), which is a universal quantum computing model that is essentially different from the circuit model and particularly well-suited to photonic quantum platforms. In this paper, we propose DC-MBQC, the first distributed quantum compilation framework tailored for MBQC. We identify and address two key challenges in enabling DQC for MBQC. First, for task allocation among quantum processing units (QPUs), we develop an adaptive graph partitioning algorithm that preserves the structure of the graph state while balancing the workload across QPUs. Second, for inter-QPU communication, we introduce the layer scheduling problem and propose an algorithm to solve it. Regrading realistic hardware requirements, we optimize the execution time of running quantum programs and the corresponding required photon lifetime to avoid fatal failures caused by photon loss. Our experiments demonstrate a $7.46\\times$ improvement on required photon lifetime and $6.82\\times$ speedup with 8 fully-connected QPUs, which further confirm the advantage of distributed quantum computing in photonic systems. The source code is publicly available at https://github.com/qfcwj/DC-MBQC.", "code_url": "https://github.com/qfcwj/DC-MBQC", "code_stars": 0, "code_last_update": "2026-01-01", "AI": {"tldr": "DC-MBQC is the first distributed quantum compilation framework for measurement-based quantum computing, addressing graph partitioning and inter-QPU communication challenges to optimize execution time and photon lifetime requirements in photonic systems.", "motivation": "While distributed quantum computing has advanced for circuit models, there's limited research on distributed measurement-based quantum computing (MBQC), which is fundamentally different and particularly suitable for photonic quantum platforms. The paper aims to bridge this gap by developing a distributed compilation framework specifically for MBQC.", "method": "The DC-MBQC framework addresses two key challenges: 1) Task allocation using an adaptive graph partitioning algorithm that preserves graph state structure while balancing workload across QPUs, and 2) Inter-QPU communication through layer scheduling problem formulation and solution algorithm. The framework optimizes execution time and required photon lifetime considering realistic hardware constraints.", "result": "Experiments demonstrate significant improvements: 7.46\u00d7 reduction in required photon lifetime and 6.82\u00d7 speedup with 8 fully-connected QPUs, confirming the advantages of distributed quantum computing in photonic systems.", "conclusion": "DC-MBQC successfully enables distributed quantum computing for measurement-based quantum computing, addressing critical challenges in task allocation and inter-QPU communication while optimizing for practical hardware constraints in photonic quantum platforms."}}
{"id": "2601.00242", "categories": ["quant-ph", "cs.AI", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00242", "abs": "https://arxiv.org/abs/2601.00242", "authors": ["Yotam Peled", "David Zenati", "Eliya Nachmani"], "title": "Neural Minimum Weight Perfect Matching for Quantum Error Codes", "comment": null, "summary": "Realizing the full potential of quantum computation requires Quantum Error Correction (QEC). QEC reduces error rates by encoding logical information across redundant physical qubits, enabling errors to be detected and corrected. A common decoder used for this task is Minimum Weight Perfect Matching (MWPM) a graph-based algorithm that relies on edge weights to identify the most likely error chains. In this work, we propose a data-driven decoder named Neural Minimum Weight Perfect Matching (NMWPM). Our decoder utilizes a hybrid architecture that integrates Graph Neural Networks (GNNs) to extract local syndrome features and Transformers to capture long-range global dependencies, which are then used to predict dynamic edge weights for the MWPM decoder. To facilitate training through the non-differentiable MWPM algorithm, we formulate a novel proxy loss function that enables end-to-end optimization. Our findings demonstrate significant performance reduction in the Logical Error Rate (LER) over standard baselines, highlighting the advantage of hybrid decoders that combine the predictive capabilities of neural networks with the algorithmic structure of classical matching.", "AI": {"tldr": "Neural Minimum Weight Perfect Matching (NMWPM) decoder combines Graph Neural Networks and Transformers to predict dynamic edge weights for MWPM quantum error correction, achieving significant logical error rate reduction.", "motivation": "Quantum Error Correction (QEC) is essential for realizing quantum computation's full potential, but existing decoders like Minimum Weight Perfect Matching (MWPM) rely on static edge weights that may not capture complex error patterns. There's a need for data-driven approaches that can learn optimal weight assignments from quantum error data.", "method": "Proposes NMWPM decoder with hybrid architecture: Graph Neural Networks extract local syndrome features, Transformers capture long-range global dependencies, and these features predict dynamic edge weights for MWPM decoder. Introduces novel proxy loss function to enable end-to-end training through non-differentiable MWPM algorithm.", "result": "Demonstrates significant performance reduction in Logical Error Rate (LER) compared to standard baselines, showing advantage of hybrid decoders combining neural network predictive capabilities with classical matching algorithmic structure.", "conclusion": "Hybrid decoders that integrate neural networks with classical matching algorithms offer superior quantum error correction performance by learning optimal edge weight assignments from data, enabling more accurate error chain identification and correction."}}
{"id": "2601.00247", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00247", "abs": "https://arxiv.org/abs/2601.00247", "authors": ["Martin Plesch", "Martin Fri\u00e1k", "Ijaz Ahamed Mohammad"], "title": "Efficient implementation of single particle Hamiltonians in exponentially reduced qubit space", "comment": "12 pages, 2 figures", "summary": "Current and near-term quantum hardware is constrained by limited qubit counts, circuit depth, and the high cost of repeated measurements. We address these challenges for solid state Hamiltonians by introducing a logarithmic-qubit encoding that maps a system with $N$ physical sites onto only $\\lceil \\log_2 N \\rceil$ qubits while maintaining a clear correspondence with the underlying physical model. Within this reduced register, we construct a compatible variational circuit and a Gray-code-inspired measurement strategy whose number of global settings grows only logarithmically with system size. To quantify the overall hardware load, we introduce a volumetric efficiency metric that combines the number of qubit, circuit depth, and the number of measurement settings into a single measure, expressing the overall computation costs. Using this metric, we show that the total space-time-sampling volume required in a variational loop can be reduced dramatically from $N^2$ to $(logN)^3$ for hardware efficient ansatz, allowing an exponential reduction in time and size of the quantum hardware. These results demonstrate that large, structured solid-state Hamiltonians can be simulated on substantially smaller quantum registers with controlled sampling overhead and manageable circuit complexity, extending the reach of variational quantum algorithms on near-term devices.", "AI": {"tldr": "Logarithmic-qubit encoding reduces qubit count from N to log\u2082N for solid-state Hamiltonians, with Gray-code measurement strategy and volumetric efficiency metric showing dramatic reduction in space-time-sampling volume from N\u00b2 to (logN)\u00b3.", "motivation": "Address limitations of near-term quantum hardware: limited qubit counts, circuit depth, and high cost of repeated measurements for simulating solid-state Hamiltonians.", "method": "Introduce logarithmic-qubit encoding mapping N physical sites to \u2308log\u2082N\u2309 qubits, construct compatible variational circuit, develop Gray-code-inspired measurement strategy with logarithmic measurement settings, and define volumetric efficiency metric combining qubits, depth, and measurements.", "result": "Total space-time-sampling volume in variational loop reduced dramatically from N\u00b2 to (logN)\u00b3 for hardware-efficient ansatz, enabling exponential reduction in time and hardware size requirements.", "conclusion": "Large structured solid-state Hamiltonians can be simulated on substantially smaller quantum registers with controlled sampling overhead and manageable circuit complexity, extending reach of variational quantum algorithms on near-term devices."}}
{"id": "2601.00259", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00259", "abs": "https://arxiv.org/abs/2601.00259", "authors": ["Rohit Kumar Shukla", "Amikam Levy"], "title": "First appearance of quasiprobability negativity in quantum many-body dynamics", "comment": "16 pages, 7 figures", "summary": "Quasiprobability distributions capture aspects of quantum dynamics that have no classical counterpart, yet the dynamical emergence of their negativity in many-body systems remains largely unexplored. We introduce the \\emph{first-time negativity} (FTN) of the Margenau-Hill quasiprobability as a dynamical indicator of when local measurement sequences in an interacting quantum system begin to exhibit genuinely nonclassical behavior. Using the Ising chain, we show that FTN discriminates clearly between interaction-dominated and field-dominated regimes, is systematically reshaped by temperature, and responds sensitively to the breaking of integrability. When measurements are performed on different sites, FTN reveals a characteristic spatio-temporal structure that reflects the finite-time spreading of operator incompatibility across the lattice. We further compare the numerical onset of negativity with a recently proposed quantum speed limit (QSL) for quasiprobabilities, which provides a geometric benchmark for the observed dynamics. Our results identify FTN as a practical and experimentally accessible probe of real-time quantum coherence and contextuality, directly suited to current platforms capable of sequential weak and strong measurements.", "AI": {"tldr": "The paper introduces First-Time Negativity (FTN) of Margenau-Hill quasiprobability as a dynamical indicator of nonclassical behavior in quantum many-body systems, showing it discriminates interaction regimes, responds to temperature and integrability breaking, and reveals spatio-temporal structure of operator incompatibility spreading.", "motivation": "Quasiprobability distributions reveal nonclassical aspects of quantum dynamics, but the dynamical emergence of their negativity in many-body systems remains largely unexplored. There is a need for practical indicators to detect when local measurement sequences begin exhibiting genuinely nonclassical behavior in interacting quantum systems.", "method": "Introduces First-Time Negativity (FTN) of the Margenau-Hill quasiprobability distribution as a dynamical indicator. Studies FTN in the Ising chain model, analyzing its behavior across interaction-dominated vs field-dominated regimes, temperature dependence, response to integrability breaking, and spatio-temporal structure when measurements are performed on different sites. Compares numerical onset of negativity with a recently proposed quantum speed limit for quasiprobabilities.", "result": "FTN clearly discriminates between interaction-dominated and field-dominated regimes in the Ising chain. It is systematically reshaped by temperature and responds sensitively to integrability breaking. When measurements are performed on different sites, FTN reveals characteristic spatio-temporal structure reflecting finite-time spreading of operator incompatibility across the lattice. The numerical onset of negativity aligns with the quantum speed limit for quasiprobabilities, providing a geometric benchmark.", "conclusion": "FTN serves as a practical and experimentally accessible probe of real-time quantum coherence and contextuality in many-body systems. It is directly suited to current platforms capable of sequential weak and strong measurements, offering a new tool to study dynamical emergence of nonclassical behavior in quantum systems."}}
{"id": "2601.00266", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00266", "abs": "https://arxiv.org/abs/2601.00266", "authors": ["Wai-Keong Mok", "Tobias Haug", "Wen Wei Ho", "John Preskill"], "title": "Nature is stingy: Universality of Scrooge ensembles in quantum many-body systems", "comment": "16 pages, 5 figures + Appendices", "summary": "Recent advances in quantum simulators allow direct experimental access to the ensemble of pure states generated by measuring part of an isolated quantum many-body system. These projected ensembles encode fine-grained information beyond thermal expectation values and provide a new window into quantum thermalization. In chaotic dynamics, projected ensembles exhibit universal statistics, a phenomenon known as deep thermalization. While infinite-temperature systems generate Haar-random ensembles, realistic physical constraints such as finite temperature or conservation laws require a more general framework. It has been proposed that deep thermalization is governed in general by the emergence of Scrooge ensembles, maximally entropic distributions of pure states consistent with the underlying constraints. Here we provide rigorous arguments supporting this proposal. To characterize this universal behavior, we invoke Scrooge $k$-designs, which approximate Scrooge ensembles, and identify three physically distinct mechanisms for their emergence. First, global Scrooge designs can arise from long-time chaotic unitary dynamics alone, without the need for measurements. Second, if the global state is highly scrambled, a local Scrooge design is induced when the complementary subsystem is measured. Third, a local Scrooge ensemble arises from an arbitrary entangled state when the complementary system is measured in a highly scrambled basis. Numerical simulations across a range of many-body systems identify coherence, entanglement, non-stabilizerness, and information scrambling as essential resources for the emergence of Scrooge-like behavior. Taken together, our results establish a unified theoretical framework for the emergence of maximally entropic, information-stingy randomness in quantum many-body systems.", "AI": {"tldr": "The paper establishes Scrooge ensembles as the universal framework for deep thermalization in quantum many-body systems, identifying three mechanisms for their emergence and key quantum resources required.", "motivation": "To provide rigorous theoretical foundations for understanding deep thermalization phenomena in quantum many-body systems, particularly how projected ensembles exhibit universal statistics beyond conventional thermalization, and to establish Scrooge ensembles as the general framework for this behavior under realistic physical constraints like finite temperature and conservation laws.", "method": "The authors develop theoretical arguments using Scrooge k-designs to characterize universal behavior, identify three distinct physical mechanisms for Scrooge ensemble emergence: (1) from long-time chaotic unitary dynamics alone, (2) from highly scrambled global states with complementary subsystem measurement, and (3) from arbitrary entangled states measured in highly scrambled bases. They validate these mechanisms through numerical simulations across various many-body systems.", "result": "The paper demonstrates that Scrooge ensembles universally govern deep thermalization, with three identified mechanisms showing how they emerge in different physical scenarios. Numerical simulations reveal that coherence, entanglement, non-stabilizerness, and information scrambling are essential resources for Scrooge-like behavior.", "conclusion": "The work establishes a unified theoretical framework for the emergence of maximally entropic, information-stingy randomness (Scrooge ensembles) in quantum many-body systems, providing rigorous support for their role in deep thermalization phenomena beyond conventional thermalization."}}
{"id": "2601.00337", "categories": ["quant-ph", "cs.CR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.00337", "abs": "https://arxiv.org/abs/2601.00337", "authors": ["Daniel Alabi", "Theshani Nuradha"], "title": "When Does Quantum Differential Privacy Compose?", "comment": "36 pages, 1 figure", "summary": "Composition is a cornerstone of classical differential privacy, enabling strong end-to-end guarantees for complex algorithms through composition theorems (e.g., basic and advanced). In the quantum setting, however, privacy is defined operationally against arbitrary measurements, and classical composition arguments based on scalar privacy-loss random variables no longer apply. As a result, it has remained unclear when meaningful composition guarantees can be obtained for quantum differential privacy (QDP).\n  In this work, we clarify both the limitations and possibilities of composition in the quantum setting. We first show that classical-style composition fails in full generality for POVM-based approximate QDP: even quantum channels that are individually perfectly private can completely lose privacy when combined through correlated joint implementations. We then identify a setting in which clean composition guarantees can be restored. For tensor-product channels acting on product neighboring inputs, we introduce a quantum moments accountant based on an operator-valued notion of privacy loss and a matrix moment-generating function. Although the resulting R\u00e9nyi-type divergence does not satisfy a data-processing inequality, we prove that controlling its moments suffices to bound measured R\u00e9nyi divergence, yielding operational privacy guarantees against arbitrary measurements. This leads to advanced-composition-style bounds with the same leading-order behavior as in the classical theory.\n  Our results demonstrate that meaningful composition theorems for quantum differential privacy require carefully articulated structural assumptions on channels, inputs, and adversarial measurements, and provide a principled framework for understanding which classical ideas do and do not extend to the quantum setting.", "AI": {"tldr": "Quantum differential privacy composition fails generally but can be restored for tensor-product channels with product inputs using a quantum moments accountant framework.", "motivation": "Classical differential privacy has strong composition theorems, but quantum differential privacy (QDP) faces challenges because privacy is defined operationally against arbitrary quantum measurements, making classical composition arguments based on scalar privacy-loss variables inapplicable. It's unclear when meaningful composition guarantees can be obtained for QDP.", "method": "First demonstrates that classical-style composition fails generally for POVM-based approximate QDP. Then introduces a quantum moments accountant framework for tensor-product channels acting on product neighboring inputs, based on an operator-valued notion of privacy loss and matrix moment-generating function. Proves that controlling moments of the resulting R\u00e9nyi-type divergence suffices to bound measured R\u00e9nyi divergence.", "result": "Shows that individually perfectly private quantum channels can completely lose privacy when combined through correlated joint implementations. For the restricted setting of tensor-product channels with product inputs, obtains advanced-composition-style bounds with the same leading-order behavior as classical theory.", "conclusion": "Meaningful composition theorems for quantum differential privacy require carefully articulated structural assumptions on channels, inputs, and adversarial measurements. Provides a principled framework for understanding which classical composition ideas extend to the quantum setting and which do not."}}
{"id": "2601.00383", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00383", "abs": "https://arxiv.org/abs/2601.00383", "authors": ["Xian Shi"], "title": "Probabilistic Entanglement Distillation and Cost under Approximately Nonentangling and Dually Nonentangling Instruments", "comment": null, "summary": "Entanglement distillation and entanglement cost are fundamental tasks in quantum entanglement theory. This work studies both in the probabilistic setting and focuses on the asymptotic error exponent of probabilistic entanglement distillation when the operational model is $\u03b4$-approximately nonentangling(ANE) and $\u03b4$-approximately dually nonentangling(ADNE) quantum instruments. While recent progress has clarified limitations of probabilistic transformations in general resource theories, an analytic formula for the error exponent of probabilistic entanglement distillation under approximately (dually) nonentangling operations has remained unavailable.\n  Building on the framework of postselected quantum hypothesis testing, we establish a direct connection between probabilistic distillation and postselected hypothesis testing against the set of separable states. In particular, we derive an analytical characterization of the distillation error exponent under ANE. Besides, we relate the exponent to postselected hypothesis testing with measurements restricted to be separable. We further investigate probabilistic entanglement dilution and establish a relation between probabilistic entanglement costs under approximately nonentangling and approximately dually nonentangling instruments, together with a bound on the probabilistic entanglement cost under nonentangling instruments", "AI": {"tldr": "Analytical characterization of error exponent for probabilistic entanglement distillation under approximately nonentangling operations via connection to postselected quantum hypothesis testing against separable states.", "motivation": "Despite recent progress in probabilistic resource transformations, an analytical formula for the error exponent of probabilistic entanglement distillation under approximately (dually) nonentangling operations has remained unavailable, motivating this study to bridge this gap.", "method": "Builds on postselected quantum hypothesis testing framework to establish direct connection between probabilistic distillation and postselected hypothesis testing against separable states; investigates both ANE (approximately nonentangling) and ADNE (approximately dually nonentangling) quantum instruments.", "result": "Derives analytical characterization of distillation error exponent under ANE operations; relates exponent to postselected hypothesis testing with separable measurements; establishes relation between probabilistic entanglement costs under ANE and ADNE instruments with bound on probabilistic entanglement cost under nonentangling instruments.", "conclusion": "Provides analytical framework connecting probabilistic entanglement distillation/dilution to postselected hypothesis testing, yielding explicit characterizations of error exponents and cost relations for approximately nonentangling operations."}}
{"id": "2601.00405", "categories": ["quant-ph", "hep-ph", "hep-th", "nucl-ex", "nucl-th"], "pdf": "https://arxiv.org/pdf/2601.00405", "abs": "https://arxiv.org/abs/2601.00405", "authors": ["Dmitri E. Kharzeev"], "title": "The Maximal Entanglement Limit in Statistical and High Energy Physics", "comment": "70 pages, 11 figures; Lectures at the 65th Jubilee Cracow School of Theoretical Physics, Zakopane, Tatra mountains, Poland, June 14-21, 2025", "summary": "These lectures advocate the idea that quantum entanglement provides a unifying foundation for both statistical physics and high-energy interactions. I argue that, at sufficiently long times or high energies, most quantum systems approach a Maximal Entanglement Limit (MEL) in which phases of quantum states become unobservable, reduced density matrices acquire a thermal form, and probabilistic descriptions emerge without invoking ergodicity or classical randomness. Within this framework, the emergence of probabilistic parton model, thermalization in the break-up of confining strings and in high-energy collisions, and the universal small $x$ behavior of structure functions arise as direct consequences of entanglement and geometry of high-dimensional Hilbert space.", "AI": {"tldr": "Quantum entanglement drives systems to a Maximal Entanglement Limit where quantum phases become unobservable, reduced density matrices become thermal, and probabilistic descriptions emerge naturally without requiring ergodicity or classical randomness.", "motivation": "To establish quantum entanglement as a unifying foundation connecting statistical physics and high-energy interactions, showing that entanglement alone can explain emergent probabilistic behavior and thermalization without invoking classical concepts.", "method": "Proposes the Maximal Entanglement Limit (MEL) framework where quantum systems at long times or high energies approach a state where quantum phases become unobservable. Uses entanglement and high-dimensional Hilbert space geometry to derive emergent phenomena.", "result": "Shows that probabilistic parton models, thermalization in confining string break-up and high-energy collisions, and universal small-x behavior of structure functions emerge as direct consequences of entanglement and Hilbert space geometry.", "conclusion": "Quantum entanglement provides a fundamental unifying principle that explains the emergence of probabilistic descriptions and thermal behavior in both statistical physics and high-energy interactions without requiring ergodicity or classical randomness."}}
{"id": "2601.00425", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00425", "abs": "https://arxiv.org/abs/2601.00425", "authors": ["Salman Sajad Wani", "Mughees Ahmed Khan", "Abrar Ahmed Naqash", "Saif Al-Kuwari"], "title": "Chip scale superconducting quantum gravimeter based on a SQUID transmon mechanical resonator", "comment": "16figs", "summary": "Precise gravitational measurements are vital for geophysics and inertial navigation, but current platforms struggle to combine absolute accuracy with high-bandwidth tracking. We address this challenge with a chip-scale superconducting gravimeter that couples a flux-tunable transmon qubit to a high-$Q$ mechanical resonator. We embed the mechanical element inside the qubit's SQUID loop. This allows us to exploit the Josephson potential's nonlinearity, creating a motion-dependent inductance that maps gravitational displacement onto the qubit's geometric phase. Using a stroboscopic measurement protocol, we suppress mechanical decoherence at revival times. This yields a predicted sensitivity of $10^2\\,\\mathrm{nGal}/\\sqrt{\\mathrm{Hz}}$, approaching the performance of atomic sensors but with kilohertz-rate sampling. With electrical {in situ} tunability and SI traceability via microwave spectroscopy, this architecture offers a practical route to high-speed, quantum-limited on-chip gravimetry.", "AI": {"tldr": "Chip-scale superconducting gravimeter using transmon qubit coupled to mechanical resonator achieves high-bandwidth gravitational measurements with quantum-limited sensitivity.", "motivation": "Current gravitational measurement platforms struggle to combine absolute accuracy with high-bandwidth tracking, limiting applications in geophysics and inertial navigation.", "method": "Couples a flux-tunable transmon qubit to a high-Q mechanical resonator embedded inside the qubit's SQUID loop, exploiting Josephson nonlinearity to create motion-dependent inductance that maps gravitational displacement onto qubit's geometric phase, using stroboscopic measurement protocol to suppress mechanical decoherence.", "result": "Predicted sensitivity of 10\u00b2 nGal/\u221aHz, approaching atomic sensor performance but with kilohertz-rate sampling, offering electrical in situ tunability and SI traceability via microwave spectroscopy.", "conclusion": "This architecture provides a practical route to high-speed, quantum-limited on-chip gravimetry with superior bandwidth compared to existing platforms."}}
{"id": "2601.00431", "categories": ["quant-ph", "physics.chem-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00431", "abs": "https://arxiv.org/abs/2601.00431", "authors": ["Seogjoo J. Jang"], "title": "Multistep quantum master equation theory for response functions in four wave mixing electronic spectroscopy of multichromophoric macromolecules", "comment": "14 pages, 0 figure", "summary": "This work provides an alternative derivation of third order response functions in four wave mixing spectroscopy of multichromophoric macromolecular systems considering only single exciton states. For the case of harmonic oscillator bath linearly and diagonally coupled to exciton states, closed form expressions showing all the explicit time dependences are derived. These expressions can provide more solid physical basis for understanding 2-dimensional electronic spectroscopy signals. For more general cases of system-bath coupling, the quantum master equation (QME) approach is employed for the derivation of multistep time evolution equations for Green function-like operators. Solution of these equations is feasible at the level of 2nd order non-Markovian QME, and the new approach can account for inter-exciton coupling, dephasing, relaxation, and non-Markovian effects in a consistent manner.", "AI": {"tldr": "Derivation of third-order response functions for four-wave mixing spectroscopy in multichromophoric systems using single exciton states, with closed-form expressions for harmonic baths and quantum master equation approach for general system-bath coupling.", "motivation": "To provide a more solid physical basis for understanding 2-dimensional electronic spectroscopy signals by deriving explicit closed-form expressions for third-order response functions in multichromophoric macromolecular systems, and to develop a consistent approach for handling general system-bath coupling cases.", "method": "Two approaches: 1) For harmonic oscillator baths linearly and diagonally coupled to exciton states, derivation of closed-form expressions showing all explicit time dependencies. 2) For general system-bath coupling, employment of quantum master equation (QME) approach to derive multistep time evolution equations for Green function-like operators, using 2nd order non-Markovian QME.", "result": "Closed-form expressions for third-order response functions in harmonic bath cases, and a new QME-based approach that can consistently account for inter-exciton coupling, dephasing, relaxation, and non-Markovian effects in general system-bath coupling scenarios.", "conclusion": "The alternative derivation provides clearer physical insights into 2D electronic spectroscopy signals, and the QME approach offers a consistent framework for modeling complex system-bath interactions in multichromophoric systems, enabling better understanding of spectroscopic observables."}}
{"id": "2601.00483", "categories": ["quant-ph", "cond-mat.mes-hall", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00483", "abs": "https://arxiv.org/abs/2601.00483", "authors": ["Long Ma", "Larissa In\u00e1cio", "Dai-Nam Le", "Lilia M. Woods", "Mathias Bostr\u00f6m"], "title": "Prediction of a measurable sign change in the Casimir force using a magnetic fluid", "comment": "9 pages, 4 figures. Submitted", "summary": "We demonstrate quantum levitation controlled by Casimir forces acting between a polystyrene surface and a Teflon-coated metallic substrate immersed in a mixture of Toluene and magnetite particles. This system experiences repulsion-attraction transitions in the Casimir interaction for distances where the effect is measurable. This Casimir trapping can be controlled by clever choices of metallic and ferrofluid materials, which are directly linked to the emergence of the trapping effect. Thermal and quantum contributions are investigated in detail, showing how the optical and magnetic properties of the ferrofluid and other materials affect the magnitude of the trapping and its distance range of observability.", "AI": {"tldr": "Quantum levitation controlled by Casimir forces between polystyrene and Teflon-coated metal in ferrofluid, with repulsion-attraction transitions enabling trapping effects.", "motivation": "To demonstrate controllable quantum levitation using Casimir forces in ferrofluids, exploring how material choices and quantum/thermal effects enable repulsion-attraction transitions for trapping applications.", "method": "Experimental system using polystyrene surface and Teflon-coated metallic substrate immersed in Toluene-magnetite ferrofluid mixture; investigation of Casimir forces with detailed analysis of thermal and quantum contributions, optical/magnetic properties effects.", "result": "Demonstrated quantum levitation with repulsion-attraction transitions in measurable distance ranges; showed Casimir trapping can be controlled by material selection (metallic substrates, ferrofluid compositions); thermal and quantum contributions significantly affect trapping magnitude and observable distance range.", "conclusion": "Casimir forces in ferrofluid systems enable controllable quantum levitation and trapping effects; material properties (optical, magnetic) crucially determine repulsion-attraction behavior and trapping observability, offering potential for quantum manipulation applications."}}
{"id": "2601.00484", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00484", "abs": "https://arxiv.org/abs/2601.00484", "authors": ["Ali Al-Bayaty", "Marek Perkowski"], "title": "A Geometrical Design Tool for Building Cost-Effective Layout-Aware n-Bit Quantum Gates Using the Bloch Sphere Approach", "comment": "11 pages, 8 figures, 5 tables", "summary": "The conventional design technique of any n-bit quantum gate is mainly achieved using unitary matrices multiplication, where n >= 2 and 1 <= m <= n-1 for m target qubits and n-m control qubits. These matrices represent quantum rotations by an n-bit quantum gate. For a quantum designer, such a conventional technique requires extensive computational time and effort, which may generate an n-bit quantum gate with a too high quantum cost. The Bloch sphere is only utilized as a visualization tool to verify the conventional design correctness for quantum rotations by a quantum gate. In contrast, this paper introduces a new concept of using the Bloch sphere as a \"geometrical design tool\" to build cost-effective n-bit quantum gates with lower quantum costs. This concept is termed the \"Bloch sphere approach (BSA)\". In BSA, a cost-effective n-bit quantum gate is built without using any unitary matrices multiplication. Instead, the quantum rotations for such a gate are visually selected using the geometrical planar intersections of the Bloch sphere. The BSA can efficiently map m targets among n-m controls for an n-bit quantum gate, to satisfy the limited layout connectivity for the physical neighboring qubits of a quantum computer. Experimentally, n-bit quantum gates built using the BSA always have lower quantum costs than those for such gates built using the conventional quantum design techniques.", "AI": {"tldr": "The paper introduces the Bloch Sphere Approach (BSA) as a geometric design tool for building cost-effective n-bit quantum gates with lower quantum costs compared to conventional unitary matrix multiplication methods.", "motivation": "Conventional quantum gate design using unitary matrix multiplication requires extensive computational time and effort, often resulting in gates with high quantum costs. The Bloch sphere is currently only used as a visualization tool, not as a design tool.", "method": "The Bloch Sphere Approach (BSA) uses the Bloch sphere as a geometrical design tool where quantum rotations are visually selected using geometrical planar intersections of the Bloch sphere, eliminating the need for unitary matrices multiplication. BSA efficiently maps m target qubits among n-m control qubits to satisfy physical connectivity constraints.", "result": "Experimental results show that n-bit quantum gates built using BSA always have lower quantum costs than those built using conventional quantum design techniques.", "conclusion": "BSA provides a more efficient, cost-effective approach to quantum gate design by leveraging geometric visualization on the Bloch sphere rather than computational matrix operations, offering practical advantages for quantum circuit design under physical connectivity constraints."}}
{"id": "2601.00487", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00487", "abs": "https://arxiv.org/abs/2601.00487", "authors": ["Wenxuan Xie", "John C Schotland"], "title": "Non-Hermitian Band Topology and Edge States in Atomic Lattices", "comment": null, "summary": "We investigate the band structure and topological phases of one- and two-dimensional bipartite atomic lattices mediated by long-range dissipative radiative coupling. By deriving an effective non-Hermitian Hamiltonian for the single-excitation sector, we demonstrate that the low-energy dynamics of the system are governed by a Dirac equation with a complex Fermi velocity. We analyze the associated topological invariants for both the SSH and honeycomb models, utilizing synthetic gauge fields to break time-reversal symmetry in the latter. Finally, we explicitly verify the non-Hermitian bulk-edge correspondence by deriving analytical solutions for edge states localized at domain boundaries.", "AI": {"tldr": "The paper investigates topological phases in 1D and 2D bipartite atomic lattices with long-range dissipative radiative coupling, revealing non-Hermitian Dirac dynamics and verifying bulk-edge correspondence.", "motivation": "To understand how long-range dissipative radiative coupling affects band structure and topological phases in bipartite atomic lattices, particularly focusing on non-Hermitian effects in the single-excitation sector.", "method": "Derived an effective non-Hermitian Hamiltonian for the single-excitation sector, analyzed topological invariants for SSH and honeycomb models using synthetic gauge fields to break time-reversal symmetry, and derived analytical solutions for edge states at domain boundaries.", "result": "Demonstrated that low-energy dynamics are governed by a Dirac equation with complex Fermi velocity, analyzed topological invariants for both models, and explicitly verified the non-Hermitian bulk-edge correspondence through analytical edge state solutions.", "conclusion": "Long-range dissipative radiative coupling in bipartite atomic lattices leads to non-Hermitian topological phases with complex Dirac dynamics, and the bulk-edge correspondence remains valid in these non-Hermitian systems as verified by analytical edge state solutions."}}
{"id": "2601.00489", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00489", "abs": "https://arxiv.org/abs/2601.00489", "authors": ["Modi Ke", "Dai-Nam Le", "Lilia M. Woods"], "title": "Casimir interactions and drift currents", "comment": "10 pages, 5 figures. Submitted", "summary": "We investigate the fluctuation-induced Casimir interactions between two parallel graphene sheets carrying steady-state drift currents. The graphene properties are modeled based on the shifted Fermi disk model to capture the non-equilibrium optical response of the system. We find that the drift current introduces a repulsive correction to the perpendicular to the layers Casimir interaction, thereby reducing the overall attractive force. Although the correction is repulsive, it does not overcome the underlying attraction between the layers. It also generates a lateral force that opposes the carrier flow direction. Both contributions are studied in terms of distance and drift velocity functionalities showing pathways for Casimir force control.", "AI": {"tldr": "Drift currents in graphene sheets create repulsive corrections to Casimir forces and generate lateral forces opposing carrier flow, enabling Casimir force control.", "motivation": "To investigate how steady-state drift currents in graphene sheets affect Casimir interactions, exploring non-equilibrium optical responses and potential pathways for controlling Casimir forces.", "method": "Modeled graphene properties using the shifted Fermi disk model to capture non-equilibrium optical response, then analyzed Casimir interactions between two parallel graphene sheets with drift currents.", "result": "Drift current introduces a repulsive correction to perpendicular Casimir interaction (reducing overall attraction) and generates a lateral force opposing carrier flow direction; both effects depend on distance and drift velocity.", "conclusion": "Drift currents in graphene provide mechanisms for controlling Casimir forces through repulsive corrections and lateral forces, though the repulsive correction doesn't overcome the fundamental attraction between layers."}}
{"id": "2601.00511", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00511", "abs": "https://arxiv.org/abs/2601.00511", "authors": ["Anton Kapustin", "Daniil Radamovich"], "title": "Chaos and thermalization in Clifford-Floquet dynamics", "comment": "20 pages, 6 figures", "summary": "We study the ergodic properties of a unitary Floquet dynamics arising from the repeated application of a translationally-invariant Clifford Quantum Cellular Automata to an infinite system of qubits in d dimensions. One expects that if the QCA does not exhibit any periodicity, a generic initial state of qubits will thermalize, that is, approach the infinite-temperature state. We show that this is true for many classes of states, both pure and mixed. In particular, this is true for all initial states that are short-range entangled and close to the equilibrium state. We also point out a subtle distinction between weak and strong thermalization.", "AI": {"tldr": "The paper analyzes ergodic properties of translationally-invariant Clifford Quantum Cellular Automata (QCA) in infinite qubit systems, showing that non-periodic QCA cause generic initial states to thermalize to infinite-temperature states, with distinctions between weak and strong thermalization.", "motivation": "To understand the thermalization behavior of unitary Floquet dynamics generated by translationally-invariant Clifford Quantum Cellular Automata applied to infinite qubit systems, particularly investigating whether generic initial states approach infinite-temperature states when the QCA lacks periodicity.", "method": "Theoretical analysis of ergodic properties of unitary Floquet dynamics arising from repeated application of translationally-invariant Clifford QCA to infinite d-dimensional qubit systems, examining different classes of initial states (both pure and mixed) and distinguishing between weak and strong thermalization.", "result": "Demonstrates that non-periodic Clifford QCA cause thermalization to infinite-temperature states for many classes of initial states, including all short-range entangled states close to equilibrium, and identifies subtle distinctions between weak and strong thermalization regimes.", "conclusion": "Translationally-invariant Clifford QCA without periodicity generically drive systems to thermalize to infinite-temperature states, with the thermalization behavior depending on the specific class of initial states and exhibiting important differences between weak and strong thermalization."}}
{"id": "2601.00622", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00622", "abs": "https://arxiv.org/abs/2601.00622", "authors": ["Thi Phuong Anh Nguyen", "Le Phuong Hoang", "Xuan Binh Cao"], "title": "Photonic Reservoir Engineering via 2D $\u039b$-Type Atomic Arrays in Waveguide QED", "comment": null, "summary": "Electromagnetically induced transparency (EIT) in $\u039b$-type atomic systems underpins quantum technologies such as high-fidelity memory and nonlinear optics, but conventional setups face intrinsic limitations. Standard geometries of one-dimensional atomic chains coupled to waveguides allow only a single bright superradiant channel, while subradiant modes remain weakly accessible, limiting control over collective radiative behavior and dark-state pathways. This leads to unwanted inelastic processes, degrading memory fidelity and reducing nonlinear photon generation efficiency. Here, we propose two two-dimensional (2D) atomic lattice geometries coupled to a photonic crystal waveguide, namely Zigzag and Orthogonal structures. In the Zigzag model, engineered collective super- and subradiant modes produce a flattened EIT window, broadening the transmission bandwidth and suppressing unwanted scattering to enhance quantum memory fidelity. In the Orthogonal model, four-wave mixing (FWM) intensity is amplified by up to six orders of magnitude relative to a conventional one-dimensional $\u039b$-type EIT chain with identical $\u0393_{1D}$, $\u03a9_c$, and probe intensity, with localized idler photons forming well-defined spectral modes. These results demonstrate a versatile route to engineer structured photonic reservoirs for on-demand photon generation, high-fidelity quantum storage, and enhanced nonlinear optical processes.", "AI": {"tldr": "2D atomic lattices coupled to photonic crystal waveguides enable engineered collective radiative modes, broadening EIT bandwidth for quantum memory and amplifying four-wave mixing by 6 orders of magnitude for enhanced nonlinear optics.", "motivation": "Conventional 1D \u039b-type atomic systems for EIT face intrinsic limitations: only single bright superradiant channel accessible, subradiant modes weakly accessible, leading to unwanted inelastic processes that degrade quantum memory fidelity and reduce nonlinear photon generation efficiency.", "method": "Proposed two 2D atomic lattice geometries coupled to photonic crystal waveguides: Zigzag and Orthogonal structures. Zigzag model engineers collective super- and subradiant modes to flatten EIT window. Orthogonal model amplifies four-wave mixing through engineered collective modes.", "result": "Zigzag structure produces flattened EIT window, broadening transmission bandwidth and suppressing unwanted scattering to enhance quantum memory fidelity. Orthogonal structure amplifies four-wave mixing intensity by up to six orders of magnitude relative to conventional 1D \u039b-type EIT chain with identical parameters, with localized idler photons forming well-defined spectral modes.", "conclusion": "2D atomic lattice geometries coupled to photonic crystal waveguides provide versatile route to engineer structured photonic reservoirs for on-demand photon generation, high-fidelity quantum storage, and enhanced nonlinear optical processes by overcoming limitations of conventional 1D atomic chains."}}
{"id": "2601.00651", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00651", "abs": "https://arxiv.org/abs/2601.00651", "authors": ["Nicola Bortolotti", "Kristian Piscicchia", "Matthias Laubenstein", "Simone Manti", "Antonino Marcian\u00f2", "Federico Nola", "Catalina Curceanu"], "title": "Experimental exclusion of a generalized K\u00e1rolyh\u00e1zy gravity-induced decoherence model", "comment": "10 pages, 4 figures", "summary": "We report new experimental constraints on the generalized version of the gravity-induced decoherence model originally proposed by K\u00e1rolyh\u00e1zy. Using data collected by the VIP Collaboration at the INFN Gran Sasso National Laboratory with a high-purity germanium detector, we derive an improved lower bound on the spatial correlation length $R_K$ characterizing metric fluctuations in the model. We obtain a bound $R_K > 4.64$ m (95\\% C.L.), which exceeds by more than an order of magnitude the previous experimental limit. When combined with the theoretical upper bound $R_K <1.98$ m derived from macroscopic localization requirements, our result excludes the generalized K\u00e1rolyh\u00e1zy model. The same conclusion applies to an associated non-Markovian formulation of the Continuous Spontaneous Localization (CSL) model. Our findings significantly tighten experimental constraints on gravity-related decoherence scenarios and demonstrate the sensitivity of underground low-background experiments to foundational modifications of quantum mechanics.", "AI": {"tldr": "Experimental constraints from underground germanium detector data exclude the generalized K\u00e1rolyh\u00e1zy gravity-induced decoherence model by showing its spatial correlation length parameter is inconsistent with theoretical bounds.", "motivation": "To test foundational modifications of quantum mechanics, specifically gravity-induced decoherence models like K\u00e1rolyh\u00e1zy's, using high-precision underground experiments to constrain parameters that could reveal quantum-gravity effects.", "method": "Used data from VIP Collaboration at INFN Gran Sasso National Laboratory with high-purity germanium detector to search for decoherence signatures, deriving constraints on spatial correlation length parameter R_K of the generalized K\u00e1rolyh\u00e1zy model.", "result": "Obtained improved lower bound R_K > 4.64 m (95% C.L.), exceeding previous experimental limit by more than an order of magnitude. Combined with theoretical upper bound R_K < 1.98 m, this excludes the generalized K\u00e1rolyh\u00e1zy model and associated non-Markovian CSL formulation.", "conclusion": "The generalized K\u00e1rolyh\u00e1zy gravity-induced decoherence model is experimentally excluded, demonstrating the power of underground low-background experiments to constrain foundational quantum mechanics modifications and gravity-related decoherence scenarios."}}
{"id": "2601.00676", "categories": ["quant-ph", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2601.00676", "abs": "https://arxiv.org/abs/2601.00676", "authors": ["Ivaldevingles Rodrigues De Souza Junior", "Andrea Trombettoni", "Carla Braitenberg"], "title": "Ultracold Quantum Gravimeters: An Introduction for Geophysicists", "comment": null, "summary": "This paper aims at providing an accessible introduction to ultracold quantum gravimeters tailored for geophysicists. We do not focus here on geophysical applications, as these are already well known to geophysicists, but rather provide a pedagogical exposition of the quantum-mechanical concepts needed to understand the operation of quantum gravimeters. We present a review of gravimeters based on two- and three-level atomic systems, focusing on the fundamental mechanisms of atomic interferometry. The functioning of Mach-Zehnder interferometers is discussed through the action of $\u03c0/2$ and $\u03c0$ pulses, showing how the resulting phase shift encodes gravitational acceleration. The effect of noise is briefly discussed.", "AI": {"tldr": "Pedagogical introduction to ultracold quantum gravimeters for geophysicists, explaining quantum-mechanical concepts and atomic interferometry fundamentals without focusing on geophysical applications.", "motivation": "To provide geophysicists with an accessible introduction to the quantum-mechanical principles underlying ultracold quantum gravimeters, bridging the gap between geophysical applications and quantum physics understanding.", "method": "Pedagogical exposition reviewing gravimeters based on two- and three-level atomic systems, explaining fundamental mechanisms of atomic interferometry, Mach-Zehnder interferometer operation using \u03c0/2 and \u03c0 pulses, and how phase shift encodes gravitational acceleration.", "result": "Comprehensive educational framework explaining how quantum gravimeters work at the atomic level, detailing the relationship between atomic interferometry phase shifts and gravitational acceleration measurements.", "conclusion": "The paper successfully provides geophysicists with the necessary quantum-mechanical foundation to understand ultracold quantum gravimeter operation, focusing on atomic interferometry principles rather than geophysical applications."}}
{"id": "2601.00708", "categories": ["quant-ph", "physics.bio-ph", "physics.chem-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.00708", "abs": "https://arxiv.org/abs/2601.00708", "authors": ["Seogjoo J. Jang"], "title": "Effects of Donor-Acceptor Quantum Coherence and Non-Markovian Bath on the Distance Dependence of Resonance Energy Transfer", "comment": "11 pages, 5 figures", "summary": "Accurate information on the distance dependence of resonance energy transfer (RET) is crucial for its utilization as a spectroscopic ruler \\re{of} nanometer scale distances. In this regard, understanding the effects of donor-acceptor quantum coherence and non-Markovian bath, which become significant at short distances, has significant implications. The present work investigates this issue theoretically by comparing results from a theory of coherent RET (CRET) with a nonequilibrium version of F\u00f6rster's RET (FRET) theory, both accounting for non-Markovian bath effects. Even for a model where the donor-acceptor electronic coupling is of transition dipole interaction form, it is shown that the RET rate in general deviates from the inverse sixth power distance dependence as opposed to the prediction of the original FRET. It is shown that the donor-acceptor quantum coherence makes the \\re{distance} dependence steeper than the sixth power although detailed manner of enhancement is sensitive to specific values of parameters. On the other hand, the non-Markovian bath effects make the \\re{distance} dependence more moderate than the sixth power for both CRET and nonueqilibrium FRET because finite time scale of the bath causes the rate to be smaller than the prediction of original FRET. While these effects are \\re{demonstrated clearly} in the population dynamics at sub-picosecond time scales, their contributions to the conventional RET efficiency are relatively minor. This indicates that the actual detection of such effects through conventional RET efficiency measurement requires either high precision or utilization of a donor with fast spontaneous decay rate of excitation.", "AI": {"tldr": "Theoretical study shows resonance energy transfer (RET) rate deviates from inverse sixth power distance dependence due to donor-acceptor quantum coherence and non-Markovian bath effects, with coherence making distance dependence steeper and bath effects making it more moderate.", "motivation": "Understanding distance dependence of RET is crucial for its use as a spectroscopic ruler at nanometer scales, requiring investigation of donor-acceptor quantum coherence and non-Markovian bath effects that become significant at short distances.", "method": "Theoretical comparison between coherent RET (CRET) theory and nonequilibrium F\u00f6rster's RET (FRET) theory, both accounting for non-Markovian bath effects, using a model with donor-acceptor electronic coupling of transition dipole interaction form.", "result": "RET rate deviates from inverse sixth power distance dependence: donor-acceptor quantum coherence makes distance dependence steeper than sixth power, while non-Markovian bath effects make it more moderate than sixth power for both CRET and nonequilibrium FRET. These effects are clear in sub-picosecond population dynamics but have minor contributions to conventional RET efficiency.", "conclusion": "Detection of quantum coherence and non-Markovian effects through conventional RET efficiency measurements requires either high precision or donors with fast spontaneous decay rates, as these effects are relatively minor in standard RET efficiency measurements despite being significant in ultrafast dynamics."}}
{"id": "2601.00711", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.00711", "abs": "https://arxiv.org/abs/2601.00711", "authors": ["Ali Abbassi", "Yann Dujardin", "Eric Gourdin", "Philippe Lacomme", "Caroline Prodhon"], "title": "Assessing Quantum Annealing to Solve the Minimum Vertex Multicut", "comment": "Published in Codit 2025", "summary": "Cybersecurity in telecommunication networks often leads to hard combinatorial optimization problems that are challenging to solve with classical methods. This work investigates the practical feasibility of using quantum annealing to address the Restricted Vertex Minimum Multicut Problem. The problem is formulated as a Quadratic Unconstrained Binary Optimization model and implemented on D-Wave s quantum annealer. Rather than focusing on solution quality alone, we analyze key aspects of the quantum workflow including minor embedding techniques, chain length, topology constraints, chain strength selection, unembedding procedures, and postprocessing. Our results show that quantum annealing faces substantial hardware-level constraints limitations in embedding and scalability, especially for large instances, while hybrid quantum-classical solvers provide improved feasibility. This study offers a realistic assessment of the D-Wave system s current capabilities and identifies crucial parameters that govern the success of quantum optimization in cybersecurity-related network problems.", "AI": {"tldr": "Quantum annealing applied to Restricted Vertex Minimum Multicut Problem for cybersecurity, revealing hardware constraints and hybrid solvers as more feasible.", "motivation": "Cybersecurity in telecommunication networks involves hard combinatorial optimization problems that are challenging for classical methods, motivating exploration of quantum annealing as an alternative approach.", "method": "Formulated the Restricted Vertex Minimum Multicut Problem as Quadratic Unconstrained Binary Optimization (QUBO) model, implemented on D-Wave quantum annealer, analyzed quantum workflow aspects including minor embedding, chain length, topology constraints, chain strength selection, unembedding procedures, and postprocessing.", "result": "Quantum annealing faces substantial hardware-level constraints in embedding and scalability, especially for large instances, while hybrid quantum-classical solvers provide improved feasibility.", "conclusion": "Study offers realistic assessment of D-Wave system's current capabilities and identifies crucial parameters governing success of quantum optimization in cybersecurity-related network problems."}}
{"id": "2601.00720", "categories": ["quant-ph", "cs.DM"], "pdf": "https://arxiv.org/pdf/2601.00720", "abs": "https://arxiv.org/abs/2601.00720", "authors": ["Ali Abbassi", "Yann Dujardin", "Eric Gourdin", "Philippe Lacomme", "Caroline Prodhon"], "title": "Quantum Approaches to the Minimum Edge Multiway Cut Problem", "comment": "Work published in QUEST IS 2025", "summary": "We investigate the minimum edge multiway cut problem, a fundamental task in evaluating the resilience of telecommunication networks. This study benchmarks the problem across three quantum computing paradigms: quantum annealing on a D-Wave quantum processing unit, photonic variational quantum circuits simulated on Quandela s Perceval platform, and IBM s gate-based Quantum Approximate Optimization Algorithm (QAOA). We assess the comparative feasibility of these approaches for early-stage quantum optimization, highlighting trade-offs in circuit constraints, encoding overhead, and scalability. Our findings suggest that quantum annealing currently offers the most scalable performance for this class of problems, while photonic and gate-based approaches remain limited by hardware and simulation depth. These results provide actionable insights for designing quantum workflows targeting combinatorial optimization in telecom security and resilience analysis.", "AI": {"tldr": "The paper benchmarks minimum edge multiway cut problem across three quantum computing paradigms (quantum annealing, photonic variational circuits, and gate-based QAOA) for telecom network resilience, finding quantum annealing currently most scalable.", "motivation": "To evaluate the feasibility of early-stage quantum optimization approaches for solving the minimum edge multiway cut problem, which is fundamental for assessing telecommunication network resilience and security.", "method": "Comparative benchmarking across three quantum computing paradigms: 1) quantum annealing on D-Wave QPU, 2) photonic variational quantum circuits simulated on Quandela's Perceval platform, and 3) gate-based Quantum Approximate Optimization Algorithm (QAOA) on IBM's quantum systems. The study analyzes trade-offs in circuit constraints, encoding overhead, and scalability.", "result": "Quantum annealing currently offers the most scalable performance for this class of combinatorial optimization problems. Photonic and gate-based approaches remain limited by hardware constraints and simulation depth. The study provides insights into the practical feasibility of each paradigm for early-stage quantum optimization.", "conclusion": "Quantum annealing is currently the most viable approach for minimum edge multiway cut problems in telecom resilience analysis, while photonic and gate-based methods face scalability limitations. The findings provide actionable guidance for designing quantum workflows targeting combinatorial optimization in telecom security applications."}}
{"id": "2601.00735", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.00735", "abs": "https://arxiv.org/abs/2601.00735", "authors": ["Alberto Acevedo", "Antonio Falc\u00f3"], "title": "Geometric Complexity of Quantum Channels via Unitary Dilations", "comment": null, "summary": "Nielsen's geometric approach to quantum circuit complexity provides a Riemannian framework for quantifying the cost of implementing unitary (closed--system) dynamics. For open dynamics, however, the reduced evolution is described by quantum channels and admits many inequivalent Stinespring realizations, so any meaningful complexity notion must specify which microscopic resources are counted as accessible and which transformations are regarded as gauge. We introduce and analyze a geometric complexity functional for families of quantum channels based on unitary dilations. We distinguish an implementation-dependent complexity, defined relative to explicit dilation data, from an intrinsic channel complexity obtained by minimizing over a physically motivated class of admissible dilations (e.g. bounded environment dimension, energy or norm constraints, and penalty structures). The functional has a subtractive form: it compares the geometric cost of the total unitary realization with a canonical surrogate term that removes purely environmental contributions. We justify this subtraction from concise postulates, including closed-system consistency, environment-only neutrality, and invariance under dilation gauge transformations that leave the channel unchanged. This leads to a companion quantity, noise complexity, quantifying the loss of geometric complexity relative to a prescribed ideal closed evolution. We establish a coherence-based lower bound for unitary geometric complexity, derive structural properties such as linear time scaling under time-homogeneous dilations, and obtain dissipator--controlled bounds in the Markovian (GKSL/Lindblad) regime under a standard dilation construction. Finally, we illustrate the framework on canonical benchmark noise models, including dephasing, amplitude damping, and depolarizing (Pauli) channels.", "AI": {"tldr": "The paper introduces a geometric complexity framework for quantum channels via unitary dilations, distinguishing implementation-dependent from intrinsic channel complexity, with subtractive functional and noise complexity measure.", "motivation": "Nielsen's geometric approach to quantum circuit complexity works for unitary dynamics but not for open quantum systems described by quantum channels, which have many inequivalent Stinespring realizations. A meaningful complexity notion for channels must specify which microscopic resources are accessible and which transformations are gauge.", "method": "Introduces geometric complexity functional for quantum channels based on unitary dilations, distinguishing implementation-dependent complexity (relative to explicit dilation data) from intrinsic channel complexity (minimized over physically motivated admissible dilations). Uses subtractive form comparing total unitary realization cost with canonical surrogate term removing environmental contributions. Justified by postulates: closed-system consistency, environment-only neutrality, and dilation gauge invariance.", "result": "Establishes coherence-based lower bound for unitary geometric complexity, derives structural properties including linear time scaling under time-homogeneous dilations, obtains dissipator-controlled bounds in Markovian (GKSL/Lindblad) regime under standard dilation construction. Illustrates framework on benchmark noise models: dephasing, amplitude damping, and depolarizing (Pauli) channels.", "conclusion": "The framework provides a principled geometric approach to quantifying complexity of quantum channels via unitary dilations, with subtractive functional separating channel complexity from environmental contributions, and introduces noise complexity to measure loss relative to ideal closed evolution."}}
{"id": "2601.00745", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00745", "abs": "https://arxiv.org/abs/2601.00745", "authors": ["Demerson N. Gon\u00e7alves", "Tharso D. Fernandes", "Pedro H. G. Lugao", "Jo\u00e3o T. Dias"], "title": "Training-Free Certified Bounds for Quantum Regression: A Scalable Framework", "comment": "16 pages, 3 tables", "summary": "We present a training-free, certified error bound for quantum regression derived directly from Pauli expectation values. Generalizing the heuristic of minimum accuracy from classification to regression, we evaluate axis-aligned predictors within the Pauli feature space. We formally prove that the optimal axis-aligned predictor constitutes a rigorous upper bound on the minimum training Mean Squared Error (MSE) attainable by any linear or kernel-based regressor defined on the same quantum feature map. Since computing this exact bound requires an intractable scan of the full Pauli basis, we introduce a Monte Carlo framework to efficiently estimate it using a tractable subset of measurement axes. We further provide non-asymptotic statistical guarantees to certify performance within a practical measurement budget. This method enables rapid comparison of quantum feature maps and early diagnosis of expressivity, allowing for the informed selection of architectures before deploying higher-complexity models.", "AI": {"tldr": "Training-free certified error bound for quantum regression derived from Pauli expectation values, providing rigorous upper bound on minimum MSE for linear/kernel regressors on quantum feature maps.", "motivation": "To enable rapid comparison of quantum feature maps and early diagnosis of expressivity without expensive training, allowing informed selection of quantum architectures before deploying complex models.", "method": "Generalize minimum accuracy heuristic from classification to regression, evaluate axis-aligned predictors in Pauli feature space, prove optimal axis-aligned predictor provides rigorous upper bound on minimum MSE, introduce Monte Carlo framework with tractable subset of measurement axes, provide non-asymptotic statistical guarantees.", "result": "Formally proven certified error bound for quantum regression, Monte Carlo estimation framework with statistical guarantees, enabling efficient comparison of quantum feature maps and expressivity diagnosis.", "conclusion": "Training-free certified error bounds derived from Pauli expectation values provide practical tool for quantum architecture selection and expressivity assessment before committing to expensive training of complex quantum models."}}
{"id": "2601.00761", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.00761", "abs": "https://arxiv.org/abs/2601.00761", "authors": ["Zhenyu Xiao", "Shinsei Ryu"], "title": "Exponentially Accelerated Sampling of Pauli Strings for Nonstabilizerness", "comment": null, "summary": "Quantum magic, quantified by nonstabilizerness, measures departures from stabilizer structure and underlies potential quantum speedups. We introduce an efficient classical algorithm that exactly computes stabilizer R\u00e9nyi entropies and stabilizer nullity for generic many-body wavefunctions of $N$ qubits. The method combines the fast Walsh-Hadamard transform with an exact partition of Pauli operators. It achieves an exponential speedup over direct approaches, reducing the average cost per sampled Pauli string from $O(2^N)$ to $O(N)$. Building on this framework, we further develop a Monte-Carlo estimator for stabilizer R\u00e9nyi entropies together with a Clifford-based variance-reduction scheme that suppresses sampling fluctuations. We benchmark the accuracy and efficiency on ensembles of random magic states, and apply the method to random Clifford circuits with doped $T$ gates, comparing different doping architectures. Our approach applies to arbitrary quantum states and provides quantitative access to magic resources both encoded in highly entangled states and generated by long-time nonequilibrium dynamics.", "AI": {"tldr": "Efficient classical algorithm for computing stabilizer R\u00e9nyi entropies and nullity with exponential speedup from O(2^N) to O(N) per Pauli string, plus Monte-Carlo estimator with Clifford-based variance reduction.", "motivation": "Quantum magic (nonstabilizerness) is crucial for quantum speedups but challenging to compute efficiently for many-body systems. Current methods are computationally expensive, limiting practical analysis of magic resources in complex quantum states.", "method": "Combines fast Walsh-Hadamard transform with exact partition of Pauli operators for exponential speedup. Develops Monte-Carlo estimator for stabilizer R\u00e9nyi entropies with Clifford-based variance reduction to suppress sampling fluctuations.", "result": "Achieves exponential speedup from O(2^N) to O(N) average cost per sampled Pauli string. Benchmarks show accurate and efficient computation on random magic states and random Clifford circuits with doped T gates across different architectures.", "conclusion": "Provides efficient quantitative access to magic resources in arbitrary quantum states, enabling analysis of highly entangled states and nonequilibrium dynamics. The method is broadly applicable to practical quantum computing scenarios."}}
{"id": "2601.00772", "categories": ["quant-ph", "math.LO"], "pdf": "https://arxiv.org/pdf/2601.00772", "abs": "https://arxiv.org/abs/2601.00772", "authors": ["Dietmar Dorninger", "Helmut L\u00e4nger"], "title": "On orthoposets of numerical events in quantum logic", "comment": null, "summary": "Let S be a set of states of a physical system and p(s) the probability of the occurrence of an event when the system is in state s in S. Such a function p from S to [0,1] is known as a numerical event or more accurately an S-probability. A set P of numerical events including the constant functions 0 and 1 and 1-p with every p in P becomes a poset when ordered by the order of real functions and can serve as a general setting for quantum logics. We call such a poset P a general set of events (GSE). The thoroughly investigated algebras of S-probabilities (including Hilbert logics), concrete logics and Boolean algebras can all be represented within this setting. In this paper we study various classes of GSEs, in particular those that are orthoposets and their interrelations and connections to known logics. Moreover, we characterize GSEs as posets by means of states and discuss the situation for GSEs to be lattices.", "AI": {"tldr": "General sets of events (GSEs) provide a unified framework for quantum logics, representing numerical event functions as posets with specific closure properties, enabling study of orthoposets and lattice structures.", "motivation": "To establish a general mathematical framework that unifies various quantum logic systems (including algebras of S-probabilities, concrete logics, and Boolean algebras) by representing them as posets of numerical event functions with specific closure properties.", "method": "Define general sets of events (GSEs) as posets of functions p:S\u2192[0,1] (S-probabilities) that include constant functions 0 and 1 and are closed under complementation (1-p). Study various classes of GSEs, particularly orthoposets, analyze their interrelations and connections to known logics, characterize GSEs via states, and investigate conditions for GSEs to form lattices.", "result": "GSEs provide a comprehensive framework that encompasses algebras of S-probabilities, Hilbert logics, concrete logics, and Boolean algebras. The paper establishes characterizations of GSEs as posets through state-based representations and analyzes conditions under which GSEs form lattice structures.", "conclusion": "General sets of events offer a unifying mathematical framework for quantum logics that generalizes existing approaches, with the poset structure providing a foundation for studying orthoposets, lattice properties, and connections between different quantum logic systems."}}
