<div id=toc></div>

# Table of Contents

- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [quant-ph](#quant-ph) [Total: 45]


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [1] [Low energy excitations in a long prism geometry: computing the lower critical dimension of the Ising spin glass](https://arxiv.org/abs/2601.07926)
*Massimo Bernaschi,Luis Antonio Fernández,Isidoro González-Adalid Pemartín,Víctor Martín-Mayor,Giorgio Parisi,Federico Ricci-Tersenghi*

Main category: cond-mat.dis-nn

TL;DR: A method using rectangular prisms to study systems with low-energy excitations, applied to Ising spin glasses to compute lower critical dimension and multifractal spectrum.


<details>
  <summary>Details</summary>
Motivation: To develop a general approach for studying systems with arbitrarily low-energy excitations in their low-temperature phase, particularly challenging systems like spin glasses where conventional methods struggle.

Method: Use rectangular prism geometry with longitudinal size >> transverse size; analyze correlation length scaling with transverse size; apply to Ising spin glasses with large-scale simulations using Houdayer's cluster method and open boundary conditions.

Result: Computed lower critical dimension and multifractal spectrum for correlation function; found lower critical dimension agrees with both Replica Symmetry Breaking and Droplet model predictions; studied 3D prisms with transverse dimensions up to L=24.

Conclusion: The novel prism geometry method successfully extracts critical information from spin glasses and shows promise for distinguishing between competing theories (Replica Symmetry Breaking vs Droplet model) in 3D spin glasses.

Abstract: We propose a general method for studying systems that display excitations with arbitrarily low energy in their low-temperature phase. We argue that in a rectangular right prism geometry, with longitudinal size much larger than the transverse size, correlations decay exponentially (at all temperatures) along the longitudinal dimension, but the scaling of the correlation length with the transverse size carries crucial information from which the lower critical dimension can be inferred. The method is applied in the particularly demanding context of Ising spin glasses at zero magnetic field. The lower critical dimension and the multifractal spectrum for the correlation function are computed from large-scale numerical simulations. Several technical novelties (such as the unexpectedly crucial performance of Houdayer's cluster method or the convenience of using open - rather than periodic - boundary conditions) allow us to study three-dimensional prisms with transverse dimensions up to $L=24$ and effectively infinite longitudinal dimensions down to low temperatures. The value that we find for the lower critical dimension turns out to be in agreement with expectations from both the Replica Symmetry Breaking theory and the Droplet model for spin glasses. We argue that our novel setting holds promise in clarifying which of the two competing theories more accurately describes three-dimensional spin glasses.

</details>


### [2] [Critical quantum states and hierarchical spectral statistics in a Cantor potential](https://arxiv.org/abs/2601.08324)
*F. Iwase*

Main category: cond-mat.dis-nn

TL;DR: A 1D quantum system with Cantor-type fractal potential exhibits hierarchical spectral statistics, bimodal level-spacing distribution, multifractal eigenstates, and anomalous IDS scaling connecting geometric fractality to quantum criticality.


<details>
  <summary>Details</summary>
Motivation: To understand how deterministic fractal geometry imprints on quantum spectral statistics and wave-function properties, contrasting with periodic and random systems.

Method: Analysis of nearest-neighbor level spacings, inverse participation ratio (IPR), scaling behavior of integrated density of states (IDS), and multifractal analysis of eigenstates using generalized fractal dimensions D_q.

Result: Hierarchical filamentary spectral structure, bimodal level-spacing distribution, critical multifractal eigenstates with D_q between extended/localized limits, anomalous IDS power-law scaling with exponent matching Cantor set Hausdorff dimension at low energies.

Conclusion: Direct connection established between deterministic fractal geometry, hierarchical spectral statistics, and quantum criticality, with geometric fractality governing spectral dimensionality.

Abstract: We study the spectral statistics and wave-function properties of a one-dimensional quantum system subject to a Cantor-type fractal potential. By analyzing the nearest-neighbor level spacings, inverse participation ratio (IPR), and the scaling behavior of the integrated density of states (IDS), we demonstrate how the self-similar geometry of the potential is imprinted on the quantum spectrum. The energy-resolved level spacings form a hierarchical, filamentary structure, in sharp contrast to those of periodic and random systems. The normalized level-spacing distribution exhibits a bimodal structure, reflecting the deterministic recurrence of spectral gaps. A multifractal analysis of eigenstates reveals critical behavior: the generalized fractal dimensions $D_q$ lie strictly between the limits of extended and localized states, exhibiting a distinct $q$-dependence. Consistently, the IPR indicates the coexistence of quasi-extended and localized features, characteristic of critical wave functions. The IDS shows anomalous power-law scaling at low energies, with an exponent close to the Hausdorff dimension of the underlying Cantor set, indicating that the geometric fractality governs the spectral dimensionality. At higher energies, this scaling crosses over to the semiclassical Weyl law. Our results establish a direct connection between deterministic fractal geometry, hierarchical spectral statistics, and quantum criticality.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [3] [LOTUS: Layer-ordered Temporally Unified Schedules For Quantum Approximate Optimization Algorithms](https://arxiv.org/abs/2601.07851)
*Phuong-Nam Nguyen*

Main category: quant-ph

TL;DR: LOTUS framework restructures QAOA into low-dimensional dynamical system using Hybrid Fourier-Autoregressive mapping, achieving significant performance improvements and computational efficiency gains.


<details>
  <summary>Details</summary>
Motivation: To address the high-dimensional, chaotic search nature of QAOA optimization by transforming it into a more structured, low-dimensional dynamical system that maintains global temporal coherence while preserving local flexibility.

Method: Introduces LOTUS framework with Hybrid Fourier-Autoregressive (HFA) mapping that replaces independent layer-wise angles, enforcing global temporal coherence through structured parameterization while maintaining local optimization flexibility.

Result: LOTUS consistently outperforms standard optimizers: 27.2% improvement in expectation values over L-BFGS-B, 20.8% over COBYLA, and reduces computational costs by requiring over 90% fewer iterations than Powell or SLSQP methods.

Conclusion: LOTUS successfully transforms QAOA optimization from chaotic high-dimensional search to efficient low-dimensional dynamical system, achieving both superior performance and significant computational efficiency gains.

Abstract: In this paper, we introduce LOTUS (Layer-Ordered Temporally-Unified Schedules), which is a framework that restructures QAOA from a high-dimensional, chaotic search into a low-dimensional dynamical system. By replacing independent layer-wise angles with a Hybrid Fourier-Autoregressive (HFA) mapping, LOTUS enforces global temporal coherence while maintaining local flexibility. LOTUS consistently outperforms standard optimizers, achieving up to a $27.2\%$ improvement in expectation values over L-BFGS-B and $20.8\%$ compared with COBYLA. Besides, our proposed method drastically reduces computational costs, requiring over $90\%$ fewer iterations than methods like Powell or SLSQP.

</details>


### [4] [Feature Entanglement-based Quantum Multimodal Fusion Neural Network](https://arxiv.org/abs/2601.07856)
*Yu Wu,Qianli Zhou,Jie Geng,Xinyang Deng,Wen Jiang*

Main category: quant-ph

TL;DR: Quantum multimodal fusion network achieves comparable accuracy to classical models with far fewer parameters while maintaining interpretability through feature entanglement.


<details>
  <summary>Details</summary>
Motivation: Address the accuracy-interpretability-complexity dilemma in multimodal learning, where classical approaches face trade-offs between black-box feature-level fusion (high accuracy but low interpretability) and decision-level fusion (interpretable but lower accuracy), along with challenges of parameter explosion and complexity.

Method: Proposes a feature entanglement-based quantum multimodal fusion neural network with three components: classical feed-forward module for unimodal processing, interpretable quantum fusion block, and quantum convolutional neural network (QCNN) for deep feature extraction. Leverages quantum expressive power to reduce complexity to linear.

Result: Simulation results demonstrate classification accuracy comparable to classical networks with dozens of times more parameters, exhibiting notable stability and performance across multimodal image datasets.

Conclusion: The quantum framework successfully addresses the multimodal learning dilemma by achieving high accuracy with interpretability and reduced complexity through quantum feature entanglement, offering a promising approach for efficient multimodal fusion.

Abstract: Multimodal learning aims to enhance perceptual and decision-making capabilities by integrating information from diverse sources. However, classical deep learning approaches face a critical trade-off between the high accuracy of black-box feature-level fusion and the interpretability of less outstanding decision-level fusion, alongside the challenges of parameter explosion and complexity. This paper discusses the accuracy-interpretablity-complexity dilemma under the quantum computation framework and propose a feature entanglement-based quantum multimodal fusion neural network. The model is composed of three core components: a classical feed-forward module for unimodal processing, an interpretable quantum fusion block, and a quantum convolutional neural network (QCNN) for deep feature extraction. By leveraging the strong expressive power of quantum, we have reduced the complexity of multimodal fusion and post-processing to linear, and the fusion process also possesses the interpretability of decision-level fusion. The simulation results demonstrate that our model achieves classification accuracy comparable to classical networks with dozens of times of parameters, exhibiting notable stability and performance across multimodal image datasets.

</details>


### [5] [Fault-Tolerant Quantum Error Correction: Implementing Hamming-Based Codes with Advanced Syndrome Extraction Techniques](https://arxiv.org/abs/2601.07860)
*Soham Bhadra,Diyansha Singh,Angana Chowdhury*

Main category: quant-ph

TL;DR: The paper compares three syndrome measurement strategies for quantum error correction, demonstrating significant improvements in error suppression and logical fidelity through intelligent ancilla management.


<details>
  <summary>Details</summary>
Motivation: Quantum error correction is essential for reliable quantum computing, but syndrome extraction using ancilla qubits introduces vulnerabilities where ancilla failures can cascade errors and destroy quantum information.

Method: Implemented and compared three syndrome measurement strategies: Shor's cat-state approach (multiple entangled ancillas), Steane's encoded-ancilla method (error-corrected logical qubits), and a flexible unified framework adapting to hardware capabilities. Used IBM's Qiskit platform for extensive simulations including randomized benchmarking and T-heavy circuits.

Result: Achieved up to 2.4× error suppression improvement, logical error rates as low as 5.1×10⁻⁵ under realistic noise (physical error rate 10⁻³), near-unity logical fidelity (0.99997) for deep circuits, and robust performance across distance-3 to distance-13 codes with characteristic threshold curves showing exponential error suppression below critical physical error rates.

Conclusion: The study provides immediately deployable tools for near-term quantum devices and establishes practical design principles for scaling toward fault-tolerant quantum computers through intelligent ancilla management in syndrome extraction.

Abstract: Building reliable quantum computers requires protecting fragile quantum states from inevitable environmental noise and operational errors. While quantum error correction codes like the Steane $[\![7,1,3]\!]$ code provide elegant theoretical solutions, their practical success hinges critically on how we measure errors - a process called syndrome extraction. The challenge lies in the ancilla qubits used for measurement: when they fail, errors can cascade across the entire quantum system, destroying the very information we're trying to protect. We address this fundamental problem by implementing and comparing three sophisticated syndrome measurement strategies: Shor's cat-state approach, which distributes measurements across multiple entangled ancillas achieving 85-92% preparation success; Steane's encoded-ancilla method using complete error-corrected logical qubits reaching 97.8% syndrome fidelity; and a flexible unified framework that adapts strategies based on hardware capabilities. Through extensive simulations using IBM's Qiskit platform spanning randomized benchmarking and T-heavy circuits, we demonstrate that intelligent ancilla management improves error suppression by up to 2.4$\times$ compared to standard approaches. Our implementations achieve logical error rates as low as $5.1 \times 10^{-5}$ under realistic noise conditions with physical error rates of $10^{-3}$, while maintaining near-unity logical fidelity (0.99997) even for deep circuits. The threshold analysis reveals robust performance across distance-3 to distance-13 codes with characteristic threshold curves showing exponential error suppression below the critical physical error rate. These results provide immediately deployable tools for near-term quantum devices and establish practical design principles for scaling toward fault-tolerant quantum computers.

</details>


### [6] [Quantum Computing and Visualization Research Challenges and Opportunities](https://arxiv.org/abs/2601.07872)
*E. Wes Bethel,Roel Van Beeumen,Talita Perciano*

Main category: quant-ph

TL;DR: The paper examines visualization research challenges and opportunities in quantum computing, from initial feasibility to practical applications.


<details>
  <summary>Details</summary>
Motivation: Quantum computing has seen rapid growth with accessible tools and platforms, creating a need to understand how visualization can support the development and practical application of quantum computing methods.

Method: The article takes a perspective from the visualization field, examining research challenges and opportunities along the development path from initial feasibility to practical use of quantum computing platforms.

Result: The paper identifies key research challenges and opportunities for visualization in quantum computing, though specific findings are not detailed in the abstract.

Conclusion: There is significant potential for visualization research to contribute to the development and practical application of quantum computing technologies, addressing challenges along the path from feasibility to meaningful problem-solving.

Abstract: Quantum computing (QC) has experienced rapid growth in recent years with the advent of robust programming environments, readily accessible software simulators and cloud-based QC hardware platforms, and growing interest in learning how to design useful methods that leverage this emerging technology for practical applications. From the perspective of the field of visualization, this article examines research challenges and opportunities along the path from initial feasibility to practical use of QC platforms applied to meaningful problems.

</details>


### [7] [Tackling Heterogeneity in Quantum Federated Learning: An Integrated Sporadic-Personalized Approach](https://arxiv.org/abs/2601.07882)
*Ratun Rahman,Shaba Shaon,Dinh C. Nguyen*

Main category: quant-ph

TL;DR: SPQFL is a novel quantum federated learning framework that addresses quantum noise heterogeneity through sporadic learning and data heterogeneity through personalized learning with model regularization, achieving improved training performance and convergence stability.


<details>
  <summary>Details</summary>
Motivation: Existing QFL frameworks struggle with optimal model training due to two key heterogeneities: (1) quantum noise heterogeneity (varying noise levels across quantum devices due to device quality and decoherence), and (2) data heterogeneity (non-IID data distributions across participating quantum devices).

Method: Proposes SPQFL (Sporadic-Personalized Quantum Federated Learning) with two key components: (1) sporadic learning to handle quantum noise heterogeneity across devices, and (2) personalized learning through model regularization to mitigate overfitting on non-IID quantum data and enhance global model convergence.

Result: Theoretical convergence analysis shows SPQFL's upper bound is influenced by both number of quantum devices and quantum noise measurements. Extensive simulations on real-world datasets demonstrate significant improvements in training performance and convergence stability compared to state-of-the-art methods.

Conclusion: SPQFL successfully addresses both quantum noise and data heterogeneity challenges in QFL through an integrated sporadic-personalized approach, providing a robust framework for distributed quantum learning with improved performance and theoretical guarantees.

Abstract: Quantum federated learning (QFL) emerges as a powerful technique that combines quantum computing with federated learning to efficiently process complex data across distributed quantum devices while ensuring data privacy in quantum networks. Despite recent research efforts, existing QFL frameworks struggle to achieve optimal model training performance primarily due to inherent heterogeneity in terms of (i) quantum noise where current quantum devices are subject to varying levels of noise due to varying device quality and susceptibility to quantum decoherence, and (ii) heterogeneous data distributions where data across participating quantum devices are naturally non-independent and identically distributed (non-IID). To address these challenges, we propose a novel integrated sporadic-personalized approach called SPQFL that simultaneously handles quantum noise and data heterogeneity in a single QFL framework. It is featured in two key aspects: (i) for quantum noise heterogeneity, we introduce a notion of sporadic learning to tackle quantum noise heterogeneity across quantum devices, and (ii) for quantum data heterogeneity, we implement personalized learning through model regularization to mitigate overfitting during local training on non-IID quantum data distributions, thereby enhancing the convergence of the global model. Moreover, we conduct a rigorous convergence analysis for the proposed SPQFL framework, with both sporadic and personalized learning considerations. Theoretical findings reveal that the upper bound of the SPQFL algorithm is strongly influenced by both the number of quantum devices and the number of quantum noise measurements. Extensive simulation results in real-world datasets also illustrate that the proposed SPQFL approach yields significant improvements in terms of training performance and convergence stability compared to the state-of-the-art methods.

</details>


### [8] [Local Scale Invariance in Quantum Theory: Experimental Predictions](https://arxiv.org/abs/2601.07883)
*Indrajit Sen,Matthew Leifer*

Main category: quant-ph

TL;DR: The paper explores experimental predictions of a local scale invariant, non-Hermitian pilot-wave quantum theory, predicting measurable effects in Aharonov-Bohm experiments and spectral line modifications.


<details>
  <summary>Details</summary>
Motivation: To investigate experimental consequences of a novel quantum theory formulation that combines local scale invariance with non-Hermitian pilot-wave mechanics, addressing Einstein's objections and seeking empirical verification.

Method: Uses Weyl's gravitational radius of charge to derive fine-structure constant for non-integrable scale effects, analyzes Aharonov-Bohm double-slit experiments with heavy molecules, and examines spectral properties using the theory's framework.

Result: Predicts position probability density depends on which slit particle crosses in Aharonov-Bohm experiments, resolves Einstein's objection by showing spectral frequencies are history-independent, finds spectral intensities are history-dependent, and predicts tiny imaginary corrections to energy eigenvalues affecting spectral linewidths.

Conclusion: The theory's trajectory dependence of probabilities makes it empirically distinguishable from other quantum formulations, with testable predictions for Aharonov-Bohm experiments and spectral measurements despite extremely small scale effects.

Abstract: We explore the experimental predictions of the local scale invariant, non-Hermitian pilot-wave (de Broglie-Bohm) formulation of quantum theory introduced in arXiv:2601.03567. We use Weyl's definition of gravitational radius of charge to obtain the fine-structure constant for non-integrable scale effects $α_S$. The minuteness of $α_S$ relative to $α$ ($α_S/α\sim 10^{-21}$) effectively hides the effects in usual quantum experiments. In an Aharonov-Bohm double-slit experiment, the theory predicts that the position probability density depends on which slit the particle trajectory crosses, due to a non-integrable scale induced by the magnetic flux. This experimental prediction can be realistically tested for an electrically neutral, heavy molecule with mass $m \sim 10^{-19} \text{g}$ at a $\sim 10^6 \text{ esu}$ flux regime. We analyse the Weyl-Einstein debate on the second-clock effect using the theory and show that spectral frequencies are history-independent. We thereby resolve Einstein's key objection against local scale invariance, and obtain two further experimental predictions. First, spectral intensities turn out to be history-dependent. Second, energy eigenvalues are modified by tiny imaginary corrections that modify spectral linewidths. We argue that the trajectory dependence of the probabilities renders our theory empirically distinguishable from other quantum formulations that do not use pilot-wave trajectories, or their mathematical equivalents, to derive experimental predictions.

</details>


### [9] [Quantum circuit compilation for fermionic excitations using the Jordan-Wigner mapping](https://arxiv.org/abs/2601.07890)
*Renata Wong*

Main category: quant-ph

TL;DR: The paper details the Jordan-Wigner mapping for UCCSD ansatz, explicitly deriving Pauli strings for hydrogen molecule excitations and discussing circuit implementation nuances.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between theoretical second quantization formalism and practical quantum hardware implementation by providing explicit derivations and implementation details for the UCCSD ansatz using Jordan-Wigner mapping.

Method: Use Jordan-Wigner mapping to translate fermionic operators to Pauli strings for UCCSD ansatz, explicitly derive Pauli strings for single and double excitations of hydrogen molecule in minimal basis, and discuss circuit implementation including mathematical rotations vs physical gates like SX.

Result: Explicit derivation of Pauli strings required for UCCSD excitations for hydrogen molecule, with detailed mapping between fermionic operators and quantum circuit implementations, including practical gate-level considerations.

Conclusion: The paper successfully bridges theory and practice by providing concrete implementation details for UCCSD on quantum hardware, highlighting important nuances in translating mathematical formulations to physical quantum circuits.

Abstract: This note bridges the gap between theoretical second quantization and practical quantum hardware by detailing the Jordan-Wigner mapping for the Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz. Using the hydrogen molecule in a minimal basis as a case study, we explicitly derive the Pauli strings required for single and double excitations. Additionally, we discuss the translation of these operators into quantum circuits, with a focus on implementation nuances such as the difference between mathematical rotations and physical gates like the $\sqrt{X}$ (SX) gate.

</details>


### [10] [Bohmian mechanics: A legitimate hydrodynamic picture for quantum mechanics, and beyond](https://arxiv.org/abs/2601.07932)
*A. S. Sanz*

Main category: quant-ph

TL;DR: Bohmian mechanics has evolved from a controversial hidden-variable theory to a practical computational tool, shifting from philosophical resistance to pragmatic acceptance in quantum mechanics applications.


<details>
  <summary>Details</summary>
Motivation: To analyze the historical evolution of Bohmian mechanics from a controversial hidden-variable theory to a practical computational tool, and to assess whether it should be considered a legitimate quantum representation worthy of inclusion in elementary quantum mechanics courses.

Method: The paper re-examines the Schrödinger equation and several specific numerical examples through a less restrictive view than the standard quantum mechanics approach, analyzing the operational and practical applications of Bohmian mechanics that have emerged since the late 1990s/early 2000s.

Result: Bohmian mechanics has undergone a significant perspective shift - the original hidden-variable framework has faded, replaced by a pragmatic approach that treats it as an analytical and computational tool, leading to gradual acceptance in various physics branches.

Conclusion: Bohmian mechanics should be considered a legitimate quantum representation due to its practical utility as an analytical and computational tool, and these ideas could potentially benefit other fields through transfer of the pragmatic approach developed in quantum mechanics.

Abstract: Since its inception, Bohmian mechanics has been surrounded by a halo of controversy. Originally proposed to bypass the limitations imposed by von Neumann's theorem on the impossibility of hidden-variable models in quantum mechanics, it faced strong opposition from the outset. Over time, however, its use in tackling specific problems across various branches of physics has led to a gradual shift in attitude, turning the early resistance into a more moderate acceptance. A plausible explanation for this change may be that, since the late 1990s and early 2000s, Bohmian mechanics has been taking on a more operational and practical role. The original hidden-variable idea has gradually faded from its framework, giving way to a more pragmatic approach that treats it as a suitable analytical and computational tool. This discussion explores how and why such a shift in perspective has occurred and, therefore, answers questions such as whether Bohmian mechanics should be considered once and for all a legitimate quantum representation (i.e., worth being taught in elementary quantum mechanics courses) or, by extension, whether these ideas can be transferred to and benefit other fields. Here, the Schrödinger equation and several specific numerical examples are re-examined in the light of a less restrictive view than the standard one usually adopted in quantum mechanics.

</details>


### [11] [Data-driven learning of non-Markovian quantum dynamics](https://arxiv.org/abs/2601.07934)
*Samuel Goodwin,Brian K. McFarland,Manuel H. Muñoz-Arias,Edward C. Tortorici,Melissa C. Revelle,Christopher G. Yale,Daniel S. Lobser,Susan M. Clark,Mohan Sarovar*

Main category: quant-ph

TL;DR: A data-driven protocol learns NMZ operators for quantum gate characterization, capturing non-Markovian dynamics from time series data across simulated and experimental qubit systems.


<details>
  <summary>Details</summary>
Motivation: Fault-tolerant quantum computing requires precise knowledge and control of qubit dynamics during gate operations. Existing characterization methods often fail to capture non-Markovian noise sources, limiting accurate diagnosis of gate errors.

Method: Data-driven learning protocol based on the Nakajima-Mori-Zwanzig (NMZ) formulation of open system dynamics. The method learns NMZ operators from time series data to reconstruct quantum evolution, including non-Markovian dynamics. Applied to three systems: simulated Markovian qubit, simulated driven qubit with Ornstein-Uhlenbeck noise, and trapped-ion experimental data with uncharacterized noise.

Result: The technique successfully learns NMZ operators (generators of time evolution) in all three cases. It identifies timescales where qubit dynamics deviate from purely Markovian models. The method captures non-Markovianity in gate generators, enabling more thorough noise source diagnosis.

Conclusion: This NMZ-based learning protocol complements existing quantum gate characterization methods like gate set tomography by explicitly capturing non-Markovian dynamics. It provides enhanced diagnostic capabilities for noise sources in quantum gates, advancing fault-tolerant quantum computing requirements.

Abstract: Fault-tolerant quantum computing requires extremely precise knowledge and control of qubit dynamics during the application of a gate. We develop a data-driven learning protocol for characterizing quantum gates that builds off previous work on learning the Nakajima-Mori-Zwanzig (NMZ) formulation of open system dynamics from time series data, which allows detailed reconstruction of quantum evolution, including non-Markovian dynamics. We demonstrate this learning technique on three different systems: a simulation of a qubit whose dynamics are purely Markovian, a simulation of a driven qubit coupled to stochastic noise produced by an Ornstein-Uhlenbeck process, and trapped-ion experimental data of a driven qubit whose noise environment is not characterized ahead of time. Our technique is able to learn the generators of time evolution, or the NMZ operators, in all three cases and can learn the timescale in which the qubit dynamics can no longer be accurately described by a purely Markovian model. Our technique complements existing quantum gate characterization methods such as gate set tomography by explicitly capturing non-Markovianity in the gate generator, thus allowing for more thorough diagnosis of noise sources.

</details>


### [12] [Attention in Krylov Space](https://arxiv.org/abs/2601.07937)
*Zihao Qi,Christopher Earls*

Main category: quant-ph

TL;DR: Transformer-based model predicts future Lanczos coefficients from short prefixes, outperforming asymptotic fits in both coefficient extrapolation and observable reconstruction for classical and quantum systems.


<details>
  <summary>Details</summary>
Motivation: Numerical instability and memory cost limit exact computation of Lanczos coefficients in the Universal Operator Growth Hypothesis. Standard asymptotic fitting approaches miss subleading, history-dependent structures that affect reconstructed observables.

Method: Treat Lanczos coefficients as causal time sequence and introduce transformer-based model to autoregressively predict future coefficients from short prefixes. Model transfers across system sizes and uses attention mechanisms to identify influential portions of coefficient history.

Result: Machine-learning model outperforms asymptotic fits by achieving order-of-magnitude reduction in error for both coefficient extrapolation and physical observable reconstruction. Model transfers across system sizes without retraining. Attention analysis identifies which portions of coefficient history are most influential for accurate forecasts.

Conclusion: Transformer-based approach provides superior alternative to asymptotic fitting for predicting Lanczos coefficients, capturing history-dependent structures that affect physical observables, with transfer learning capabilities across system sizes.

Abstract: The Universal Operator Growth Hypothesis formulates time evolution of operators through Lanczos coefficients. In practice, however, numerical instability and memory cost limit the number of coefficients that can be computed exactly. In response to these challenges, the standard approach relies on fitting early coefficients to asymptotic forms, but such procedures can miss subleading, history-dependent structures in the coefficients that subsequently affect reconstructed observables. In this work, we treat the Lanczos coefficients as a causal time sequence and introduce a transformer-based model to autoregressively predict future Lanczos coefficients from short prefixes. For both classical and quantum systems, our machine-learning model outperforms asymptotic fits, in both coefficient extrapolation and physical observable reconstruction, by achieving an order-of-magnitude reduction in error. Our model also transfers across system sizes: it can be trained on smaller systems and then be used to extrapolate coefficients on a larger system without retraining. By probing the learned attention patterns and performing targeted attention ablations, we identify which portions of the coefficient history are most influential for accurate forecasts.

</details>


### [13] [Quantum automated theorem proving](https://arxiv.org/abs/2601.07953)
*Zheng-Zhi Sun,Qi Ye,Dong-Ling Deng*

Main category: quant-ph

TL;DR: Quantum automated theorem proving framework using quantum superposition and entanglement to achieve quadratic speedup in query complexity for propositional, first-order logic, and geometric theorem proving.


<details>
  <summary>Details</summary>
Motivation: Automated theorem proving is essential for various applications, and enhancing theorem-proving capabilities remains a key pursuit in AI. The authors aim to leverage quantum computing's superposition and entanglement features to achieve potential advantages over classical approaches.

Method: Proposes a generic quantum automated theorem proving framework with quantum representations of knowledge bases and corresponding reasoning algorithms. Introduces quantum resolution for propositional and first-order logic, and quantum algebraic proving method for geometric theorems (extending Wu's algebraic approach).

Result: Achieves quadratically reduced query complexity for automated reasoning with quantum resolution in both propositional and first-order logic. Demonstrates quantum computers can prove geometric theorems with quadratic better query complexity, including geometry problems from International Mathematical Olympiad.

Conclusion: Establishes a primary approach toward building quantum automatic theorem provers, which would be crucial for practical applications of both near-term and future quantum technologies.

Abstract: Automated theorem proving, or more broadly automated reasoning, aims at using computer programs to automatically prove or disprove mathematical theorems and logical statements. It takes on an essential role across a vast array of applications and the quest for enhanced theorem-proving capabilities remains a prominent pursuit in artificial intelligence. Here, we propose a generic framework for quantum automated theorem proving, where the intrinsic quantum superposition and entanglement features would lead to potential advantages. In particular, we introduce quantum representations of knowledge bases and propose corresponding reasoning algorithms for a variety of tasks. We show how automated reasoning can be achieved with quantum resolution in both propositional and first-order logic with quadratically reduced query complexity. In addition, we propose the quantum algebraic proving method for geometric theorems, extending Wu's algebraic approach beyond the classical setting. Through concrete examples, including geometry problems from the International Mathematical Olympiad, we demonstrate how a quantum computer may prove geometric theorems with quadratic better query complexity. Our results establish a primary approach towards building quantum automatic theorem provers, which would be crucial for practical applications of both near-term and future quantum technologies.

</details>


### [14] [Interferometric discrepancy between the Schrödinger and Klein-Gordon wave equations due to their dissimilar phase velocities](https://arxiv.org/abs/2601.08007)
*Frank Victor Kowalski*

Main category: quant-ph

TL;DR: Interference occurs when beamsplitter speed exceeds phase velocity of free massive particle momentum eigenstates, unlike electromagnetic waves or non-relativistic Klein-Gordon limit where this is impossible.


<details>
  <summary>Details</summary>
Motivation: To explore unique interference phenomena in quantum mechanics where beamsplitter motion relative to particle phase velocity creates interference effects not possible for electromagnetic waves or in non-relativistic limits.

Method: Analysis of Schrödinger equation predictions for beamsplitter trajectories with speeds exceeding phase velocity of free massive particles in momentum eigenstates, comparing with electromagnetic waves and non-relativistic Klein-Gordon equation limits.

Result: Identifies interference phenomena specific to quantum mechanical systems where beamsplitter speed can exceed particle phase velocity, unlike classical electromagnetic waves or non-relativistic quantum limits where this condition cannot be met.

Conclusion: Quantum mechanical systems exhibit unique interference effects when beamsplitter motion exceeds particle phase velocity, highlighting fundamental differences between quantum particle behavior and classical wave phenomena or non-relativistic approximations.

Abstract: The Schrödinger equation predicts interference when a beamsplitter's trajectory includes a segment where its speed exceeds the phase velocity of a free non-zero rest mass particle that is in a momentum eigenstate. Such interference is neither possible for electromagnetic waves nor for eigenstates of momentum in the non-relativistic limit of the Klein-Gordon equation since the speed of the beamsplitter cannot exceed the phase velocity of the wave. The dual behavior of reflection and transmission in this case is discussed for dielectric and diffracting beamsplitters.

</details>


### [15] [Learning Better Error Correction Codes with Hybrid Quantum-Assisted Machine Learning](https://arxiv.org/abs/2601.08014)
*Yariv Yanay*

Main category: quant-ph

TL;DR: A hybrid classical-quantum algorithm combining reinforcement learning with quantum device calls to search for stabilizer codes tailored to specific device errors and photon loss.


<details>
  <summary>Details</summary>
Motivation: To advance quantum error correction by developing codes specifically optimized for real quantum hardware imperfections, moving beyond purely classical code construction methods.

Method: Hybrid classical-quantum algorithm combining classical reinforcement learning with calls to two commercial quantum devices, using Quantum Lego formalism to systematically construct stabilizer codes from basic building blocks.

Result: Demonstrated ability to search for and identify stabilizer codes optimized for correcting device-specific errors and induced photon loss errors using real quantum hardware.

Conclusion: The hybrid approach successfully extends automated code discovery to real quantum devices, enabling hardware-tailored error correction that accounts for specific device imperfections.

Abstract: Quantum error correction is one of the fundamental building blocks of digital quantum computation. The Quantum Lego formalism has introduced a systematic way of constructing new stabilizer codes out of basic lego-like building blocks, which in previous work we have used to generate improved error correcting codes via an automated reinforcement learning process. Here, we take this a step further and show the use of a hybrid classical-quantum algorithm. We combine classical reinforcement learning with calls to two commercial quantum devices to search for a stabilizer code to correct errors specific to the device, as well as an induced photon loss error.

</details>


### [16] [On measurement-dependent variance in quantum neural networks](https://arxiv.org/abs/2601.08029)
*Andrey Kardashin,Konstantin Antipin*

Main category: quant-ph

TL;DR: Measuring observables with restricted support in variational quantum circuits leads to larger label prediction variance in regression QML tasks due to the limited number of distinct eigenvalues.


<details>
  <summary>Details</summary>
Motivation: In quantum machine learning tasks using variational quantum circuits, some architectures (like QCNNs) measure only restricted parts of the quantum state. The authors investigate how this restricted measurement affects prediction variance in regression tasks.

Method: Theoretical analysis demonstrating that measuring observables with limited support results in larger prediction variance, linking this phenomenon to the number of distinct eigenvalues of the measured observable after variational circuit application.

Result: Observables with restricted support produce larger label prediction variance in regression QML tasks, with the effect fundamentally tied to the limited number of distinct eigenvalues available for measurement outcomes.

Conclusion: The restricted support of measured observables in certain QML architectures (like QCNNs) inherently increases prediction variance, which has implications for the design and performance analysis of quantum machine learning models.

Abstract: Variational quantum circuits have become a widely used tool for performing quantum machine learning (QML) tasks on labeled quantum states. In some specific tasks or for specific variational ansätze, one may perform measurements on a restricted part of the overall input state. This is the case for, e.g., quantum convolutional neural networks (QCNNs), where after each layer of the circuit a subset of qubits of the processed state is measured or traced out, and at the end of the network one typically measures a local observable. In this work, we demonstrate that measuring observables with restricted support results in larger label prediction variance in regression QML tasks. We show that the reason for this is, essentially, the number of distinct eigenvalues of the observable one measures after the application of a variational circuit.

</details>


### [17] [Quantum Energetic Advantage before Computational Advantage in Boson Sampling](https://arxiv.org/abs/2601.08068)
*Ariane Soret,Nessim Dridi,Stephen C. Wein,Valérian Giesz,Shane Mansfield,Pierre-Emmanuel emeriau*

Main category: quant-ph

TL;DR: Quantum energetic advantage for Boson Sampling emerges before computational advantage, with photonic architecture enabling near-term observation.


<details>
  <summary>Details</summary>
Motivation: Understanding energetic efficiency is crucial for assessing quantum computer scalability and determining if quantum technologies can outperform classical computation beyond just runtime considerations.

Method: Using the Metric-Noise-Resource methodology to establish quantitative connections between experimental control parameters, dominant noise processes, and energetic resources through a performance metric tailored to Boson Sampling.

Result: Demonstrated quantum energetic advantage (lower energy cost per sample compared to best-known classical implementation) emerges before computational advantage, even where classical algorithms remain faster. Proposed experimentally feasible Boson Sampling architecture with complete noise and loss budget enables near-term observation.

Conclusion: Quantum energetic advantage for Boson Sampling can be achieved in near-term photonic quantum computing architectures, providing an important metric for quantum technology assessment beyond computational speed alone.

Abstract: Understanding the energetic efficiency of quantum computers is essential for assessing their scalability and for determining whether quantum technologies can outperform classical computation beyond runtime alone. In this work, we analyze the energy required to solve the Boson Sampling problem, a paradigmatic task for quantum advantage, using a realistic photonic quantum computing architecture. Using the Metric-Noise-Resource methodology, we establish a quantitative connection between experimental control parameters, dominant noise processes, and energetic resources through a performance metric tailored to Boson Sampling. We estimate the energy cost per sample and identify operating regimes that optimize energetic efficiency. By comparing the energy consumption of quantum and state-of-the-art classical implementations, we demonstrate the existence of a quantum energetic advantage -- defined as a lower energy cost per sample compared to the best-known classical implementation -- that emerges before the onset of computational advantage, even in regimes where classical algorithms remain faster. Finally, we propose an experimentally feasible Boson Sampling architecture, including a complete noise and loss budget, that enables a near-term observation of quantum energetic advantage.

</details>


### [18] [Learning parameter curves in feedback-based quantum optimization algorithms](https://arxiv.org/abs/2601.08085)
*Vicente Peña Pérez,Matthew D. Grace,Christian Arenz,Alicia B. Magann*

Main category: quant-ph

TL;DR: Machine learning can predict quantum algorithm parameters, reducing measurement costs in feedback-based quantum algorithms for MaxCut problems.


<details>
  <summary>Details</summary>
Motivation: Feedback-based quantum algorithms (FQAs) require costly qubit measurements at each iteration. The authors investigate whether classical machine learning can predict FQA parameter sequences to eliminate these measurement overheads.

Method: Train a teacher-student model to map MaxCut problem instances to FQA parameter curves in a single classical inference step. Use numerical experiments across various problem sizes, including unseen sizes during training.

Result: The model accurately predicts FQA parameter curves across different problem sizes. Predicted curves perform similarly to FQA reference curves and outperform linear quantum annealing schedules.

Conclusion: Machine learning offers a heuristic, practical approach to reduce sampling costs and resource overheads in quantum algorithms by predicting parameter sequences without qubit measurements.

Abstract: Feedback-based quantum algorithms (FQAs) operate by iteratively growing a quantum circuit to optimize a given task. At each step, feedback from qubit measurements is used to inform the next quantum circuit update. In practice, the sampling cost associated with these measurements can be significant. Here, we ask whether FQA parameter sequences can be predicted using classical machine learning, obviating the need for qubit measurements altogether. To this end, we train a teacher-student model to map a MaxCut problem instance to an associated FQA parameter curve in a single classical inference step. Numerical experiments show that this model can accurately predict FQA parameter curves across a range of problem sizes, including problem sizes not seen during model training. To evaluate performance, we compare the predicted parameter curves in simulation against FQA reference curves and linear quantum annealing schedules. We observe similar results to the former and performance improvements over the latter. These results suggest that machine learning can offer a heuristic, practical path to reducing sampling costs and resource overheads in quantum algorithms.

</details>


### [19] [Quantum observers can communicate across multiverse branches](https://arxiv.org/abs/2601.08102)
*Maria Violaris*

Main category: quant-ph

TL;DR: The paper demonstrates that inter-branch communication in Everettian quantum mechanics is possible within standard quantum theory, challenging conventional wisdom about multiverse limitations.


<details>
  <summary>Details</summary>
Motivation: To challenge the common belief that observers in different branches of an Everettian multiverse cannot communicate without violating quantum linearity, and to explore the fundamental limits of what's possible in quantum theory.

Method: Uses a Wigner's-friend thought experiment where Wigner has quantum control over the friend, creating a scenario where the friend in superposition can receive messages from distinct copies of themselves in other branches with Wigner's assistance.

Result: Demonstrates that inter-branch communication is possible within standard quantum theory, though it requires that observers have no memory of the messages they sent to maintain unitarity.

Conclusion: The thought experiment challenges conventional wisdom about Everettian multiverse limitations and has surprising applications for testing Everettian quantum theory against single-world theories using knowledge-creation paradoxes.

Abstract: It is commonly thought that observers in distinct branches of an Everettian multiverse cannot communicate without violating the linearity of quantum theory. Here we show a counterexample, demonstrating that inter-branch communication is in fact possible, entirely within standard quantum theory. We do this by considering a Wigner's-friend scenario, where an observer (Wigner) can have quantum control over another observer (the friend). We present a thought experiment where the friend in superposition can receive a message written by a distinct copy of themselves in the multiverse, with the aid of Wigner. To maintain the unitarity of quantum theory, the observers must have no memory of the message that they sent. Our thought experiment challenges conventional wisdom regarding the ultimate limits of what is possible in an Everettian multiverse. It has a surprising potential application which involves using knowledge-creation paradoxes for testing Everettian quantum theory against single-world theories.

</details>


### [20] [Cost scaling of MPS and TTNS simulations for 2D and 3D systems with area-law entanglement](https://arxiv.org/abs/2601.08132)
*Thomas Barthel*

Main category: quant-ph

TL;DR: TTNS reduce graph distances compared to MPS for D>1 systems, but surprisingly MPS remains more efficient for large systems despite TTNS's theoretical advantages.


<details>
  <summary>Details</summary>
Motivation: To understand why MPS simulations outperform TTNS for large D>1 systems despite TTNS's ability to reduce graph distances between physical degrees of freedom, and to analyze computational cost scaling under entanglement area laws.

Method: Theoretical analysis of computational cost scaling for TTNS and MPS under entanglement area law assumptions, focusing on D=2 and 3 spatial dimensions with different boundary conditions.

Result: For large systems in D>1 dimensions, MPS simulations of low-energy states are more efficient than TTNS simulations despite TTNS's reduced graph distances, due to scaling properties under entanglement area laws.

Conclusion: The theoretical advantage of TTNS in reducing graph distances does not translate to practical efficiency gains for large D>1 systems, with MPS remaining the more efficient approach for simulating low-energy states under entanglement area law constraints.

Abstract: Tensor network states are an indispensable tool for the simulation of strongly correlated quantum many-body systems. In recent years, tree tensor network states (TTNS) have been successfully used for two-dimensional systems and to benchmark quantum simulation approaches for condensed matter, nuclear, and particle physics. In comparison to the more traditional approach based on matrix product states (MPS), the graph distance of physical degrees of freedom can be drastically reduced in TTNS. Surprisingly, it turns out that, for large systems in $D>1$ spatial dimensions, MPS simulations of low-energy states are nevertheless more efficient than TTNS simulations. With a focus on $D=2$ and 3, the scaling of computational costs for different boundary conditions is determined under the assumption that the system obeys an entanglement (log-)area law, implying that bond dimensions scale exponentially in the surface area of the associated subsystems.

</details>


### [21] [Dissipative ground-state preparation of a quantum spin chain on a trapped-ion quantum computer](https://arxiv.org/abs/2601.08137)
*Kazuhiro Seki,Yuta Kikuchi,Tomoya Hayata,Seiji Yunoki*

Main category: quant-ph

TL;DR: Dissipative ground-state preparation protocol for quantum spin chains implemented on trapped-ion quantum computer with up to 19 spins, showing robustness despite hardware noise.


<details>
  <summary>Details</summary>
Motivation: To develop and experimentally demonstrate a dissipative protocol for ground-state preparation of quantum spin chains on quantum hardware, extending beyond Lindblad dynamics regime and showing robustness to noise.

Method: Derived Kraus representation of dissipation channel valid for arbitrary temporal discretization steps, implemented dissipative ground-state preparation of transverse-field Ising chain on Quantinuum's Reimei trapped-ion quantum computer with up to 4110 entangling gates, and applied zero-noise extrapolation.

Result: Protocol consistently converges to low-energy states far from maximally mixed state despite hardware noise, with zero-noise extrapolation improving energy expectation values to agree with noiseless simulations within statistical uncertainties.

Conclusion: Dissipative ground-state preparation protocol is intrinsically robust and effective for quantum spin chains on noisy quantum hardware, with theoretical framework extending beyond Lindblad regime.

Abstract: We demonstrate a dissipative protocol for ground-state preparation of a quantum spin chain on a trapped-ion quantum computer. As a first step, we derive a Kraus representation of a dissipation channel for the protocol recently proposed by Ding et al. [Phys. Rev. Res. 6, 033147 (2024)] that still holds for arbitrary temporal discretization steps, extending the analysis beyond the Lindblad dynamics regime. The protocol guarantees that the fidelity with the ground state monotonically increases (or remains unchanged) under repeated applications of the channel to an arbitrary initial state, provided that the ground state is the unique steady state of the dissipation channel. Using this framework, we implement dissipative ground-state preparation of a transverse-field Ising chain for up to 19 spins on the trapped-ion quantum computer Reimei provided by Quantinuum. Despite the presence of hardware noise, the dynamics consistently converges to a low-energy state far away from the maximally mixed state even when the corresponding quantum circuits contain as many as 4110 entangling gates, demonstrating the intrinsic robustness of the protocol. By applying zero-noise extrapolation, the resulting energy expectation values are systematically improved to agree with noiseless simulations within statistical uncertainties.

</details>


### [22] [Quantum State Discrimination Enhanced by FPGA-Based AI Engine Technology](https://arxiv.org/abs/2601.08213)
*Anastasiia Butko,Artem Marisov,David I. Santiago,Irfan Siddiqi*

Main category: quant-ph

TL;DR: FPGA-based AI Engine system enables real-time quantum state discrimination for superconducting qubits using neural networks, overcoming previous limitations of offline processing.


<details>
  <summary>Details</summary>
Motivation: Quantum state discrimination is critical but error-prone and time-consuming in superconducting quantum processors, with most methods requiring offline execution due to timing constraints and algorithmic complexity.

Method: Developed multi-layer neural network implemented on AMD Xilinx VCK190 FPGA platform using specialized AI/ML accelerators (AI Engine technology) for optimized real-time state discrimination.

Result: Enabled accurate in-situ state discrimination and supports mid-circuit measurement experiments for multiple qubits while reducing FPGA resource usage.

Conclusion: FPGA-based AI Engine approach provides enhanced real-time quantum state discrimination, leveraging specialized accelerators to overcome previous limitations and support advanced quantum computing experiments.

Abstract: Identifying the state of a quantum bit (qubit), known as quantum state discrimination, is a crucial operation in quantum computing. However, it has been the most error-prone and time-consuming operation on superconducting quantum processors. Due to stringent timing constraints and algorithmic complexity, most qubit state discrimination methods are executed offline. In this work, we present an enhanced real-time quantum state discrimination system leveraging FPGA-based AI Engine technology. A multi-layer neural network has been developed and implemented on the AMD Xilinx VCK190 FPGA platform, enabling accurate in-situ state discrimination and supporting mid-circuit measurement experiments for multiple qubits. Our approach leverages recent advancements in architecture research and design, utilizing specialized AI/ML accelerators to optimize quantum experiments and reduce the use of FPGA resources.

</details>


### [23] [Reference-frame-independent Quantum secure direct communication](https://arxiv.org/abs/2601.08238)
*Jia-Wei Ying,Shi-Pu Gu,Xing-Fu Wang,Wei Zhong,Ming-Ming Du,Xi-Yun Li,Shu-Ting Shen,An-Lei Zhang,Lan Zhou,Yu-Bo Sheng*

Main category: quant-ph

TL;DR: Proposes a reference-frame-independent quantum secure direct communication protocol that tolerates misalignment in two reference frame directions, extends communication distance significantly compared to single-photon-based QSDC.


<details>
  <summary>Details</summary>
Motivation: Current QSDC protocols require precise reference frame calibration between communicating parties, which is challenging in mobile communications scenarios where continuous accurate calibration is difficult to achieve.

Method: Develops an RFI QSDC protocol that only requires calibration accuracy in one reference frame direction while allowing misalignment angle β in the other two directions. Introduces β-independent parameter C into security analysis, constructs system model, and optimizes signal state pulse intensity for optimal performance under different channel attenuation levels.

Result: At 10 dB attenuation (25 km), secrecy message capacities are 8.765×10⁻⁶ bit/pulse (β=0°) and 4.150×10⁻⁶ bit/pulse (β=45°). Maximum transmission distances are 27.875 km (β=0°) and 26.750 km (β=45°), representing 155.9% and 149.7% improvements over single-photon-based QSDC.

Conclusion: The proposed RFI QSDC protocol significantly extends communication distance while tolerating reference frame misalignment, making it more practical for mobile communication scenarios where continuous precise calibration is challenging.

Abstract: Current quantum secure direct communication (QSDC) protocols guarantee communication security by estimating the error rates of photons in the X and Z bases. This take the reference frame calibration between communicating parties as a necessary prerequisite. However, in mobile communications scenarios, achieving continuous and accurate reference frame calibration poses significant challenges. To address this issue, this paper proposes a reference-frame-independent (RFI) QSDC protocol. This protocol only requires ensuring the calibration accuracy of one direction of the reference frame, while allowing a misalignment angle $β$ in the other two directions. To improve the protocol's robustness against reference frame fluctuations, we introduce a $β$-independent parameter C into the security analysis framework and rederive the protocol's security bounds. Additionally, we construct a system model and optimize the pulse intensity of the signal states, enabling the protocol to achieve optimal performance under each level of channel attenuation. At an attenuation of 10 dB (corresponding to a communication distance of 25 km), the secrecy message capacities for $β= 0^{ \circ} $ and $45^{ \circ} $ are $8.765 \times10^{-6}$ bit/pulse and $4.150 \times10^{-6}$ bit/pulse, respectively. Compared with the single-photon-based QSDC, the communication distance of the protocol proposed in this paper is significantly extended. When $β= 0^{ \circ} $ and $45^{ \circ} $, the maximum transmission distances of the RFI QSDC protocol are 27.875 km and 26.750 km, which is about 155.9 % and 149.7 % of that of the single-photon-based QSDC protocol.

</details>


### [24] [Efficient and broadband quantum frequency comb generation in a monolithic AlGaAs-on-insulator microresonator](https://arxiv.org/abs/2601.08289)
*Xiaodong Zheng,Xu Jing,Chenbo Liu,Yufu Li,Runqiu He,Lina Xia,Fei Wang,Yuechan Kong,Tangsheng Chen,Liangliang Lu,Jiayun Dai,Bin Niu*

Main category: quant-ph

TL;DR: Demonstration of an efficient chip-scale multi-wavelength quantum light source using AlGaAs-on-insulator microresonators, generating 11 distinct frequency-correlated photon pairs with high brightness and energy-time entanglement verification.


<details>
  <summary>Details</summary>
Motivation: Photonic frequency encoding aligns well with existing optical communication infrastructure, and utilizing multiple frequency modes can reduce hardware resources while improving quantum performance. There's a need for efficient chip-scale quantum light sources that can generate frequency-correlated photon pairs across multiple spectral modes.

Method: Leveraged AlGaAs-on-insulator platform with high material nonlinearity and low nonlinear loss. Used optimized submicron waveguide geometry to achieve high effective nonlinearity (~550 m⁻¹W⁻¹) and broad generation bandwidth. Implemented microresonator with ~200 GHz free spectral range at telecom wavelengths. Verified energy-time entanglement using Franson interferometry.

Result: Generated 11 distinct wavelength pairs across 35.2 nm bandwidth with average spectral brightness of 2.64 GHz mW⁻²nm⁻¹. Achieved average net visibility of 93.1% in Franson interferometry, confirming energy-time entanglement for each frequency mode pair. Demonstrated exceptional optical gain and lasing capabilities.

Conclusion: The AlGaAs-on-insulator platform shows outstanding potential for realizing fully integrated, ready-to-deploy quantum photonic systems on chip, offering efficient multi-wavelength quantum light generation with verified entanglement properties.

Abstract: The exploration of photonic systems for quantum information processing has generated widespread interest in multiple cutting-edge research fields. Photonic frequency encoding stands out as an especially viable approach, given its natural alignment with established optical communication technologies, including fiber networks and wavelength-division multiplexing systems. Substantial reductions in hardware resources and improvements in quantum performance can be expected by utilizing multiple frequency modes. The integration of nonlinear photonics with microresonators provides a compelling way for generating frequency-correlated photon pairs across discrete spectral modes. Here, by leveraging the high material nonlinearity and low nonlinear loss, we demonstrate an efficient chip-scale multi-wavelength quantum light source based on AlGaAs-on-insulator, featuring a free spectral range of approximately 200 GHz at telecom wavelengths. The optimized submicron waveguide geometry provides both high effective nonlinearity (~550 m$^{-1}$W$^{-1}$) and broad generation bandwidth, producing eleven distinct wavelength pairs across a 35.2 nm bandwidth with an average spectral brightness of 2.64 GHz mW$^{-2}$nm$^{-1}$. The generation of energy-time entanglement for each pair of frequency modes is verified through Franson interferometry, yielding an average net visibility of 93.1%. With its exceptional optical gain and lasing capabilities, the AlGaAs-on-insulator platform developed here shows outstanding potential for realizing fully integrated, ready-to-deploy quantum photonic systems on chip.

</details>


### [25] [A Preparation Nonstationarity Loophole in Superconducting-Qubit Bell Tests](https://arxiv.org/abs/2601.08290)
*Prosanta Pal,Shubhanshu Karoliya,Gargee Sharma,Ramakrishna Podila*

Main category: quant-ph

TL;DR: Bell tests on superconducting quantum processors can be misinterpreted due to slow temporal drift in preparation processes, requiring a relaxed Bell bound and drift-aware protocols for reliable quantum certification.


<details>
  <summary>Details</summary>
Motivation: The standard interpretation of Bell/CHSH tests assumes repeated circuit executions sample a single stationary preparation ensemble, but this assumption can be violated on contemporary superconducting quantum hardware, affecting the interpretation of observed Bell violations.

Method: Developed an ensemble-divergence framework to quantify preparation nonstationarity, introduced operational witness δ_op based on bin-resolved outcome statistics, and conducted experiments using Pauli-axis measurements on IBM superconducting processors with full two-qubit readout mitigation.

Result: Observed statistically significant operational drift persisting after readout mitigation, ruling out measurement artifacts. Drift from CHSH-optimal measurements was eliminated by mitigation, showing such settings are unsuitable for diagnosing preparation nonstationarity. Observed Bell violations imply only modest ensemble divergences comparable to Hall-type measurement-dependence models.

Conclusion: Identified a preparation-dependent loophole relevant to Bell tests on NISQ devices, highlighting the necessity of drift-aware protocols for reliable quantum certification and demonstrating that preparation drift combined with experimental scheduling can mimic measurement dependence effects.

Abstract: Bell or Clauser-Horne-Shimony-Holt (CHSH) tests on superconducting quantum processors are commonly interpreted under the assumption that repeated circuit executions sample a single, stationary preparation ensemble. Here we show that this assumption can be violated on contemporary hardware, with direct implications for the interpretation of observed Bell violations. We introduce an ensemble-divergence framework in which slow temporal drift of the preparation process induces context-dependent effective ensembles, even when measurement independence and locality are preserved. This leads to a relaxed Bell bound $|S| \le 2 + 6δ_{\mathrm{ens}}$, where $δ_{\mathrm{ens}}$ quantifies preparation nonstationarity. Because $δ_{\mathrm{ens}}$ is not directly observable, we develop an operational witness $δ_{\mathrm{op}}$ based on bin-resolved outcome statistics for fixed measurement channels. Using Pauli-axis measurements on IBM superconducting processors, we observe statistically significant operational drift that persists after full two-qubit readout mitigation, ruling out measurement artifacts. In contrast, drift extracted from CHSH-optimal measurements is eliminated by mitigation, demonstrating that such settings are unsuitable for diagnosing preparation nonstationarity. We further show that the observed Bell violations imply only modest ensemble divergences, comparable in scale to those required in Hall-type measurement-dependence models, but arising here solely from preparation drift combined with experimental scheduling. Our results identify a preparation-dependent loophole relevant to Bell tests on noisy intermediate-scale quantum devices and highlight the necessity of drift-aware protocols for reliable quantum certification.

</details>


### [26] [Extending Qubit Coherence Time via Hybrid Dynamical Decoupling](https://arxiv.org/abs/2601.08315)
*Qi Yao,Jun Zhang,Wenxian Zhang,Chaohong Lee*

Main category: quant-ph

TL;DR: Hybrid dynamical decoupling combining pulsed DD with bath spin polarization extends qubit coherence by 2-3 orders of magnitude in central spin systems.


<details>
  <summary>Details</summary>
Motivation: To address qubit decoherence from environmental coupling by integrating two parallel techniques: dynamical decoupling (DD) and bath engineering, creating a more effective hybrid approach.

Method: A hybrid DD approach that integrates pulsed dynamical decoupling with bath spin polarization within the central spin model, applicable to systems like GaAs semiconductor quantum dots and quantum simulators.

Result: Significant extension of central spin coherence time by approximately 2-3 orders of magnitude compared to free-induced decay time, with dominant contribution from DD and moderate improvement from spin-bath polarization.

Conclusion: The integration of uniaxial dynamical decoupling and auxiliary bath-spin engineering paves the way for prolonging coherence times in practical quantum systems (GaAs/AlGaAs, silicon, Si/SiGe), holding substantial promise for quantum information processing applications.

Abstract: Dynamical decoupling (DD) and bath engineering are two parallel techniques employed to mitigate qubit decoherence resulting from their unavoidable coupling to the environment. Here, we present a hybrid DD approach that integrates pulsed DD with bath spin polarization to enhance qubit coherence within the central spin model. This model, which can be realized using GaAs semiconductor quantum dots or analogous quantum simulators, demonstrates a significant extension of the central spin's coherence time by approximately 2 to 3 orders of magnitude that compared with the free-induced decay time, where the dominant contribution from DD and a moderate improvement from spin-bath polarization. This study, which integrates uniaxial dynamical decoupling and auxiliary bath-spin engineering, paves the way for prolonging coherence times in various practical quantum systems, including GaAs/AlGaAs, silicon and Si/SiGe. And this advancement holds substantial promise for applications in quantum information processing.

</details>


### [27] [Quantum fluctuations of vacuum versus photon-pairs concerning Spontaneous-Parametric Down-Conversion and Four-Wave-Mixing](https://arxiv.org/abs/2601.08349)
*Benoît Boulanger,Gaspar Mougin-Trichon,Véronique Boutou*

Main category: quant-ph

TL;DR: Theoretical investigation defines quantitative limit between spontaneous and stimulated regimes in SPDC/FWM nonlinear processes using semi-classical model, identifying threshold values for photon-pair flux and field ratios.


<details>
  <summary>Details</summary>
Motivation: To establish a clear quantitative boundary between spontaneous (quantum fluctuation-driven) and stimulated (generated photon-driven) regimes in nonlinear optical processes like SPDC and FWM, which is essential for understanding the full quantum picture across different pump intensities.

Method: Semi-classical modeling with analytical calculations, defining unitless photon-pairs flux per frequency unit, calculating electric field ratios between generated photons and vacuum, and plotting pump intensity as function of interaction length for different nonlinear susceptibilities.

Result: Identified threshold values: photon-pairs flux per frequency unit = 0.369 at the regime boundary; electric field ratio (generated photons/vacuum) = 1.718; provided pump intensity vs. interaction length plots for different χ² (SPDC) and χ³ (FWM) values.

Conclusion: The quantitative limit provides a useful guide for distinguishing spontaneous (vacuum fluctuation-seeded) from stimulated (optical parametric amplification) regimes, enabling more targeted quantum calculations and measurements across the full intensity spectrum of SPDC/FWM processes.

Abstract: The limit between the two regimes of spontaneous-parametric down-conversion (SPDC) or four-wave-mixing (FWM) regarding the pump intensity has been theoretically investigated using a semi-classical model and analytical calculations. A unitless quantity has been defined, corresponding to the photon-pairs flux per frequency unit: it has been found equal to 0.369 at this limit. The ratio between the magnitudes of the electric fields of the generated photons and of vacuum has been also calculated, equal to 1.718, and the pump intensity has been plotted as a function of the interaction length for different values of the second-order electric susceptibility in the case of SPDC and of the third-order electric susceptibility for FWM. These quantitative results confirm that below the limit, the nonlinear process can be truly considered as spontaneous, i.e. mainly seeded by the quantum fluctuations of vacuum, while the generated photons mainly govern the pump photon splitting above the limit, which corresponds more to an optical parametric amplification / difference frequency generation regime. Knowing quantitatively the limit between the two regimes thanks to the present calculations will be a useful guide for further quantum calculations and measurements from either side of the limit in order to catch the full quantum picture of SPDC and FWM from low to high pump intensities.

</details>


### [28] [Verification of continuous variable entanglement with undetected photons](https://arxiv.org/abs/2601.08364)
*Sanjukta Kundu,Balakrishnan Viswanathan,Pawel Szczypkowski,Gabriela Barreto Lemos,Mayukh Lahiri,Radek Lapkiewicz*

Main category: quant-ph

TL;DR: Experimental verification of transverse spatial entanglement in photon-pairs using nonlinear interferometry without coincidence detection, demonstrating EPR and MGVT criterion violations via single-photon interference.


<details>
  <summary>Details</summary>
Motivation: To develop a method for verifying spatial entanglement that doesn't rely on coincidence detection, works under experimental losses, and can be applied to non-degenerate sources where suitable detectors may not exist for one photon.

Method: Nonlinear interferometric technique using spontaneous parametric down conversion (SPDC) photon-pairs. Single-photon interference measurements of one photon from each pair to demonstrate entanglement, avoiding coincidence detection requirements.

Result: Successful experimental violation of both Einstein-Podolsky-Rosen (EPR) criterion and Mancini-Giovannetti-Vitali-Tombesi (MGVT) criterion. Good agreement between experimental results and theoretical predictions. Method performs well under experimental losses.

Conclusion: The nonlinear interferometric technique provides a robust method for certifying spatial entanglement without coincidence detection, applicable to non-degenerate sources and potentially extendable to discrete degrees of freedom for high-dimensional orbital angular momentum (OAM) entanglement certification.

Abstract: We verify transverse spatial entanglement of photon-pairs generated in spontaneous parametric down conversion using a nonlinear interferometric technique without relying on any coincidence detection. We experimentally demonstrate the violation of the Einstein-Podolsky-Rosen criterion and of the Mancini-Giovannetti-Vitali-Tombesi criterion using single photon interference of one of the photons of the pairs. We also provide a comprehensive theoretical analysis. The experimental results that we have obtained show good agreement with the theoretical values. Our method performs well under experimental losses and can be applied to highly non-degenerate sources, where there are no suitable detectors for one of the photons in the quantum state and our method could also be extended to the discrete degrees of freedom to certify high-dimensional (OAM) entanglement.

</details>


### [29] [A Methodological Analysis of Empirical Studies in Quantum Software Testing](https://arxiv.org/abs/2601.08367)
*Yuechen Li,Minqi Shao,Jianjun Zhao,Qichen Wang*

Main category: quant-ph

TL;DR: Systematic methodological analysis of 59 empirical studies in quantum software testing reveals diverse practices, identifies limitations/inconsistencies, and provides recommendations for improving study design and reporting.


<details>
  <summary>Details</summary>
Motivation: As quantum software testing (QST) grows in importance with increasing scale/complexity of quantum systems, empirical studies are widely used but lack methodological consistency, making it difficult to interpret results and compare findings across studies.

Method: Systematic examination of 59 primary studies from a literature pool of 384 papers, analyzing key methodological dimensions through 10 research questions covering objects under test, baseline comparison, testing setup, experimental configuration, and tool/artifact support.

Result: Characterization of current empirical practices in QST, identification of recurring limitations and inconsistencies, and highlighting of open methodological challenges through cross-study analysis.

Conclusion: Derivation of insights and recommendations to inform the design, execution, and reporting of future empirical studies in quantum software testing to improve methodological rigor and comparability.

Abstract: In quantum software engineering (QSE), quantum software testing (QST) has attracted increasing attention as quantum software systems grow in scale and complexity. Since QST evaluates quantum programs through execution under designed test inputs, empirical studies are widely used to assess the effectiveness of testing approaches. However, the design and reporting of empirical studies in QST remain highly diverse, and a shared methodological understanding has yet to emerge, making it difficult to interpret results and compare findings across studies. This paper presents a methodological analysis of empirical studies in QST through a systematic examination of 59 primary studies identified from a literature pool of size 384. We organize our analysis around ten research questions that cover key methodological dimensions of QST empirical studies, including objects under test, baseline comparison, testing setup, experimental configuration, and tool and artifact support. Through cross-study analysis along these dimensions, we characterize current empirical practices in QST, identify recurring limitations and inconsistencies, and highlight open methodological challenges. Based on our findings, we derive insights and recommendations to inform the design, execution, and reporting of future empirical studies in QST.

</details>


### [30] [A dataflow programming framework for linear optical distributed quantum computing](https://arxiv.org/abs/2601.08389)
*Giovanni de Felice,Boldizsár Poór,Cole Comfort,Lia Yeh,Mateusz Kupper,William Cashman,Bob Coecke*

Main category: quant-ph

TL;DR: A graphical framework for distributed quantum computing that integrates linear optics, ZX-calculus, and dataflow programming to enable formal analysis and optimization of networked quantum architectures with photonic components.


<details>
  <summary>Details</summary>
Motivation: Photonic systems offer promising platforms for interconnecting quantum processors and enabling scalable networked architectures, but designing and verifying such architectures requires a unified formalism that integrates linear algebraic reasoning with probabilistic and control-flow structures.

Method: Introduces a graphical framework combining linear optics, ZX-calculus, and dataflow programming within a synchronous dataflow model with discrete-time dynamics. The language supports formal analysis of distributed protocols involving qubits and photonic modes with explicit classical control and feedforward interfaces.

Result: Classifies entangling photonic fusion measurements, shows how their induced Pauli errors can be corrected via a novel flow structure for fusion networks, establishes correctness proofs for new repeat-until-success protocols enabling arbitrary fusions, and constructs qubit architectures with practical optical components (beam splitters, switches, photon sources) with graphical proofs of determinism and universal quantum computation support.

Conclusion: The framework establishes a foundation for verifiable compilation and automated optimization in networked quantum computing by providing a unified graphical formalism for distributed quantum architectures with photonic interconnects.

Abstract: Photonic systems offer a promising platform for interconnecting quantum processors and enabling scalable, networked architectures. Designing and verifying such architectures requires a unified formalism that integrates linear algebraic reasoning with probabilistic and control-flow structures. In this work, we introduce a graphical framework for distributed quantum computing that brings together linear optics, the ZX-calculus, and dataflow programming. Our language supports the formal analysis and optimization of distributed protocols involving both qubits and photonic modes, with explicit interfaces for classical control and feedforward, all expressed within a synchronous dataflow model with discrete-time dynamics. Within this setting, we classify entangling photonic fusion measurements, show how their induced Pauli errors can be corrected via a novel flow structure for fusion networks, and establish correctness proofs for new repeat-until-success protocols enabling arbitrary fusions. Layer by layer, we construct qubit architectures incorporating practical optical components such as beam splitters, switches, and photon sources, with graphical proofs that they are deterministic and support universal quantum computation. Together, these results establish a foundation for verifiable compilation and automated optimization in networked quantum computing.

</details>


### [31] [On-chip semi-device-independent quantum random number generator exploiting contextuality](https://arxiv.org/abs/2601.08392)
*Maddalena Genzini,Caterina Vigliar,Mujtaba Zahidy,Hamid Tebyanian,Andrzej Gajda,Klaus Petermann,Lars Zimmermann,Davide Bacco,Francesco Da Ros*

Main category: quant-ph

TL;DR: Silicon photonic chip-based semi-device-independent QRNG using KCBS contextuality violation, achieving 10σ non-classicality with certified min-entropy of 0.077 bits/round at 21.7 bits/s generation rate.


<details>
  <summary>Details</summary>
Motivation: Develop practical, integrated quantum random number generators that don't require entanglement, can operate in semi-device-independent mode, and are compatible with scalable photonic quantum networks.

Method: Two silicon photonic chips: one with heralded single-photon source, another with reconfigurable interferometric mesh for qutrit state preparation, transformations, and measurements. Tests KCBS contextuality inequality using single-photon interference in complex optical network.

Result: Contextuality violation exceeding classical bound by >10σ, confirming non-classical behavior. Certified conditional min-entropy per round: Hmin = 0.077 ± 0.002 bits, corresponding to asymptotic generation rate of 21.7 ± 0.5 bits/s.

Conclusion: Demonstrates viable route to general-purpose, untrusted QRNGs compatible with integrated photonic quantum networks, using contextuality violation for security certification without entanglement.

Abstract: We present a semi-device-independent quantum random number generator (QRNG) based on the violation of a contextuality inequality, implemented by the integration of two silicon photonic chips. Our system combines a heralded single-photon source with a reconfigurable interferometric mesh to implement qutrit state preparation, transformations, and measurements suitable for testing a KCBS contextuality inequality. This architecture enables the generation of random numbers from the intrinsic randomness of single-photon interference in a complex optical network, while simultaneously allowing a quantitative certification of their security without requiring entanglement. We observe a contextuality violation exceeding the classical bound by more than 10σ, unambiguously confirming non-classical behavior. From this violation, we certify a conditional min-entropy per experimental round of Hmin = 0.077 +- 0.002, derived via a tailored semidefinite-programming-based security analysis. Each measurement outcome therefore contains at least 0.077 +- 0.002 bits of extractable genuine randomness, corresponding to an asymptotic generation rate of 21.7 +- 0.5 bits/s. These results establish a viable route towards general-purpose, untrusted quantum random number generators compatible with practical integrated photonic quantum networks.

</details>


### [32] [Rigorous phase-error-estimation security framework for QKD with correlated sources](https://arxiv.org/abs/2601.08417)
*Guillermo Currás-Lorenzo,Margarida Pereira,Kiyoshi Tamaki,Marcos Curty*

Main category: quant-ph

TL;DR: Framework extends phase-error-based security proofs to handle encoding correlations in QKD modulators, bridging theory-practice gap.


<details>
  <summary>Details</summary>
Motivation: Real-world QKD modulators have bandwidth limitations that create correlations between consecutive pulses, violating assumptions in existing security proofs and creating a gap between theoretical guarantees and practical implementations.

Method: Introduces a simple yet powerful mathematical framework that directly extends phase-error-estimation-based security proofs for imperfect but uncorrelated sources to incorporate encoding correlations.

Result: The framework overcomes limitations of previous approaches in terms of generality and rigor, significantly narrowing the gap between theoretical security guarantees and real-world QKD implementations.

Conclusion: Provides a practical solution to address encoding correlations in QKD systems, enhancing security analysis for real-world implementations while maintaining mathematical rigor.

Abstract: Practical QKD modulators introduce correlations between consecutively emitted pulses due to bandwidth limitations, violating key assumptions underlying many security proof techniques. Here, we address this problem by introducing a simple yet powerful mathematical framework to directly extend phase-error-estimation-based security proofs for imperfect but uncorrelated sources to also incorporate encoding correlations. Our framework overcomes important limitations of previous approaches in terms of generality and rigor, significantly narrowing the gap between theoretical security guarantees and real-world QKD implementations.

</details>


### [33] [Toolchain for shuttling trapped-ion qubits in segmented traps](https://arxiv.org/abs/2601.08495)
*Andreas Conta,Santiago Bogino,Frodo Köhncke,Ferdinand Schmidt-Kaler,Ulrich Poschinger*

Main category: quant-ph

TL;DR: A numerical toolchain for generating voltage waveforms that enable fast, low-excitation ion shuttling in segmented RF traps, supporting arbitrary geometries and rapid prototyping.


<details>
  <summary>Details</summary>
Motivation: Scalable trapped-ion quantum computing requires reliable ion transport through complex segmented RF trap architectures without excessive motional excitation, necessitating systematic methods for generating appropriate control voltages.

Method: Developed a numerical framework combining electrostatic field solver, unconstrained optimization, waveform postprocessing, and dynamical simulations to compute voltage waveforms that realize prescribed transport trajectories while respecting experimental constraints like voltage limits and bandwidth.

Result: The toolchain supports arbitrary trap geometries (including junctions and multi-zone layouts), demonstrates numerical stability, shows good agreement between measured and predicted secular frequencies, and enables rapid prototyping of complex trap architectures.

Conclusion: The framework provides an extensible and efficient numerical foundation for designing and validating transport protocols in current and next-generation trapped-ion quantum processors.

Abstract: Scalable trapped-ion quantum computing requires fast and reliable transport of ions through complex, segmented radiofrequency trap architectures without inducing excessive motional excitation. We present a numerical toolchain for the systematic generation of time-dependent electrode voltages enabling fast, low-excitation ion shuttling in segmented radiofrequency traps. Based on a model of the trap electrode geometry, the framework combines an electrostatic field solver, efficient unconstrained optimization, waveform postprocessing, and dynamical simulations of ion motion to compute voltage waveforms that realize prescribed transport trajectories while respecting experimental constraints such as voltage limits and bandwidth. The toolchain supports arbitrary trap geometries, including junctions and multi-zone layouts, and allows for the flexible incorporation of optimization objectives. We provide a detailed assessment of the accuracy of the framework by investigating its numerical stability and by comparing measured and predicted secular frequencies. The framework is optimized for numerical performance, enabling rapid numerical prototyping of trap architectures of increasing complexity. As application examples, we apply the framework to the transport of a potential well along a linear, uniformly segmented trap, and we compute a solution for shuttling a potential well around the corner of an X-type trap junction. The presented approach provides an extensible and highly efficient numerical foundation for designing and validating transport protocols in current and next-generation trapped-ion processors.

</details>


### [34] [MultiQ: Multi-Programming Neutral Atom Quantum Architectures](https://arxiv.org/abs/2601.08504)
*Francisco Romão,Daniel Vonk,Emmanuil Giortamis,Pramod Bhatotia*

Main category: quant-ph

TL;DR: MultiQ enables multi-programming on neutral atom QPUs by partitioning qubit arrays for concurrent circuit execution, achieving 3.8-12.3× throughput gains with minimal fidelity loss.


<details>
  <summary>Details</summary>
Motivation: Neutral atom QPUs face performance challenges: large circuits suffer fidelity drops while small circuits underutilize hardware and face initialization latency. Current compilers lack multi-programming support.

Method: MultiQ is a cross-layer system with compiler, controller, and checker. It creates virtual zone layouts for optimal spatio-temporal utilization, parallelizes co-located circuits via single hardware instructions, and verifies functional independence of bundled circuits.

Result: Throughput increases 3.8× to 12.3× when multi-programming 4 to 14 circuits, with fidelity largely maintained (1.3% improvement for 4 circuits to 3.5% loss for 14 circuits).

Conclusion: MultiQ enables concurrent execution of multiple quantum circuits on neutral atom QPUs, significantly boosting throughput and hardware utilization while maintaining acceptable fidelity levels.

Abstract: Neutral atom Quantum Processing Units (QPUs) are emerging as a popular quantum computing technology due to their large qubit counts and flexible connectivity. However, performance challenges arise as large circuits experience significant fidelity drops, while small circuits underutilize hardware and face initialization latency issues. To tackle these problems, we propose $\textit{multi-programming on neutral atom QPUs}$, allowing the co-execution of multiple circuits by logically partitioning the qubit array. This approach increases resource utilization and mitigates initialization latency while maintaining result fidelity. Currently, state-of-the-art compilers for neutral atom architectures do not support multi-programming.
  To fill this gap, we introduce MultiQ, the first system designed for this purpose. MultiQ addresses three main challenges: (i) it compiles circuits into a $\textit{virtual zone layout}$ to optimize spatio-temporal hardware utilization; (ii) it parallelizes the execution of co-located circuits, allowing single hardware instructions to operate on different circuits; and (iii) it includes an algorithm to verify the functional independence of the bundled circuits. MultiQ functions as a cross-layer system comprising a compiler, controller, and checker. Our compiler generates \emph{virtual zone layouts} to enhance performance, while the controller efficiently maps these layouts onto the hardware and resolves any conflicts. The checker ensures the correct bundling of circuits.
  Experimental results show a throughput increase from 3.8$\times$ to 12.3$\times$ when multi-programming 4 to 14 circuits, with fidelity largely maintained, ranging from a 1.3% improvement for four circuits to only a 3.5% loss for fourteen circuits. Overall, MultiQ facilitates concurrent execution of multiple quantum circuits, boosting throughput and hardware utilization.

</details>


### [35] [Symmetry-Adapted State Preparation for Quantum Chemistry on Fault-Tolerant Quantum Computers](https://arxiv.org/abs/2601.08533)
*Viktor Khinevich,Wataru Mizukami*

Main category: quant-ph

TL;DR: Systematic resource-efficient constructions of continuous symmetry projectors (U(1) particle number and SU(2) total spin) for fault-tolerant quantum computation using LCU and GQSP/GQSVT methods, with substantial cost advantages over unfiltered QPE approaches.


<details>
  <summary>Details</summary>
Motivation: To develop practical and scalable continuous-symmetry projectors for state preparation in quantum chemistry that can be coherently applied as state filters prior to quantum phase estimation, reducing overall computational costs in fault-tolerant quantum simulations.

Method: Employs linear combination of unitaries (LCU) and generalized quantum signal processing (GQSP/GQSVT) to implement symmetry projectors. Analyzes asymptotic gate complexities for explicit circuit realizations, with GQSP offering favorable resource usage for particle number and S_z symmetries, and structured decomposition reducing T gate counts for total spin projection.

Result: Symmetry filtering substantially increases QPE success probability, leading to lower overall costs compared to unfiltered approaches. Resource estimates show symmetry filtering costs 3-4 orders of magnitude lower than subsequent phase estimation. For FeMoco, QPE costs ~10^10 T gates while symmetry projectors require only ~10^6-10^7 T gates.

Conclusion: Continuous-symmetry projectors are established as practical and scalable tools for state preparation in quantum chemistry, providing a pathway toward more efficient fault-tolerant quantum simulations, especially relevant in large, strongly correlated systems like FeMoco.

Abstract: We present systematic and resource-efficient constructions of continuous symmetry projectors, particularly $U(1)$ particle number and $SU(2)$ total spin, tailored for fault-tolerant quantum computations. Our approach employs a linear combination of unitaries (LCU) as well as generalized quantum signal processing (GQSP and GQSVT) to implement projectors. These projectors can then be coherently applied as state filters prior to quantum phase estimation (QPE). We analyze their asymptotic gate complexities for explicit circuit realizations. For the particle number and $S_z$ symmetries, GQSP offers favorable resource usage features owing to its low ancilla qubit requirements and robustness to finite precision rotation gate synthesis. For the total spin projection, the structured decomposition of $\hat{P}_{S,M_S}$ reduces the projector T gate count. Numerical simulations show that symmetry filtering substantially increases the QPE success probability, leading to a lower overall cost compared to that of unfiltered approaches across representative molecular systems. Resource estimates further indicate that the cost of symmetry filtering is $3$ to $4$ orders of magnitude lower than that of the subsequent phase estimation step This advantage is especially relevant in large, strongly correlated systems, such as FeMoco, a standard strongly correlated open-shell benchmark. For FeMoco, the QPE cost is estimated at ${\sim}10^{10}$ T gates, while our symmetry projector requires only ${\sim}10^{6}$--$10^{7}$ T gates. These results establish continuous-symmetry projectors as practical and scalable tools for state preparation in quantum chemistry and provide a pathway toward realizing more efficient fault-tolerant quantum simulations.

</details>


### [36] [Asymptotically good CSS codes that realize the logical transversal Clifford group fault-tolerantly](https://arxiv.org/abs/2601.08568)
*K. Sai Mineesh Reddy,Navin Kashyap*

Main category: quant-ph

TL;DR: Framework for constructing CSS codes with fault-tolerant logical transversal Z-rotations, enabling asymptotically good CSS codes with fault-tolerant logical transversal Clifford group, plus analysis of CSS-T codes with specific transversal gate implementations.


<details>
  <summary>Details</summary>
Motivation: To develop quantum error-correcting codes that support fault-tolerant logical transversal operations, particularly Z-rotations and Clifford gates, which are essential for fault-tolerant quantum computation while maintaining code properties.

Method: Introduces a framework for constructing Calderbank-Shor-Steane (CSS) codes that support fault-tolerant logical transversal Z-rotations. Investigates CSS-T codes specifically, analyzing conditions for transversal T gate implementations and revising characterizations.

Result: Obtained asymptotically good CSS codes that fault-tolerantly realize the logical transversal Clifford group. For CSS-T codes: (a) found asymptotically good codes where transversal T implements logical transversal S†; (b) showed condition C₂∗C₁⊆C₁⊥ is necessary but insufficient; (c) revised characterizations for codes where transversal T implements logical identity and logical transversal T.

Conclusion: The framework enables construction of CSS codes with fault-tolerant logical transversal operations, advancing fault-tolerant quantum computation. The analysis of CSS-T codes clarifies conditions for transversal T gate implementations and provides refined characterizations for specific logical gate realizations.

Abstract: This paper introduces a framework for constructing Calderbank-Shor-Steane (CSS) codes that support fault-tolerant logical transversal $Z$-rotations. Using this framework, we obtain asymptotically good CSS codes that fault-tolerantly realize the logical transversal Clifford group. Furthermore, investigating CSS-T codes, we: (a) demonstrate asymptotically good CSS-T codes wherein the transversal $T$ realizes the logical transversal $S^{\dagger}$; (b) show that the condition $C_2 \ast C_1 \subseteq C_1^{\perp}$ is necessary but not sufficient for CSS-T codes; and (c) revise the characterizations of CSS-T codes wherein the transversal $T$ implements the logical identity and the logical transversal $T$, respectively.

</details>


### [37] [Quantum Computing -- Strategic Recommendations for the Industry](https://arxiv.org/abs/2601.08578)
*Marvin Erdmann,Lukas Karch,Abhishek Awasthi,Caitlin Isobel Jones,Pallavi Bhardwaj,Florian Krellner,Jonas Stein,Claudia Linnhoff-Popien,Nico Kraus,Peter Eder,Sarah Braun,Tong Liu*

Main category: quant-ph

TL;DR: Survey paper assessing quantum computing's near-term potential for industrial optimization and machine learning, evaluating hardware roadmaps and use cases through a standardized traffic-light framework.


<details>
  <summary>Details</summary>
Motivation: To provide a realistic assessment of quantum computing's readiness for industrial applications in optimization and machine learning, grounded in practical hardware trajectories and real-world use case evaluations.

Method: Synthesizes hardware roadmaps from different quantum architectures (superconducting and ion-trap), applies standardized traffic-light evaluation framework with consistent criteria (model formulation, scalability, solution quality, runtime, transferability), and assesses use cases through three categories from optimistic to pessimistic.

Result: Identifies where quantum approaches currently show promise, where hybrid classical-quantum strategies are most viable, and where classical methods remain superior, providing concrete verdicts on industrial applicability.

Conclusion: The paper offers a balanced, practical assessment of quantum computing's industrial potential, distinguishing between realistic near-term applications and longer-term prospects while providing a standardized framework for ongoing evaluation.

Abstract: This whitepaper surveys the current landscape and short- to mid-term prospects for quantum-enabled optimization and machine learning use cases in industrial settings. Grounded in the QCHALLenge program, it synthesizes hardware trajectories from different quantum architectures and providers, and assesses their maturity and potential for real-world use cases under a standardized traffic-light evaluation framework. We provide a concise summary of relevant hardware roadmaps, distinguishing superconducting and ion-trap technologies, their current states, modalities, and projected scaling trajectories. The core of the presented work are the use case evaluations in the domains of optimization problems and machine learning applications. For the conducted experiments, we apply a consistent set of evaluation criteria (model formulation, scalability, solution quality, runtime, and transferability) which are assessed in a shared system of three categories, ranging from optimistic (solutions produced by quantum computers are competitive with classical methods and/or a clear path to a quantum advantage is shown) to pessimistic (significant hurdles prevent practical application of quantum solutions now and potentially in the future). The resulting verdicts illuminate where quantum approaches currently offer promise, where hybrid classical-quantum strategies are most viable, and where classical methods are expected to remain superior.

</details>


### [38] [Phase-sensitive superposition of quantum states](https://arxiv.org/abs/2601.08579)
*Xiaotong Wang,Shunlong Luo,Yue Zhang*

Main category: quant-ph

TL;DR: The paper introduces phase-sensitive superposition quantifiers from an information-theoretic perspective, establishes conservation relations, connects them to coherence measures, characterizes dephasing channels, and analyzes superposition dynamics in quantum algorithms.


<details>
  <summary>Details</summary>
Motivation: Despite superposition being fundamental to quantum mechanics and the source of quantum phenomena like coherence and entanglement, its quantification beyond coherence resource theory remains understudied. The authors aim to quantify superposition from an information-theoretic viewpoint, particularly focusing on phase-sensitive aspects.

Method: Introduces a family of phase-sensitive superposition quantifiers that account for amplitude phases in fixed basis states. Establishes conservation relations reminiscent of wave-particle duality, evaluates the second moment's connection to l²-norm coherence, characterizes dephasing channels from maximally superposed states, and analyzes minimum/maximum superposition properties.

Result: Developed phase-sensitive superposition measures, demonstrated their conservation relations, showed the second moment's intrinsic connection to l²-norm coherence, characterized dephasing channels induced by maximally superposed states, and revealed complementary relations between superposition and success probability in the Grover search algorithm.

Conclusion: The introduced phase-sensitive superposition quantifiers provide new tools for analyzing structural features and implications of quantum superposition, offering insights into superposition dynamics in quantum algorithms and establishing fundamental complementary relations reminiscent of wave-particle duality.

Abstract: Although the principle of superposition lies at the heart of quantum mechanics and is the root of almost all quantum phenomena such as coherence and entanglement, its quantification, except for that related to the resource theory of coherence and interference, remains relatively less studied. In this work, we address quantification of superposition from an information-theoretic perspective. We introduce a family of quantifiers of superposition, the phase-sensitive superposition, by taking into account the phases of amplitudes in the superposition of a fixed basis states (e.g., computational basis states). We establish a conservation relation for the phase-sensitive superposition, which is a kind of complementary relation and is reminiscent of wave-particle duality. We evaluate explicitly the second moment of phase-sensitive superposition and show that it is intrinsically related to the $l^2$-norm coherence. We characterize the dephasing channel induced by the maximally superposed states. We investigate the minimum and maximum superpositions, reveal their basic properties, and illustrate them through various examples. We further explore the dynamics of superposition in the Grover search algorithm, and demonstrate a complementary relation between superposition and success probability of the search algorithm. These results and quantifiers offer tools for analyzing structural features and implications of quantum superposition.

</details>


### [39] [Entanglement-swapping measurements for deterministic entanglement distribution](https://arxiv.org/abs/2601.08581)
*Mir Alimuddin,Jaemin Kim,Acín,Leonardo Zambrano*

Main category: quant-ph

TL;DR: Characterization of deterministic entanglement-swapping measurements that eliminate postselection, with optimal protocols based on complex Hadamard matrices and complete classification across dimensions.


<details>
  <summary>Details</summary>
Motivation: Standard entanglement swapping protocols are probabilistic and require postselection based on measurement outcomes, which limits practical quantum network implementation. The goal is to identify measurements that make entanglement swapping deterministic, eliminating the need for postselection.

Method: Theoretical characterization of measurements where entanglement swapping becomes deterministic for arbitrary pure inputs, with every measurement outcome producing local-unitarily equivalent states. Identification of optimal measurements maximizing concurrence-type entanglement measures, constructed from complex Hadamard matrices. Complete classification of deterministic entanglement-swapping measurements across different dimensions.

Result: Optimal deterministic entanglement-swapping measurements are built from complex Hadamard matrices. Complete classification shows: unique solutions for dimensions d=2,3; infinite solutions for d=4; 72 inequivalent classes for d=5. For d=2,3, the end-to-end state in multi-node networks is independent of measurement order at repeaters.

Conclusion: Established optimal entanglement-swapping schemes that are post-selection free, enabling deterministic entanglement distribution across generic quantum network architectures without unfavorable measurement outcomes.

Abstract: Entanglement swapping is a key primitive for distributing entanglement across nodes in quantum networks. In standard protocols, the outcome of the intermediate measurement determines the resulting state, making the process inherently probabilistic and requiring postselection. In this work, we fully characterize those measurements under which entanglement swapping becomes deterministic: for arbitrary pure inputs, every measurement outcome produces local-unitarily equivalent states. We also show that an optimal measurement, maximizing a concurrence-type entanglement measure, is built from complex Hadamard matrices. For this optimal protocol, we provide a complete, dimension-dependent classification of deterministic entanglement-swapping measurements: unique in dimensions $d=2,3$, infinite for $d=4$, and comprising $72$ inequivalent classes for $d=5$. We further consider a general network with multiple swapping nodes and show that, for $d=2,3$ the resulting end-to-end state is independent of the order in which the repeaters perform the optimal measurements. Our results establish optimal entanglement-swapping schemes that are post-selection free, in the sense that they distribute entanglement across generic quantum network architectures without unfavorable measurement outcomes.

</details>


### [40] [Sample Complexity of Composite Quantum Hypothesis Testing](https://arxiv.org/abs/2601.08588)
*Jacob Paul Simpson,Efstratios Palias,Sharu Theresa Jose*

Main category: quant-ph

TL;DR: This paper provides tight sample complexity bounds for symmetric composite binary quantum hypothesis testing, matching upper and lower bounds up to constants, and extends the analysis to differentially private settings.


<details>
  <summary>Details</summary>
Motivation: While asymptotic error exponents for symmetric composite binary quantum hypothesis testing are well-studied, the finite-sample regime remains poorly understood, creating a gap in understanding the practical sample requirements for this problem.

Method: The authors derive lower bounds that generalize simple QHT sample complexity and introduce new upper bounds for various uncertainty sets (both finite and infinite cardinalities), then extend the analysis to differentially private settings.

Result: The upper and lower bounds match up to universal constants, providing a tight characterization of sample complexity for symmetric composite binary QHT, including extensions to privacy-preserving settings.

Conclusion: The paper successfully bridges the gap between asymptotic and finite-sample analysis for composite quantum hypothesis testing, establishing tight sample complexity bounds and extending the framework to include differential privacy considerations.

Abstract: This paper investigates symmetric composite binary quantum hypothesis testing (QHT), where the goal is to determine which of two uncertainty sets contains an unknown quantum state. While asymptotic error exponents for this problem are well-studied, the finite-sample regime remains poorly understood. We bridge this gap by characterizing the sample complexity -- the minimum number of state copies required to achieve a target error level. Specifically, we derive lower bounds that generalize the sample complexity of simple QHT and introduce new upper bounds for various uncertainty sets, including of both finite and infinite cardinalities. Notably, our upper and lower bounds match up to universal constants, providing a tight characterization of the sample complexity. Finally, we extend our analysis to the differentially private setting, establishing the sample complexity for privacy-preserving composite QHT.

</details>


### [41] [Open quantum spin chains with non-reciprocity: a theoretical approach based on the time-dependent generalized Gibbs ensemble](https://arxiv.org/abs/2601.08606)
*Alice Marché,Hironobu Yoshida,Alberto Nardin,Hosho Katsura,Leonardo Mazza*

Main category: quant-ph

TL;DR: Theoretical approach using time-dependent generalized Gibbs ensemble to study open quantum spin chains with non-reciprocal dissipation, deriving evolution equations for rapidity distributions and computing magnetization dynamics.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretical framework that can describe the physics of non-reciprocal open quantum spin chains beyond existing analyses based on non-interacting fermions, addressing the problem of anomalous power-law exponents identified in previous work.

Method: Time-dependent generalized Gibbs ensemble approach applied to open quantum spin chains with non-reciprocal dissipation. In weak dissipation regime, system characterized by rapidity distribution with derived coupled differential equations governing their time evolution. Results benchmarked against numerical simulations.

Result: Successfully computed both magnetization density and current dynamics, identifying relations between them. The approach accurately describes system physics beyond non-interacting fermion analyses. The anomalous power-law exponent problem from previous work is discussed within this framework.

Conclusion: The time-dependent generalized Gibbs ensemble provides a theoretical approach capable of describing non-reciprocal open quantum spin chain physics beyond limitations of non-interacting fermion analyses, offering insights into magnetization dynamics and addressing previous anomalous exponent issues.

Abstract: We study an open quantum spin chain with non-reciprocal dissipation using a theoretical approach known as time-dependent generalized Gibbs ensemble. In the regime of weak dissipation the system is fully characterized by its rapidity distribution and we derive a closed set of coupled differential equations governing their time evolution. We check the accuracy of this theory by benchmarking the results against numerical simulations. Using this framework we are able to compute both the magnetization density and current dynamics, identifying some relations between the two. The problem of the anomalous power-law exponents identified in a previous work is discussed. Our work constitutes a theoretical approach that is able to describe the physics of non-reciprocal open quantum spin chains beyond analyses based on non-interacting fermions.

</details>


### [42] [Fragility of Optimal Measurements due to Noise in Probe States for Quantum Sensing](https://arxiv.org/abs/2601.08712)
*Andrew Kolmer Forbes,Marco A. Rodríguez-García,Ivan H. Deutsch*

Main category: quant-ph

TL;DR: The paper introduces "Fisher information fragility" - a framework quantifying how discontinuities in classical Fisher information (CFI) increase sensitivity to noise, and shows how to design more robust POVMs for quantum metrology.


<details>
  <summary>Details</summary>
Motivation: In quantum parameter estimation, while the quantum Fisher information (QFI) sets the fundamental precision limit via the quantum Cramér-Rao bound, practical measurements via POVMs aim to saturate this bound. However, discontinuities can appear in CFI but not QFI, and these discontinuities make the extracted information vulnerable to noise. The paper aims to understand and quantify this vulnerability.

Method: The authors develop a framework using Jensen's inequality to analyze how discontinuities in CFI increase fragility to noise. They demonstrate how this framework can be used to design POVMs that are more robust, enabling quantum advantage in metrology despite noise.

Result: The paper shows that discontinuities in CFI quantify how much Fisher information is lost in noisy environments, introducing the concept of "Fisher information fragility." The framework provides tools to design measurement strategies that maintain higher information extraction under noise.

Conclusion: Fisher information discontinuities represent important features that quantify vulnerability to noise in quantum metrology. By understanding and mitigating this fragility through careful POVM design, one can achieve more robust quantum advantage in parameter estimation.

Abstract: For a given quantum state used in sensing, the quantum Cramér-Rao bound (QCRB) sets a fundamental limit on the precision achievable by an unbiased estimator of an unknown parameter, determined by the inverse of the quantum Fisher information (QFI). The QFI serves as an upper bound on the classical Fisher information (CFI), representing the maximum extractable information about the unknown parameter from measurements on a physical system. Thus, a central goal in quantum parameter estimation is to find a measurement, described by a POVM, that saturates the QFI (achieves maximum CFI), and thereby achieves the QCRB. In the idealization that one uses pure states and unitary encodings for sensing, discontinuities can appear in the CFI but not the QFI. In this article, we demonstrate that these discontinuities are important features, quantifying how much Fisher information is lost in the presence of noise. We refer to this as the Fisher information "fragility". We present a simple framework for understanding how discontinuities increase fragility through Jensen's inequality, and demonstrate how one can use this framework to design more robust POVMs for quantum advantage in metrology.

</details>


### [43] [Kernel Learning for Regression via Quantum Annealing Based Spectral Sampling](https://arxiv.org/abs/2601.08724)
*Yasushi Hasegawa,Masayuki Ohzeki*

Main category: quant-ph

TL;DR: A QA-in-the-loop kernel learning framework that uses quantum annealing to generate spectral distributions for random Fourier features, enabling data-adaptive kernel learning for regression with improved performance over Gaussian kernels.


<details>
  <summary>Details</summary>
Motivation: Quantum annealing devices operate at finite temperature and under noise, producing stochastic samples that approximate Gibbs-Boltzmann distributions. This property can be leveraged beyond simple Markov-chain Monte Carlo substitution to directly determine learned kernels for regression tasks.

Method: 1) Represent shift-invariant kernels via Bochner's theorem as expectations over spectral distributions. 2) Model spectral distribution with (multi-layer) restricted Boltzmann machines (RBMs). 3) Generate discrete RBM samples using quantum annealing. 4) Map discrete samples to continuous frequencies via Gaussian-Bernoulli transformation. 5) Construct data-adaptive kernel using resulting random Fourier features (RFF). 6) Perform Nadaraya-Watson regression with nonnegative squared-kernel weights to avoid denominator issues. 7) Train kernel parameters by minimizing leave-one-out NW mean squared error. 8) Evaluate with local linear regression using same squared-kernel weights.

Result: Experiments on multiple benchmark regression datasets show decreased training loss, structural changes in kernel matrices, and improved R² and RMSE over baseline Gaussian-kernel Nadaraya-Watson regression. Increasing the number of random features at inference further enhances accuracy.

Conclusion: The proposed QA-in-the-loop kernel learning framework successfully integrates quantum annealing as a core component for kernel determination, demonstrating practical advantages over conventional Gaussian kernels through data-adaptive spectral distribution learning and improved regression performance.

Abstract: While quantum annealing (QA) has been developed for combinatorial optimization, practical QA devices operate at finite temperature and under noise, and their outputs can be regarded as stochastic samples close to a Gibbs--Boltzmann distribution. In this study, we propose a QA-in-the-loop kernel learning framework that integrates QA not merely as a substitute for Markov-chain Monte Carlo sampling but as a component that directly determines the learned kernel for regression. Based on Bochner's theorem, a shift-invariant kernel is represented as an expectation over a spectral distribution, and random Fourier features (RFF) approximate the kernel by sampling frequencies. We model the spectral distribution with a (multi-layer) restricted Boltzmann machine (RBM), generate discrete RBM samples using QA, and map them to continuous frequencies via a Gaussian--Bernoulli transformation. Using the resulting RFF, we construct a data-adaptive kernel and perform Nadaraya--Watson (NW) regression. Because the RFF approximation based on $\cos(\bmω^{\top}Δ\bm{x})$ can yield small negative values and cancellation across neighbors, the Nadaraya--Watson denominator $\sum_j k_{ij}$ may become close to zero. We therefore employ nonnegative squared-kernel weights $w_{ij}=k(\bm{x}_i,\bm{x}_j)^2$, which also enhances the contrast of kernel weights. The kernel parameters are trained by minimizing the leave-one-out NW mean squared error, and we additionally evaluate local linear regression with the same squared-kernel weights at inference. Experiments on multiple benchmark regression datasets demonstrate a decrease in training loss, accompanied by structural changes in the kernel matrix, and show that the learned kernel tends to improve $R^2$ and RMSE over the baseline Gaussian-kernel NW. Increasing the number of random features at inference further enhances accuracy.

</details>


### [44] [Enhancing classical simulation with noisy quantum devices](https://arxiv.org/abs/2601.08772)
*Ruiqi Zhang,Fuchuan Wei,Zhaohui Wei*

Main category: quant-ph

TL;DR: NDE-CS protocol uses noisy quantum hardware to enhance classical simulation of quantum circuits by learning how target circuits decompose into Clifford circuits under realistic noise, enabling accurate estimation of ideal expectation values with dramatically reduced sampling cost.


<details>
  <summary>Details</summary>
Motivation: As quantum devices improve but remain noisy, existing approaches focus on recovering noiseless outputs. The authors propose instead to directly leverage noisy hardware as computational resources to enhance classical simulation of quantum circuits, harnessing noise as an asset rather than just mitigating it.

Method: Introduces Noisy-device-enhanced Classical Simulation (NDE-CS) protocol that combines noisy executions of target circuits with noisy Clifford circuits to learn how target circuits decompose into Clifford circuits under realistic noise. The learned relation is then reused in the noiseless Clifford limit, enabling accurate estimation of ideal expectation values with reduced sampling cost.

Result: Numerical simulations on Trotterized Ising circuits show orders-of-magnitude reductions in sampling cost compared to purely classical Monte Carlo approaches while maintaining same accuracy. Compared to Sparse Pauli Dynamics (SPD), NDE-CS shows much more favorable scaling where SPD scales exponentially with system size.

Conclusion: NDE-CS establishes a scalable hybrid simulation approach for quantum circuits where noise is harnessed as a computational asset, demonstrating that noisy quantum devices can be directly leveraged to enhance classical simulation rather than just being corrected or mitigated.

Abstract: As quantum devices continue to improve in scale and precision, a central challenge is how to effectively utilize noisy hardware for meaningful computation. Most existing approaches aim to recover noiseless circuit outputs from noisy ones through error mitigation or correction. Here, we show that noisy quantum devices can be directly leveraged as computational resources to enhance the classical simulation of quantum circuits. We introduce the Noisy-device-enhanced Classical Simulation (NDE-CS) protocol, which improves stabilizer-based classical Monte Carlo simulation methods by incorporating data obtained from noisy quantum hardware. Specifically, NDE-CS uses noisy executions of a target circuit together with noisy Clifford circuits to learn how the target circuit can be expressed in terms of Clifford circuits under realistic noise. The same learned relation can then be reused in the noiseless Clifford limit, enabling accurate estimation of ideal expectation values with substantially reduced sampling cost. Numerical simulations on Trotterized Ising circuits demonstrate that NDE-CS achieves orders-of-magnitude reductions in sampling cost compared to the underlying purely classical Monte Carlo approaches from which it is derived, while maintaining the same accuracy. We also compare NDE-CS with Sparse Pauli Dynamics (SPD), a powerful classical framework capable of simulating quantum circuits at previously inaccessible scales, and provide an example where the cost of SPD scales exponentially with system size, while NDE-CS scales much more favorably. These results establish NDE-CS as a scalable hybrid simulation approach for quantum circuits, where noise can be harnessed as a computational asset.

</details>


### [45] [Single-Period Floquet Control of Bosonic Codes with Quantum Lattice Gates](https://arxiv.org/abs/2601.08782)
*Tangyou Huang,Lei Du,Lingzhen Guo*

Main category: quant-ph

TL;DR: Analytical Floquet method enables single-period synthesis of arbitrary unitaries for bosonic codes, using quantum lattice gates to implement logical operations efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing Floquet protocols for bosonic codes require slow adiabatic ramps with thousands of driving periods, creating a bottleneck for practical implementation.

Method: Introduces an analytical and deterministic Floquet method that directly synthesizes arbitrary unitaries within a single period, generating phase-space unitary ensembles with Haar-random statistics. Uses quantum lattice gates that decompose quantum circuits into primitive operations by harnessing Josephson junction nonlinearity.

Result: Successfully prepares various prototypical bosonic codes from vacuum and implements single-qubit logical gates with high fidelities using quantum lattice gates.

Conclusion: The method circumvents the bottleneck of slow adiabatic protocols, enabling practical pseudorandom unitaries and efficient continuous-variable quantum computing for bosonic codes.

Abstract: Bosonic codes constitute a promising route to fault-tolerant quantum computing. {Existing Floquet protocols enable analytical construction of bosonic codes but typically rely on slow adiabatic ramps with thousands of driving periods.} In this work, we circumvent this bottleneck by introducing an analytical and deterministic Floquet method that directly synthesizes arbitrary unitaries within a single period. The phase-space unitary ensembles generated by our approach reproduce the Haar-random statistics, enabling practical pseudorandom unitaries in continuous-variable systems. We prepare various prototypical bosonic codes from vacuum and implement single-qubit logical gates with high fidelities using quantum lattice gates. By harnessing the full intrinsic nonlinearity of Josephson junctions, quantum lattice gates decompose quantum circuits into primitive operations for efficient continuous-variable quantum computing.

</details>


### [46] [Optimal logical Bell measurements on stabilizer codes with linear optics](https://arxiv.org/abs/2601.08820)
*Simon D. Reiß,Peter van Loock*

Main category: quant-ph

TL;DR: Logical Bell measurements on stabilizer codes can be mapped to single physical Bell measurements, establishing a general upper bound on success probability and ruling out combining partial physical measurements into full logical information.


<details>
  <summary>Details</summary>
Motivation: Bell measurements are fundamental in quantum information but cannot be performed perfectly with linear optics, even without photon loss. The paper aims to understand logical Bell measurements on stabilizer codes and determine their fundamental limitations.

Method: Using stabilizer group theory, the authors demonstrate that any logical Bell measurement on stabilizer codes can be mapped to a single physical Bell measurement on any qubit pair from the two codes. They formulate sufficient criteria to find measurement schemes where a single successful physical BM yields full logical information through adapted subsequent measurements.

Result: The mapping provides a general upper bound on logical BM success probability and rules out the possibility of combining partial physical BM outcomes into full logical stabilizer information. The approach works for various codes (quantum parity, five-qubit, standard/rotated planar surface, tree, and seven-qubit Steane codes) and attains the general upper bound for all these codes.

Conclusion: The stabilizer group theory approach provides a general framework for analyzing logical Bell measurements, establishing fundamental limits on their success probability and enabling optimal measurement schemes that achieve these bounds across diverse stabilizer codes.

Abstract: Bell measurements (BMs) are ubiquitous in quantum information and technology. They are basic elements for quantum commmunication, computation, and error correction. In particular, when performed on logical qubits encoded in physical photonic qubits, they allow for a read-out of stabilizer syndrome information to enhance loss tolerance in qubit-state transmission and fusion. However, even in an ideal setting without photon loss, BMs cannot be done perfectly based on the simplest experimental toolbox of linear optics. Here we demonstrate that any logical BM on stabilizer codes can always be mapped onto a single physical BM perfomed on any qubit pair from the two codes. As a necessary condition for the success of a logical BM, this provides a general upper bound on its success probability, especially ruling out the possibility that the stabilizer information obtainable from only partially succeeding, physical linear-optics BMs could be combined into the full logical stabilizer information. We formulate sufficient criteria to find schemes for which a single successful BM on the physical level will always allow to obtain the full logical information by suitably adapting the subsequent physical measurements. Our approach based on stabilizer group theory is generally applicable to any stabilizer code, which we demonstrate for quantum parity, five-qubit, standard and rotated planar surface, tree, and seven-qubit Steane codes. Our schemes attain the general upper bound for all these codes, while this bound had previously only been reached for the quantum parity code.

</details>


### [47] [Breaking the Orthogonality Barrier in Quantum LDPC Codes](https://arxiv.org/abs/2601.08824)
*Kenta Kasai*

Main category: quant-ph

TL;DR: The paper overcomes limitations in quantum LDPC code design by using controlled-commutativity permutation matrices and partial orthogonality constraints, enabling construction of high-girth quantum LDPC codes without the usual distance upper bounds.


<details>
  <summary>Details</summary>
Motivation: In classical LDPC codes, increasing Tanner graph girth while maintaining regular degree distributions improves both belief-propagation decoding performance and minimum distance. However, in quantum LDPC codes, enforcing both orthogonality and regularity typically reduces girth and imposes structural upper bounds on minimum distance, creating a fundamental trade-off that limits code performance.

Method: The authors use permutation matrices with controlled commutativity and restrict orthogonality constraints to only necessary parts of the construction while preserving regular check-matrix structures. This approach breaks the conventional trade-off between orthogonality, regularity, girth, and minimum distance in quantum LDPC code design.

Result: The method enables construction of quantum LDPC codes with large girth and without the usual distance upper bounds. As a concrete demonstration, the authors construct a girth-8, (3,12)-regular [[9216,4612, ≤48]] quantum LDPC code. Under BP decoding with low-complexity post-processing, this code achieves a frame error rate as low as 10⁻⁸ on the depolarizing channel with 4% error probability.

Conclusion: The proposed design approach successfully overcomes the limitations of conventional quantum LDPC code construction by breaking the trade-off between orthogonality, regularity, girth, and minimum distance, enabling high-performance quantum LDPC codes with large girth and improved error correction capabilities.

Abstract: Classical low-density parity-check (LDPC) codes are a widely deployed and well-established technology, forming the backbone of modern communication and storage systems. It is well known that, in this classical setting, increasing the girth of the Tanner graph while maintaining regular degree distributions leads simultaneously to good belief-propagation (BP) decoding performance and large minimum distance. In the quantum setting, however, this principle does not directly apply because quantum LDPC codes must satisfy additional orthogonality constraints between their parity-check matrices. When one enforces both orthogonality and regularity in a straightforward manner, the girth is typically reduced and the minimum distance becomes structurally upper bounded.
  In this work, we overcome this limitation by using permutation matrices with controlled commutativity and by restricting the orthogonality constraints to only the necessary parts of the construction, while preserving regular check-matrix structures. This design breaks the conventional trade-off between orthogonality, regularity, girth, and minimum distance, allowing us to construct quantum LDPC codes with large girth and without the usual distance upper bounds. As a concrete demonstration, we construct a girth-8, (3,12)-regular $[[9216,4612, \leq 48]]$ quantum LDPC code and show that, under BP decoding combined with a low-complexity post-processing algorithm, it achieves a frame error rate as low as $10^{-8}$ on the depolarizing channel with error probability $4 \%$.

</details>
