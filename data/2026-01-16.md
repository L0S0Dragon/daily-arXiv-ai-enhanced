<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 70]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 4]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Limits of Rank Recovery in Bilinear Observation Problems](https://arxiv.org/abs/2601.09754)
*Seungbeom Choi*

Main category: quant-ph

TL;DR: Bilinear observation problems exhibit stable rank plateaus that persist across tolerance refinements, indicating dimensional deficits that cannot be resolved by numerical refinement alone but require structural problem modifications.


<details>
  <summary>Details</summary>
Motivation: To examine the assumption that rank deficiency in bilinear observation problems can be resolved through numerical refinement, and to understand the limitations of rank recovery in such problems by distinguishing between numerical refinement and structural problem modification.

Method: Analyzed the rank and nullity of bilinear observation operators under systematic tolerance variation, studied the operator directly rather than specific reconstruction algorithms, resolved nullspace into algebraic sectors defined by variable block structure, and compared refinement with explicit problem formulation modifications.

Result: Identified extended rank plateaus persisting across broad tolerance ranges, revealing stable dimensional deficits not removed by refinement. Nullspace exhibits organized internal structure with pronounced concentration in specific algebraic sectors. Rank recovery requires changes to the bilinear observation structure itself, not just numerical refinements.

Conclusion: Bilinear observation problems have inherent dimensional limitations that cannot be overcome by numerical refinement alone; effective rank recovery requires structural modifications to the observation problem formulation, delineating clear boundaries between numerical refinement and problem modification for accessing dimensional structure.

Abstract: Bilinear observation problems arise in many physical and information-theoretic settings, where observables and states enter multiplicatively. Rank-based diagnostics are commonly used in such problems to assess the effective dimensionality accessible to observation, often under the implicit assumption that rank deficiency can be resolved through numerical refinement. Here we examine this assumption by analyzing the rank and nullity of a bilinear observation operator under systematic tolerance variation. Rather than focusing on a specific reconstruction algorithm, we study the operator directly and identify extended rank plateaus that persist across broad tolerance ranges. These plateaus indicate stable dimensional deficits that are not removed by refinement procedures applied within a fixed problem definition. To investigate the origin of this behavior, we resolve the nullspace into algebraic sectors defined by the block structure of the variables. The nullspace exhibits a pronounced but nonexclusive concentration in specific sectors, revealing an organized internal structure rather than uniform dimensional loss. Comparing refinement with explicit modification of the problem formulation further shows that rank recovery in the reported setting requires a change in the structure of the observation problem itself. Here, "problem modification" refers to changes that alter the bilinear observation structure (e.g., admissible operator/state families or coupling constraints), in contrast to refinements that preserve the original formulation such as tolerance adjustment and numerical reparameterizations. Together, these results delineate limits of rank recovery in bilinear observation problems and clarify the distinction between numerical refinement and problem modification in accessing effective dimensional structure.

</details>


### [2] [Fractional Revival Dynamics in Kerr-Type Systems: Angular Momentum Moments and Classical Analogs](https://arxiv.org/abs/2601.09763)
*Ashish Kumar Patra,Saikumar Krithivasan*

Main category: quant-ph

TL;DR: Analysis of fractional revival dynamics in angular momentum observables and classical analogs of quantum revival phenomena using Kerr-type nonlinear Hamiltonian model.


<details>
  <summary>Details</summary>
Motivation: To extend analysis of quantum revival phenomena in two directions: 1) investigating fractional revival dynamics in angular momentum observables, and 2) examining classical analogs of quantum revival phenomena to identify structural similarities between quantum and classical systems.

Method: Using Kerr-type nonlinear Hamiltonian as paradigmatic model, analyzing autocorrelation function, moment dynamics, and phase-space structures with visualizations including quantum carpets. Deriving explicit expressions for time evolution of angular momentum moments.

Result: Higher-order angular momentum moments provide clear and selective signatures of fractional revivals. Structural similarities identified between quantum fractional revivals and recurrence behavior in classical systems.

Conclusion: Results broaden experimentally accessible diagnostics of fractional revivals and provide unified perspective on revival phenomena across quantum and classical dynamical systems.

Abstract: Wave packet revivals and fractional revivals are hallmark quantum interference phenomena that arise in systems with nonlinear energy spectra, and their signatures in expectation values of observables have been studied extensively in earlier work. In this article, we build on these studies and extend the analysis in two important directions. First, we investigate fractional revival dynamics in angular momentum observables, deriving explicit expressions for the time evolution of their moments and demonstrating that higher-order angular momentum moments provide clear and selective signatures of fractional revivals. Second, we examine classical analogs of quantum revival phenomena and elucidate structural similarities between quantum fractional revivals and recurrence behavior in representative classical systems. Using the Kerr-type nonlinear Hamiltonian as a paradigmatic model, we analyze the autocorrelation function, moment dynamics, and phase-space structures, supported by visualizations such as quantum carpets. Our results broaden the range of experimentally accessible diagnostics of fractional revivals and provide a unified perspective on revival phenomena across quantum and classical dynamical systems.

</details>


### [3] [Three questions on the future of quantum science and technology](https://arxiv.org/abs/2601.09769)
*S. Radenkovic,M. Dugic,I. Radojevic*

Main category: quant-ph

TL;DR: Analysis of current status and future development of Quantum Science and Technology


<details>
  <summary>Details</summary>
Motivation: To provide comprehensive understanding of the current state and future trajectory of Quantum Science and Technology, addressing the need for systematic assessment in this rapidly evolving field.

Method: Analysis and synthesis of current research, technological developments, and expert perspectives in quantum science and technology.

Result: Presentation of current status assessment and future development roadmap for Quantum Science and Technology.

Conclusion: Quantum Science and Technology is at a critical development stage with significant future potential across multiple application domains.

Abstract: The answers on the current status and future development of Quantum Science and Technology are presented.

</details>


### [4] [Hierarchical time crystals](https://arxiv.org/abs/2601.09779)
*Jan Carlo Schumann,Igor Lesanovsky,Parvinder Solanki*

Main category: quant-ph

TL;DR: A hierarchical time crystal phase emerges from coupling discrete and continuous time crystals, inducing simultaneous two-fold temporal symmetry breaking with an emergent discrete symmetry.


<details>
  <summary>Details</summary>
Motivation: To explore the intriguing effects that arise from mutual interaction between discrete and continuous time crystals, which have been extensively studied as standalone systems but not in coupled configurations.

Method: Using a time-independent coupled system of discrete and continuous time crystals to induce simultaneous two-fold temporal symmetry breaking, demonstrating the phenomenon for fundamentally different coupling schemes.

Result: The coupled system produces a hierarchical time crystal phase where one subsystem breaks an emergent discrete temporal symmetry that doesn't exist in the dynamical generator but emerges dynamically, creating a convoluted non-equilibrium phase that is robust across wide parameter ranges.

Conclusion: Hierarchical time crystals represent a novel phase of matter resulting from coupling different types of time crystals, demonstrating robust emergence of simultaneous symmetry breaking with dynamically emergent discrete symmetries.

Abstract: Spontaneous symmetry breaking is one of the central organizing principles in physics. Time crystals have emerged as an exotic phase of matter, spontaneously breaking the time translational symmetry, and are mainly categorized as discrete or continuous. While these distinct types of time crystals have been extensively explored as standalone systems, intriguing effects can arise from their mutual interaction. Here, we demonstrate that a time-independent coupled system of discrete and continuous time crystals induces a simultaneous two-fold temporal symmetry breaking, resulting in a hierarchical time crystal phase. Interestingly, one of the subsystems breaks an emergent discrete temporal symmetry that does not exist in the dynamical generator but rather emerges dynamically, leading to a convoluted non-equilibrium phase. We demonstrate that hierarchical time crystals are robust, emerging for fundamentally different coupling schemes and persisting across wide ranges of system parameters.

</details>


### [5] [Zero-Error List Decoding for Classical-Quantum Channels](https://arxiv.org/abs/2601.09786)
*Marco Dalai,Filippo Girardi,Ludovico Lami*

Main category: quant-ph

TL;DR: Study of zero-error capacity of pure-state classical-quantum channels with list decoding, providing achievability bound for list-size 2 and general converse bound, identifying conditions where bounds coincide, and revealing quantum peculiarity where sphere-packing bound divergence rate may not be achievable.


<details>
  <summary>Details</summary>
Motivation: To investigate the zero-error capacity of pure-state classical-quantum channels under list decoding, extending classical information theory concepts to quantum settings and exploring unique quantum phenomena that differ from classical counterparts.

Method: Develop achievability bound for list-size two and general converse bound valid for any fixed list size. Analyze channels where pairwise absolute state overlaps form positive semi-definite matrices. Compare with classical sphere-packing bound behavior.

Result: Achievability and converse bounds are derived, coinciding for channels with positive semi-definite overlap matrices. Discovered quantum peculiarity: unlike classical case, the rate at which sphere-packing bound diverges might not be achievable by zero-error list codes, even with arbitrarily large list size.

Conclusion: The work establishes fundamental bounds for zero-error capacity of pure-state classical-quantum channels with list decoding, reveals conditions for tight bounds, and highlights a distinctive quantum phenomenon where classical intuition about sphere-packing bound achievability fails to extend to quantum settings.

Abstract: The aim of this work is to study the zero-error capacity of pure-state classical-quantum channels in the setting of list decoding. We provide an achievability bound for list-size two and a converse bound holding for every fixed list size. The two bounds coincide for channels whose pairwise absolute state overlaps form a positive semi-definite matrix. Finally, we discuss a remarkable peculiarity of the classical-quantum case: differently from the fully classical setting, the rate at which the sphere-packing bound diverges might not be achievable by zero-error list codes, even when we take the limit of fixed but arbitrarily large list size.

</details>


### [6] [Background cancellation for frequency-selective quantum sensing](https://arxiv.org/abs/2601.09792)
*Ricard Puig,Nathan Constantinides,Bharath Hebbe Madhusudhana,Daniel Bowring,C. Huerta Alderete,Andrew T. Sornborger*

Main category: quant-ph

TL;DR: A quantum sensing method using time-independent interactions and entanglement to create a passive, tunable frequency filter that detects only target frequencies above a threshold, eliminating complex control and heavy post-processing.


<details>
  <summary>Details</summary>
Motivation: Current quantum sensing methods for detecting weak time-dependent signals require complex dynamical control of quantum sensors and extensive classical post-processing, creating implementation challenges.

Method: Proposes a quantum sensor that uses time-independent interactions and entanglement to function as a passive, tunable, thresholded frequency filter. The frequency selectivity and thresholding behavior are encoded directly into the sensor's dynamics.

Result: The sensor becomes responsive only to a target frequency of choice whose amplitude exceeds a predetermined threshold, enabling selective detection without complex control schemes.

Conclusion: This approach circumvents the need for complex dynamical control and reduces post-processing overhead in quantum sensing applications for detecting weak time-dependent signals.

Abstract: A key challenge in quantum sensing is the detection of weak time dependent signals, particularly those that arise as specific frequency perturbations over a background field. Conventional methods usually demand complex dynamical control of the quantum sensor and heavy classical post-processing. We propose a quantum sensor that leverages time independent interactions and entanglement to function as a passive, tunable, thresholded frequency filter. By encoding the frequency selectivity and thresholding behavior directly into the dynamics, the sensor is responsive only to a target frequency of choice whose amplitude is above a threshold. This approach circumvents the need for complex control schemes and reduces the post-processing overhead.

</details>


### [7] [Localization of quantum states within subspaces](https://arxiv.org/abs/2601.09817)
*L. L. Salcedo*

Main category: quant-ph

TL;DR: The paper proposes a formal definition for quantum state localization probability within a subspace, identifies the localized component, establishes mathematical properties, and discusses quantum information applications.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical framework for quantifying how much a quantum state is localized within a specific subspace of the full Hilbert space, addressing a fundamental concept in quantum mechanics that lacks precise formalization.

Method: Proposes a precise mathematical definition for localization probability, explicitly identifies the localized component of quantum states, and establishes formal mathematical properties through theoretical analysis.

Result: Develops a formal definition of localization probability, identifies the localized state component, establishes mathematical properties of the localization measure, and provides quantum information interpretations.

Conclusion: The paper provides a rigorous mathematical foundation for quantifying quantum state localization, with established properties and potential applications in quantum information theory.

Abstract: A precise definition is proposed for the localization probability of a quantum state within a given subspace of the full Hilbert space of a quantum system. The corresponding localized component of the state is explicitly identified, and several mathematical properties are established. Applications and interpretations in the context of quantum information are also discussed.

</details>


### [8] [Fragmented Topological Excitations in Generalized Hypergraph Product Codes](https://arxiv.org/abs/2601.09850)
*Meng-Yuan Li,Yue Wu*

Main category: quant-ph

TL;DR: Generalized hypergraph product codes reveal fracton topological orders with non-monotonic ground state degeneracy, non-Abelian defects, and novel fragmented topological excitations in 4D.


<details>
  <summary>Details</summary>
Motivation: To explore fracton topological orders in a family of generalized hypergraph product codes and understand their connection to exactly solvable spin models, particularly investigating exotic properties like non-monotonic ground state degeneracy and novel excitation types.

Method: Analyze a family of codes obtained through a general construction approach, specifically generalized hypergraph product codes termed "orthoplex models" based on stabilizer geometry. Investigate properties in 3D and 4D versions of these models.

Result: In 3D orthoplex models: non-monotonic ground state degeneracy as function of system size and non-Abelian lattice defects. In 4D: discovery of fragmented topological excitations - point-like in real space but projecting to connected objects (loops) in lower dimensions, representing intermediate class between point-like and extended excitations.

Conclusion: Generalized hypergraph product codes provide a versatile, analytically tractable platform for studying fracton orders, revealing rich features including fragmented excitations that bridge point-like and extended topological excitation classes.

Abstract: Product code construction is a powerful tool for constructing quantum stabilizer codes, which serve as a promising paradigm for realizing fault-tolerant quantum computation. Furthermore, the natural mapping between stabilizer codes and the ground states of exactly solvable spin models also motivates the exploration of many-body orders in the stabilizer codes. In this work, we investigate the fracton topological orders in a family of codes obtained by a recently proposed general construction. More specifically, this code family can be regarded as a class of generalized hypergraph product (HGP) codes. We term the corresponding exactly solvable spin models \textit{orthoplex models}, based on the geometry of the stabilizers. In the 3D orthoplex model, we identify a series of intriguing properties within this model family, including non-monotonic ground state degeneracy (GSD) as a function of system size and non-Abelian lattice defects. Most remarkably, in 4D we discover \textit{fragmented topological excitations}: while such excitations manifest as discrete, isolated points in real space, their projections onto lower-dimensional subsystems form connected objects such as loops, revealing the intrinsic topological nature of these excitations. Therefore, fragmented excitations constitute an intriguing intermediate class between point-like and spatially extended topological excitations. In addition, these rich features establish the generalized HGP codes as a versatile and analytically tractable platform for studying the physics of fracton orders.

</details>


### [9] [Multi-level quantum emitter in an optical waveguide: paradoxes and resolutions](https://arxiv.org/abs/2601.09854)
*Ben Lang*

Main category: quant-ph

TL;DR: Theoretical investigation of optical dipole interactions between multi-level quantum systems and single-mode optical waveguides, revealing paradoxical directional photon flux behaviors and polarization-dependent reflection/transmission properties.


<details>
  <summary>Details</summary>
Motivation: To explore fundamental quantum optical phenomena in waveguide quantum electrodynamics, particularly examining paradoxical situations where quantum states produce opposite photon fluxes and investigating how polarization affects interaction properties despite apparent contradictions with quantum mechanical principles.

Method: Theoretical analysis of optical dipole interactions between multi-level quantum systems and single-mode optical waveguides with arbitrary local polarization. Mathematical investigation of paradoxical scenarios including non-orthogonal quantum states producing opposite photon fluxes and polarization-dependent reflection/transmission properties. Examination of a four-level system as a specific case study.

Result: Discovery of paradoxical situations where two non-orthogonal quantum states produce photon fluxes in opposite directions, yet consistent with quantum mechanical unitarity. Finding that isotropic quantum emitters can switch between 100% transmission and 100% reflection based on infinitesimal waveguide polarization rotations in the zero-loss limit. Demonstration that a four-level system can function as a non-destructive parity measurement of photon number.

Conclusion: The study reveals counterintuitive quantum optical phenomena in waveguide QED systems, showing that apparent paradoxes in directional photon emission and polarization-dependent interactions are consistent with quantum mechanical principles. These findings provide fundamental insights into quantum light-matter interactions and suggest potential applications in quantum information processing, such as non-destructive photon number parity measurements.

Abstract: We theoretically investigate the optical dipole interaction between a multi-level quantum system and a single-mode optical waveguide of any local polarisation. We investigate several paradoxical seeming situations, for example we find a situation in which there exist two non-orthogonal quantum states, each of which results in a photon flux in the opposite direction to the other. We show how, despite appearances, this does not break the unitary requirements of quantum mechanics. We also find that an isotropic quantum emitter can be either reflective or transmissive to light depending on the waveguide polarisation at the emitter location, indeed in the zero loss limit such a system changes from 100% transmission to 100% reflection due to an infinitesimal polarisation rotation. An example case for a four level system is also considered, which is found to operate as a non-destructive parity measurement of the photon number.

</details>


### [10] [Time-Dynamic Circuits for Fault-Tolerant Shift Automorphisms in Quantum LDPC Codes](https://arxiv.org/abs/2601.09911)
*Younghun Kim,Spiro Gicev,Martin Sevior,Muhammad Usman*

Main category: quant-ph

TL;DR: Dynamic syndrome measurement circuits enable shift automorphisms in qLDPC codes without reducing circuit distance, achieving logical error rates comparable to idle operations and orders of magnitude improvement over SWAP-based approaches.


<details>
  <summary>Details</summary>
Motivation: Shift automorphisms are fundamental for completing universal gate sets in qLDPC codes, but existing SWAP-based implementations yield unacceptably high logical error rates compared to fault-tolerant idle operations, creating a practical barrier to realizing low-overhead quantum memories.

Method: Dynamically varying syndrome measurement circuits to implement shift automorphisms without reducing circuit distance, benchmarked on twisted and untwisted weight-6 generalized toric codes (including gross code family) using BP-OSD decoder under circuit-level noise model (SI1000).

Result: Dynamic circuits achieve logical error rates comparable to idle operations, with more than an order of magnitude reduction relative to SWAP-based schemes for gross code at physical error rate 10^-3, improving both error resilience and time overhead of shift automorphisms.

Conclusion: The approach provides a practical pathway for implementing shift automorphisms in qLDPC codes, enabling alternative syndrome extraction circuit designs and extending dynamic circuit techniques beyond surface codes toward more efficient quantum error correction architectures.

Abstract: Quantum low-density parity-check (qLDPC) codes have emerged as a promising approach for realizing low-overhead logical quantum memories. Recent theoretical developments have established shift automorphisms as a fundamental building block for completing the universal set of logical gates for qLDPC codes. However, practical challenges remain because the existing SWAP-based shift automorphism yields logical error rates that are orders of magnitude higher than those for fault-tolerant idle operations. In this work, we address this issue by dynamically varying the syndrome measurement circuits to implement the shift automorphisms without reducing the circuit distance. We benchmark our approach on both twisted and untwisted weight-6 generalized toric codes, including the gross code family. Our time-dynamic circuits for shift automorphisms achieve performance comparable to the idle operations under the circuit-level noise model (SI1000). Specifically, the dynamic circuits achieve more than an order of magnitude reduction in logical error rates relative to the SWAP-based scheme for the gross code at a physical error rate of $10^{-3}$, employing the BP-OSD decoder. Our findings improve both the error resilience and the time overhead of the shift automorphisms in qLDPC codes. Furthermore, our work can lead to alternative syndrome extraction circuit designs, such as leakage removal protocols, providing a practical pathway to utilizing dynamic circuits that extend beyond surface codes towards qLDPC codes.

</details>


### [11] [Learning to Decode in Parallel: Self-Coordinating Neural Network for Real-Time Quantum Error Correction](https://arxiv.org/abs/2601.09921)
*Kai Zhang,Zhengzhong Yi,Shaojun Guo,Linghang Kong,Situ Wang,Xiaoyu Zhan,Tan He,Weiping Lin,Tao Jiang,Dongxin Gao,Yiming Zhang,Fangming Liu,Fang Zhang,Zhengfeng Ji,Fusheng Chen,Jianxin Chen*

Main category: quant-ph

TL;DR: A neural network decoder framework achieves real-time parallel decoding for fault-tolerant quantum computation by training a transformer-based network on local corrections, enabling high-throughput, high-accuracy error correction on superconducting quantum processors.


<details>
  <summary>Details</summary>
Motivation: Existing neural network decoders like AlphaQubit lack the parallelism needed for real-time syndrome decoding in FTQC, and integrating them with parallel window decoding schemes is challenging because they output global logical corrections rather than local physical corrections.

Method: Train a recurrent, transformer-based neural network specifically for parallel window decoding. While still outputting a single bit, derive training labels from consistent local corrections and train on various decoding window types simultaneously, enabling self-coordination across neighboring windows.

Result: Overcomes throughput bottleneck preventing AlphaQubit-type decoders in FTQC. Achieves SOTA accuracy and required throughput for real-time QEC. Benchmarked on Zuchongzhi 3.2 processor with surface codes up to distance 7, showing superior accuracy. A single TPU v6e can decode surface codes up to distance 25 within 1μs per round.

Conclusion: First scalable neural-network-based parallel decoding framework that simultaneously achieves state-of-the-art accuracy and stringent throughput requirements for real-time quantum error correction, enabling practical fault-tolerant quantum computation.

Abstract: Fast, reliable decoders are pivotal components for enabling fault-tolerant quantum computation (FTQC). Neural network decoders like AlphaQubit have demonstrated potential, achieving higher accuracy than traditional human-designed decoding algorithms. However, existing implementations of neural network decoders lack the parallelism required to decode the syndrome stream generated by a superconducting logical qubit in real time. Moreover, integrating AlphaQubit with sliding window-based parallel decoding schemes presents non-trivial challenges: AlphaQubit is trained solely to output a single bit corresponding to the global logical correction for an entire memory experiment, rather than local physical corrections that can be easily integrated. We address this issue by training a recurrent, transformer-based neural network specifically tailored for parallel window decoding. While it still outputs a single bit, we derive training labels from a consistent set of local corrections and train on various types of decoding windows simultaneously. This approach enables the network to self-coordinate across neighboring windows, facilitating high-accuracy parallel decoding of arbitrarily long memory experiments.
  As a result, we overcome the throughput bottleneck that previously precluded the use of AlphaQubit-type decoders in FTQC. Our work presents the first scalable, neural-network-based parallel decoding framework that simultaneously achieves SOTA accuracy and the stringent throughput required for real-time quantum error correction. Using an end-to-end experimental workflow, we benchmark our decoder on the Zuchongzhi 3.2 superconducting quantum processor on surface codes with distances up to 7, demonstrating its superior accuracy. Moreover, we demonstrate that, using our approach, a single TPU v6e is capable of decoding surface codes with distances up to 25 within 1us per decoding round.

</details>


### [12] [Beyond Optimization: Harnessing Quantum Annealer Dynamics for Machine Learning](https://arxiv.org/abs/2601.09938)
*Akitada Sakurai,Aoi Hayashi,Tadayoshi Matumori,Daisuke Kaji,Tadashi Kadowaki,Kae Nemoto*

Main category: quant-ph

TL;DR: Quantum annealing used for machine learning by encoding classical data into Ising Hamiltonian, evolving on quantum annealer, and using resulting probability distributions as feature maps for classification.


<details>
  <summary>Details</summary>
Motivation: While quantum annealing is typically used for combinatorial optimization, its coherent dynamics offer potential for machine learning applications, suggesting a novel approach to leverage quantum annealers for classification tasks.

Method: Encode classical data into an Ising Hamiltonian, evolve it on a quantum annealer, use resulting probability distributions as feature maps for classification. Experiments conducted on quantum annealer with Digits dataset and simulations on MNIST.

Result: Short annealing times yield higher classification accuracy, while longer times reduce accuracy but lower sampling costs. Participation ratio introduced as measure of effective model size shows strong correlation with generalization performance.

Conclusion: Quantum annealing can be effectively used for machine learning classification tasks, with annealing time playing a crucial role in balancing accuracy and sampling costs, and participation ratio serving as a useful metric for understanding generalization.

Abstract: Quantum annealing is typically regarded as a tool for combinatorial optimization, but its coherent dynamics also offer potential for machine learning. We present a model that encodes classical data into an Ising Hamiltonian, evolves it on a quantum annealer, and uses the resulting probability distributions as feature maps for classification. Experiments on the quantum annealer machine with the Digits dataset, together with simulations on MNIST, demonstrate that short annealing times yield higher classification accuracy, while longer times reduce accuracy but lower sampling costs. We introduce the participation ratio as a measure of the effective model size and show its strong correlation with generalization.

</details>


### [13] [Three Months in the Life of Cloud Quantum Computing](https://arxiv.org/abs/2601.09943)
*Darrell Teegarden,Allison Casey,F. Gino Serpa,Patrick Becker,Asmita Brahme,Saanvi Kataria,Paul Lopata*

Main category: quant-ph

TL;DR: Systematic analysis of cloud quantum computing environments over three months, documenting connection metrics, algorithm execution, qubit scaling effects, execution times, and costs across platforms using out-of-the-box settings.


<details>
  <summary>Details</summary>
Motivation: Cloud quantum computing has become accessible but requires complex toolchains that are hardware-dependent and evolving. Understanding trade-offs in this process is essential for evaluating quantum computers' power and utility, yet systematic empirical data is lacking.

Method: Three-month systematic investigation using out-of-the-box settings and tools across multiple quantum computers and cloud platforms. Executed a single algorithm consistently across machines, collected metadata on connections, algorithm execution, qubit scaling, simulations, execution times, and costs.

Result: Provides concrete metadata including connection metrics to different services, algorithm execution data, effects of varying qubit numbers, comparisons to simulations, execution times, and cost information across platforms.

Conclusion: Offers empirical insights into cloud quantum computing environments through carefully curated data, providing a consistent view of algorithm execution across platforms and time to help researchers explore quantum computing potential without focusing on algorithm innovation or platform-specific optimization.

Abstract: Quantum Computing (QC) has evolved from a few custom quantum computers, which were only accessible to their creators, to an array of commercial quantum computers that can be accessed on the cloud by anyone. Accessing these cloud quantum computers requires a complex chain of tools that facilitate connecting, programming, simulating algorithms, estimating resources, submitting quantum computing jobs, retrieving results, and more. Some steps in the chain are hardware dependent and subject to change as both hardware and software tools, such as available gate sets and optimizing compilers, evolve. Understanding the trade-offs inherent in this process is essential for evaluating the power and utility of quantum computers. ARLIS has been systematically investigating these environments to understand these complexities. The work presented here is a detailed summary of three months of using such quantum programming environments. We show metadata obtained from these environments, including the connection metrics to the different services, the execution of algorithms, the testing of the effects of varying the number of qubits, comparisons to simulations, execution times, and cost. Our objective is to provide concrete data and insights for those who are exploring the potential of quantum computing. It is not our objective to present any new algorithms or optimize performance on any particular machine or cloud platform; rather, this work is focused on providing a consistent view of a single algorithm executed using out-of-the-box settings and tools across machines, cloud platforms, and time. We present insights only available from these carefully curated data.

</details>


### [14] [Parallelizing the Variational Quantum Eigensolver: From JIT Compilation to Multi-GPU Scaling](https://arxiv.org/abs/2601.09951)
*Rylan Malarchick,Ashton Steed*

Main category: quant-ph

TL;DR: VQE implementation for H₂ potential energy surface achieves 117× speedup through GPU acceleration and parallelization, reducing runtime from 10 minutes to 5 seconds.


<details>
  <summary>Details</summary>
Motivation: To accelerate variational quantum eigensolver calculations for quantum chemistry applications by leveraging HPC resources and parallelization techniques to enable interactive exploration of molecular systems.

Method: Implemented VQE using PennyLane framework on HPC cluster with 4× NVIDIA H100 GPUs. Employed four-phase optimization: (1) optimizer + JIT compilation, (2) GPU device acceleration, (3) MPI parallelization, and (4) multi-GPU scaling. Conducted scaling studies from 4-26 qubits comparing CPU vs GPU performance.

Result: Achieved 117× total speedup (593.95s → 5.04s) for H₂ potential energy surface across 100 bond lengths. Individual speedups: 4.13× from optimizer+JIT, 3.60-80.5× from GPU acceleration (scaling with qubit count), 28.5× from MPI, and 3.98× from multi-GPU with 99.4% parallel efficiency. Single H100 can simulate up to 29 qubits before memory limits.

Conclusion: Comprehensive parallelization and GPU acceleration dramatically accelerate VQE calculations, enabling interactive quantum chemistry exploration. The approach demonstrates near-perfect scaling and establishes practical quantum simulation capabilities on current HPC hardware.

Abstract: The Variational Quantum Eigensolver (VQE) is a hybrid quantum-classical algorithm for computing ground state energies of molecular systems. We implement VQE to calculate the potential energy surface of the hydrogen molecule (H$_2$) across 100 bond lengths using the PennyLane quantum computing framework on an HPC cluster featuring 4$\times$ NVIDIA H100 GPUs (80GB each). We present a comprehensive parallelization study with four phases: (1) Optimizer + JIT compilation achieving 4.13$\times$ speedup, (2) GPU device acceleration achieving 3.60$\times$ speedup at 4 qubits scaling to 80.5$\times$ at 26 qubits, (3) MPI parallelization achieving 28.5$\times$ speedup, and (4) Multi-GPU scaling achieving 3.98$\times$ speedup with 99.4% parallel efficiency across 4 H100 GPUs. The combined effect yields 117$\times$ total speedup for the H$_2$ potential energy surface (593.95s $\rightarrow$ 5.04s). We conduct a CPU vs GPU scaling study from 4--26 qubits, finding GPU advantage at all scales with speedups ranging from 10.5$\times$ to 80.5$\times$. Multi-GPU benchmarks demonstrate near-perfect scaling with 99.4% efficiency and establish that a single H100 can simulate up to 29 qubits before hitting memory limits. The optimized implementation reduces runtime from nearly 10 minutes to 5 seconds, enabling interactive quantum chemistry exploration.

</details>


### [15] [Statistical-noise-enhanced multi-photon interference](https://arxiv.org/abs/2601.09977)
*Rikizo Ikuta*

Main category: quant-ph

TL;DR: Three-photon interference in symmetric circuits shows non-monotonic visibility dependence on photon statistics, with engineered super-Poissonian statistics maximizing visibility beyond single-photon signatures, revealing statistical complementarity between quantum and classical advantages.


<details>
  <summary>Details</summary>
Motivation: The paper investigates how photon statistics governs multi-photon interference, challenging the conventional understanding that higher intensity correlation functions monotonically degrade interference visibility as seen in standard two-photon Hong-Ou-Mandel interference.

Method: The study examines three-photon interference in symmetric circuits, specifically using discrete Fourier transform circuits. Engineered super-Poissonian photon-number fluctuations are created using a modulated laser to manipulate photon statistics and study their effects on interference visibility.

Result: The research reveals that in three-photon interference, visibility does not monotonically degrade with higher intensity correlation functions. Engineered super-Poissonian statistics can maximize visibility, surpassing single-photon signatures. By tuning symmetric circuit parameters, the visibility hierarchy inverts relative to Poissonian statistics benchmarks.

Conclusion: The findings demonstrate a trade-off where quantum and classical advantages are mutually exclusive resources for interference, indicating a form of statistical complementarity that challenges conventional understanding of photon statistics in multi-photon interference.

Abstract: Photon statistics plays a governing role in multi-photon interference. While interference visibility in the standard two-photon case, known as Hong-Ou-Mandel interference, monotonically degrades with higher intensity correlation functions, we show that this monotonicity does not hold for three-photon interference in symmetric circuits. We reveal that, in the discrete Fourier transform circuit, engineered super-Poissonian photon-number fluctuations, realized using a modulated laser, maximize the visibility, surpassing the magnitude of the single-photon signature. In addition, by tuning the symmetric circuit parameters, we demonstrate that the visibility hierarchy inverts relative to the benchmark of Poissonian statistics. This trade-off implies that quantum and classical advantages are mutually exclusive resources for interference, indicating a form of statistical complementarity.

</details>


### [16] [Double Markovity for quantum systems](https://arxiv.org/abs/2601.09995)
*Masahito Hayashi,Jinpei Zhao*

Main category: quant-ph

TL;DR: Quantum analogues of double Markovity established for tripartite and four-party states, enabling extension of SDR technique to quantum systems.


<details>
  <summary>Details</summary>
Motivation: The SDR technique is powerful for Gaussian optimality in classical information theory but relies on double Markovity. Extending SDR-type arguments to quantum systems requires quantum analogues of double Markovity, which was a key bottleneck.

Method: For tripartite states, characterized simultaneous Markov conditions A-B-C and A-C-B via compatible projective measurements on B and C inducing common classical label J. For strictly positive four-party states, showed equivalence between A-(BD)-C and A-(CD)-B conditions and A-D-(BC) condition.

Result: Established quantum analogues of double Markovity: (1) for tripartite states, simultaneous Markov conditions correspond to existence of compatible projective measurements yielding common classical label; (2) for four-party states, certain Markov conditions are equivalent to A-D-(BC) condition.

Conclusion: These results remove the key bottleneck in extending SDR-type arguments to quantum systems by providing the necessary quantum analogues of double Markovity, enabling application of SDR technique to quantum information theory problems.

Abstract: The subadditivity-doubling-rotation (SDR) technique is a powerful route to Gaussian optimality in classical information theory and relies on strict subadditivity and its equality-case analysis, where double Markovity is a standard tool. We establish quantum analogues of double Markovity. For tripartite states, we characterize the simultaneous Markov conditions A-B-C and A-C-B via compatible projective measurements on B and C that induce a common classical label J yielding A-J-(BC). For strictly positive four-party states, we show that A-(BD)-C and A-(CD)-B hold if and only if A-D-(BC) holds. These results remove a key bottleneck in extending SDR-type arguments to quantum systems.

</details>


### [17] [Reentrant topological phases and entanglement scalings in moiré-modulated extended Su-Schrieffer-Heeger Model](https://arxiv.org/abs/2601.09997)
*Guo-Qing Zhang,L. F. Quezada,Shi-Hai Dong*

Main category: quant-ph

TL;DR: The paper investigates reentrant phase transitions and universality class invariance in moiré-modulated extended SSH models, revealing bulk-boundary correspondence and universal characteristics of moiré-induced transitions.


<details>
  <summary>Details</summary>
Motivation: While moiré physics offers opportunities for advancing quantum phase transitions, the properties of reentrant phase transitions driven by moiré strength remain poorly understood, particularly in 1D condensed-matter systems.

Method: For the simplified case with intercell hopping w=0, analytical derivation of renormalization relations; for general cases, numerical phase boundary calculations in thermodynamic limit; analysis of bulk-boundary correspondence through zero-energy edge modes and entanglement spectrum degeneracy; examination of central charge from entanglement entropy and winding number changes.

Result: Analytical explanation of reentrant phenomenon via renormalization relations; numerical phase boundaries for general cases; revelation of bulk-boundary correspondence through degeneracy of zero-energy edge modes and entanglement spectrum; establishment of correspondence between central charge and winding number changes during phase transitions.

Conclusion: The study provides insights into universal characteristics and bulk-boundary correspondence for moiré-induced reentrant phase transitions in 1D condensed-matter systems, advancing understanding of moiré physics in quantum phase transitions.

Abstract: Recent studies of moiré physics have unveiled a wealth of opportunities for significantly advancing the field of quantum phase transitions. However, properties of reentrant phase transitions driven by moiré strength are poorly understood. Here, we investigate the reentrant sequence of phase transitions and the invariant of universality class in moiré-modulated extended Su-Schrieffer-Heeger (SSH) model. For the simplified case with intercell hopping $w=0$, we analytically derive renormalization relations of Hamiltonian parameters to explain the reentrant phenomenon. For the general case, numerical phase boundaries are calculated in the thermodynamic limit. The bulk boundary correspondence between zero-energy edge modes and entanglement spectrum is revealed from the degeneracy of both quantities. We also address the correspondence between the central charge obtained from entanglement entropy and the change in winding number during the phase transition. Our results shed light on the understanding of universal characteristics and bulk-boundary correspondence for moiré induced reentrant phase transitions in 1D condensed-matter systems.

</details>


### [18] [Contextuality Derived from Minimal Decision Dynamics: Quantum Tug-of-War Decision Making](https://arxiv.org/abs/2601.10034)
*Song-Ju Kim*

Main category: quant-ph

TL;DR: Quantum probability emerges as necessary for modeling adaptive decision dynamics, not just as descriptive convenience, due to generative contextuality from conservation-based state updates and measurement disturbance.


<details>
  <summary>Details</summary>
Motivation: To determine whether quantum probability in decision making is merely a convenient modeling assumption or a necessary consequence of underlying decision dynamics, addressing the challenge of context dependence that classical probability theory struggles to explain.

Method: Developed a quantum extension of the Tug-of-War (TOW) model, incorporating conservation-based internal state updates and measurement-induced disturbance, then analyzed whether this framework admits non-contextual classical descriptions with unified internal states.

Result: The quantum TOW model precludes any non-contextual classical description with a single unified internal state, demonstrating that contextuality emerges generatively from adaptive learning dynamics. The measurement structure admits KCBS-type contextuality witnesses in minimal single-system settings.

Conclusion: Quantum probability is not merely a descriptive convenience but an unavoidable effective theory for adaptive decision dynamics, as contextuality arises structurally from physically grounded constraints on decision making.

Abstract: Decision making often exhibits context dependence that challenges classical probability theory. While quantum cognition has successfully modeled such phenomena, it remains unclear whether quantum probability is merely a convenient assumption or a necessary consequence of decision dynamics. Here we present a theoretical framework in which contextuality arises generatively from physically grounded constraints on decision making. By developing a quantum extension of the Tug-of-War (TOW) model, we show that conservation-based internal state updates and measurement-induced disturbance preclude any non-contextual classical description with a single, unified internal state. Contextuality therefore emerges as a structural consequence of adaptive learning dynamics. We further show that the resulting measurement structure admits Klyachko-Can-Binicioglu-Shumovsky (KCBS)-type contextuality witnesses in a minimal single-system setting. These results indicate that quantum probability is not merely a descriptive convenience, but an unavoidable effective theory for adaptive decision dynamics.

</details>


### [19] [Towards Minimal Fault-tolerant Error-Correction Sequence with Quantum Hamming Codes](https://arxiv.org/abs/2601.10042)
*Sha Shi,Xiao-Yang Xu,Min-Quan Cheng,Dong-Sheng Wang,Yun-Jiang Wang*

Main category: quant-ph

TL;DR: The paper constructs efficient fault-tolerant measurement sequences (FTMSs) for quantum Hamming codes with specific parameters, reducing sequence length to 2r+1 (just one measurement beyond non-fault-tolerant sequences) and enabling hardware-efficient circuit reuse through symmetry transformations.


<details>
  <summary>Details</summary>
Motivation: The high overhead of fault-tolerant measurement sequences poses a major challenge for implementing quantum stabilizer codes, creating a need for more efficient FTMS designs to make quantum error correction practical.

Method: Uses cyclic matrix transformations to systematically combine rows of the initial stabilizer matrix while preserving a self-dual CSS-like symmetry. This symmetry enables hardware-efficient circuit reuse where measurement circuits for the first r stabilizers can be transformed into circuits for the remaining r stabilizers by toggling boundary Hadamard gates.

Result: Demonstrates that sequence length can be reduced to exactly 2r+1 - only one additional measurement beyond the original non-fault-tolerant sequence, establishing a tight lower bound. The approach simultaneously reduces time overhead via shortened FTMS length and hardware overhead through symmetry-enabled circuit multiplexing.

Conclusion: Provides an important advance toward designing minimal FTMSs for quantum Hamming codes and may shed light on similar challenges in other quantum stabilizer codes, addressing a key open problem in fault-tolerant quantum error correction.

Abstract: The high overhead of fault-tolerant measurement sequences (FTMSs) poses a major challenge for implementing quantum stabilizer codes. Here, we address this problem by constructing efficient FTMSs for the class of quantum Hamming codes $[\![2^r-1, 2^r-1-2r, 3]\!]$ with $r=3k+1$ ($k \in \mathbb{Z}^+$). Our key result demonstrates that the sequence length can be reduced to exactly $2r+1$-only one additional measurement beyond the original non-fault-tolerant sequence, establishing a tight lower bound. The proposed method leverages cyclic matrix transformations to systematically combine rows of the initial stabilizer matrix and preserving a self-dual CSS-like symmetry analogous to that of the original quantum Hamming codes. This induced symmetry enables hardware-efficient circuit reuse: the measurement circuits for the first $r$ stabilizers are transformed into circuits for the remaining $r$ stabilizers simply by toggling boundary Hadamard gates, eliminating redundant hardware. For distance-3 fault-tolerant error correction, our approach simultaneously reduces the time overhead via shorting the FTMS length and the hardware overhead through symmetry-enabled circuit multiplexing. These results provide an important advance towards the important open problem regarding the design of minimal FTMSs for quantum Hamming codes and may shed light on similar challenges in other quantum stabilizer codes.

</details>


### [20] [Optimal qudit overlapping tomography and optimal measurement order](https://arxiv.org/abs/2601.10059)
*Shuowei Ma,Qianfan Wang,Lvzhou Li,Fei Shi*

Main category: quant-ph

TL;DR: Optimal overlapping tomography for qudit systems using generalized Gell-Mann matrices and combinatorial covering arrays, with explicit constructions for qutrits and efficient measurement scheduling.


<details>
  <summary>Details</summary>
Motivation: Quantum state tomography becomes infeasible for large systems due to exponential scaling. Overlapping tomography addresses this for qubits, but extension to higher-dimensional qudit systems remains unexplored, limiting efficient characterization of qudit-based quantum systems.

Method: Construct local measurement settings from generalized Gell-Mann matrices and establish correspondence with combinatorial covering arrays. Present two explicit constructions of optimal measurement schemes, with specific bound for n-qutrit systems. Develop efficient algorithm to optimize measurement ordering to minimize switching overhead.

Result: For n-qutrit systems, pairwise tomography requires at most 8 + 56⌈log₈ n⌉ measurement settings, with explicit scheme achieving this bound. Optimized scheduling reduces switching costs by ~50% compared to worst-case ordering.

Conclusion: The work provides practical pathway for efficient characterization of qudit systems using optimal overlapping tomography, facilitating applications in quantum communication and computation by overcoming exponential scaling limitations.

Abstract: Quantum state tomography is essential for characterizing quantum systems, but it becomes infeasible for large systems due to exponential resource scaling. Overlapping tomography addresses this challenge by reconstructing all $k$-body marginals using few measurement settings, enabling the efficient extraction of key information for many quantum tasks. While optimal schemes are known for qubits, the extension to higher-dimensional qudit systems remains largely unexplored. Here, we investigate optimal qudit overlapping tomography, constructing local measurement settings from generalized Gell-Mann matrices. By establishing a correspondence with combinatorial covering arrays, we present two explicit constructions of optimal measurement schemes. For $n$-qutrit systems, we prove that pairwise tomography requires at most $8 + 56\left\lceil \log_{8} n \right\rceil$ measurement settings, and provide an explicit scheme achieving this bound. Furthermore, we develop an efficient algorithm to determine the optimal order of these measurement settings, minimizing the experimental overhead associated with switching configurations. Compared to the worst-case ordering, our optimized schedule reduces switching costs by approximately 50\%. These results provide a practical pathway for efficient characterization of qudit systems, facilitating their application in quantum communication and computation.

</details>


### [21] [Geometric Criteria for Complete Mode Conversion in Detuned Systems via Piecewise-Coherent Modulation](https://arxiv.org/abs/2601.10066)
*Awanish Pandey*

Main category: quant-ph

TL;DR: Geometric Bloch-sphere framework enables complete mode conversion in detuned systems via piecewise-coherent modulation, with applications to optical isolation and universal bounds on switching events.


<details>
  <summary>Details</summary>
Motivation: Static phase detuning fundamentally limits coherent state transfer in asymmetric classical and quantum systems, creating a need for methods to overcome this constraint and achieve complete mode conversion despite detuning.

Method: Introduces a Bloch-sphere formulation for piecewise-coherent modulation that recasts coupled-mode dynamics as geometric trajectories, transforming algebraic control into path optimization. This reveals a cone of inaccessibility at the target pole and yields exact geodesic criteria for complete mode conversion.

Result: The framework enables breaking time-reversal symmetry to realize a magnet-free optical isolator with near-unity contrast. For detuning larger than coupling between modes, a recursive multi-step protocol enables deterministic transfer for arbitrary detunings, with derivation of a universal geometric lower bound on required coupling-switching events.

Conclusion: Geometric control via piecewise-coherent modulation on the Bloch sphere provides a powerful framework for overcoming detuning constraints in coupled-mode systems, enabling complete state transfer and practical applications like optical isolation with universal performance bounds.

Abstract: Static phase detuning fundamentally constrains coherent state transfer in asymmetric classical and quantum systems. We introduce a Bloch-sphere formulation for piecewise-coherent modulation that recasts coupled-mode dynamics as geometric trajectories, transforming algebraic control into path optimization. The approach reveals a cone of inaccessibility at the target pole and yields exact geodesic criteria for complete mode conversion in detuned systems. Leveraging this framework, we break time-reversal symmetry to realize a magnet-free optical isolator with near-unity contrast. Furthermore, for detuning larger than coupling between modes, we develop a recursive multi-step protocol enabling deterministic transfer for arbitrary detunings and derive a universal geometric lower bound on the required number of coupling-switching events.

</details>


### [22] [Pseudomode approach to Fano effect in dissipative cavity quantum electrodynamics](https://arxiv.org/abs/2601.10087)
*Kazuki Kobayashi,Tatsuro Yuge*

Main category: quant-ph

TL;DR: The paper establishes a unified framework for describing the Fano effect in dissipative cavity QED systems, showing how Fano interference emerges from spectral function structure and clarifying its non-Markovian origins.


<details>
  <summary>Details</summary>
Motivation: To understand the Fano effect in dissipative cavity quantum electrodynamics, particularly how interference between direct emitter radiation and cavity-mediated radiation can be systematically described and how this relates to non-Markovian dynamics encoded in spectral functions.

Method: Two complementary approaches: (1) Starting from a two-level system coupled to a structured reservoir, rederiving a quantum master equation via pseudomode approach with a single auxiliary mode; (2) Applying Fano diagonalization to a common-environment setup with explicit cavity mode in the strongest-interference regime.

Result: Identified the spectral function of system-environment interaction consisting of constant and non-Lorentzian contributions forming Fano profile; showed constant term is essential for Lindblad master equation and directly related to Fano interference rate; independently derived same spectral function via Fano diagonalization, establishing consistency between approaches.

Conclusion: Established unified framework for describing Fano effect in single-mode cavity QED systems, clarifying its non-Markovian origin encoded in spectral function structure, with constant term playing crucial role in obtaining Lindblad form and quantifying interference effects.

Abstract: We study the Fano effect in dissipative cavity quantum electrodynamics, which originates from the interference between the emitter's direct radiation and that mediated by a cavity mode. Starting from a two-level system coupled to a structured reservoir, we show that a quantum master equation previously derived within the Born-Markov approximation can be rederived by introducing a single auxiliary mode via pseudomode approach. We identify the corresponding spectral function of the system--environment interaction and demonstrate that it consists of a constant and a non-Lorentzian contribution forming the Fano profile. The constant term is shown to be essential for obtaining a Lindblad master equation and is directly related to the rate associated with this Fano interference. Furthermore, by applying Fano diagonalization to a common-environment setup including an explicit cavity mode, we independently derive the same spectral function in the strongest-interference regime. Our results establish a unified framework for describing the Fano effect in single-mode cavity QED systems and clarify its non-Markovian origin encoded in the spectral function.

</details>


### [23] [Classical simulation of a quantum circuit with noisy magic inputs](https://arxiv.org/abs/2601.10111)
*Jiwon Heo,Sojeong Park,Changhun Oh*

Main category: quant-ph

TL;DR: Characterizes how noise on magic states affects classical simulability of quantum circuits, developing efficient classical sampling algorithms with explicit noise thresholds for when quantum advantage disappears.


<details>
  <summary>Details</summary>
Motivation: Magic states enable universal quantum computation but are inevitably noisy in realistic devices. Understanding how this noise affects classical simulability is crucial for determining when quantum advantage persists versus when circuits become efficiently classically simulable.

Method: Adopts a resource-centric noise model where only injected magic components are noisy while baseline operations remain efficiently simulable. Develops approximate classical sampling algorithms with controlled error and proves explicit noise-dependent conditions for polynomial-time simulation. Applies framework to both qubit circuits with Clifford baselines and fermionic circuits with matchgate baselines.

Result: Establishes explicit noise thresholds that determine when quantum circuits transition from classically intractable to efficiently simulable. Provides numerical estimates of simulation cost, concrete thresholds, and runtime scaling across relevant parameter regimes for noise channels like dephasing and particle loss.

Conclusion: Noise on magic resources fundamentally changes the classical simulability landscape, with explicit thresholds determining when quantum advantage disappears. The framework provides practical tools for assessing when noisy quantum circuits can be efficiently simulated classically, bridging resource theory with practical quantum computation.

Abstract: Magic states are essential for universal quantum computation and are widely viewed as a key source of quantum advantage, yet in realistic devices they are inevitably noisy. In this work, we characterize how noise on injected magic resources changes the classical simulability of quantum circuits and when it induces a transition from classically intractable behavior to efficient classical simulation. We adopt a resource-centric noise model in which only the injected magic components are noisy, while the baseline states, operations, and measurements belong to an efficiently simulable family. Within this setting, we develop an approximate classical sampling algorithm with controlled error and prove explicit noise-dependent conditions under which the algorithm runs in polynomial time. Our framework applies to both qubit circuits with Clifford baselines and fermionic circuits with matchgate baselines, covering representative noise channels such as dephasing and particle loss. We complement the analysis with numerical estimates of the simulation cost, providing concrete thresholds and runtime scaling across practically relevant parameter regimes.

</details>


### [24] [Casimir interactions as a probe of broadband optical response](https://arxiv.org/abs/2601.10118)
*Calum F. Shelden,Jeremy N. Munday*

Main category: quant-ph

TL;DR: Machine learning inversion of Lifshitz theory enables reconstruction of broadband optical response from Casimir force measurements, establishing Casimir interactions as a spectroscopic tool.


<details>
  <summary>Details</summary>
Motivation: The connection between Casimir forces and real-frequency optical properties has been obscured by the imaginary-frequency formulation of Lifshitz theory, limiting the use of Casimir interactions as a materials probe.

Method: Supervised machine learning is used to invert Lifshitz theory, allowing determination of complex permittivity from a single force-distance curve, with measurements at different separations constraining distinct frequency ranges.

Result: The method reconstructs a material's broadband optical response over more than seven orders of magnitude in frequency from force measurements, showing how quantum fluctuations sample different electromagnetic spectrum regions at different separations.

Conclusion: Casimir interactions can serve as a physically constrained, broadband spectroscopic tool for optical characterization in regimes inaccessible to conventional techniques.

Abstract: Casimir forces arise from quantum electromagnetic fluctuations and depend on the dielectric response of interacting materials across the entire frequency spectrum. Although this dependence is central to Lifshitz theory of the Casimir effect, the formulation of the force in terms of dielectric functions evaluated at imaginary frequencies has largely obscured its connection to real-frequency optical properties, limiting the use of Casimir interactions as a probe of materials. Here we demonstrate that Casimir force measurements encode sufficient information to reconstruct a material's broadband optical response. Using supervised machine learning to invert Lifshitz theory, we determine the complex permittivity of a material over more than seven orders of magnitude in frequency from a single force-distance curve. We show that measurements at different separations selectively constrain distinct frequency ranges of the dielectric response, providing direct physical insight into how quantum fluctuations sample the electromagnetic spectrum. These results establish Casimir interactions as a physically constrained, broadband spectroscopic tool and open new opportunities for optical characterization in regimes inaccessible to conventional techniques.

</details>


### [25] [Bridging Superconducting and Neutral-Atom Platforms for Efficient Fault-Tolerant Quantum Architectures](https://arxiv.org/abs/2601.10144)
*Xiang Fang,Jixuan Ruan,Sharanya Prabhu,Ang Li,Travis Humble,Dean Tullsen,Yufei Ding*

Main category: quant-ph

TL;DR: Heterogeneous quantum architectures combining superconducting and neutral atom platforms achieve 752× speedup over neutral atom-only systems and 10× qubit reduction versus superconducting-only systems by strategically assigning computational roles based on hardware strengths.


<details>
  <summary>Details</summary>
Motivation: Homogeneous quantum systems face limitations as no single qubit modality offers optimal operation speed, connectivity, and scalability simultaneously. The transition to fault-tolerant quantum computing requires architectures that can leverage the complementary strengths of different quantum hardware platforms.

Method: Proposes Heterogeneous Quantum Architectures (HQA) combining superconducting (SC) and neutral atom (NA) platforms with two architectural strategies: (1) MagicAcc - offloads latency-critical Magic State Factory to fast SC devices while performing computation on scalable NA arrays; (2) Memory-Compute Separation (MCSep) - uses NA arrays for high-density qLDPC memory storage and SC devices for fast surface-code processing. Evaluation uses comprehensive end-to-end cost model.

Result: Principled heterogeneity yields significant performance gains: designs achieve 752× speedup over NA-only baselines on average and reduce physical qubit footprint by over 10× compared to SC-only systems.

Conclusion: Heterogeneous quantum architectures leveraging cross-modality interconnects provide a clear pathway to optimize space-time efficiency of future fault-tolerant quantum computers by strategically combining the complementary strengths of different quantum hardware platforms.

Abstract: The transition to the fault-tolerant era exposes the limitations of homogeneous quantum systems, where no single qubit modality simultaneously offers optimal operation speed, connectivity, and scalability. In this work, we propose a strategic approach to Heterogeneous Quantum Architectures (HQA) that synthesizes the distinct advantages of the superconducting (SC) and neutral atom (NA) platforms. We explore two architectural role assignment strategies based on hardware characteristics: (1) We offload the latency-critical Magic State Factory (MSF) to fast SC devices while performing computation on scalable NA arrays, a design we term MagicAcc, which effectively mitigates the resource-preparation bottleneck. (2) We explore a Memory-Compute Separation (MCSep) paradigm that utilizes NA arrays for high-density qLDPC memory storage and SC devices for fast surface-code processing. Our evaluation, based on a comprehensive end-to-end cost model, demonstrates that principled heterogeneity yields significant performance gains. Specifically, our designs achieve $752\times$ speedup over NA-only baselines on average and reduce the physical qubit footprint by over $10\times$ compared to SC-only systems. These results chart a clear pathway for leveraging cross-modality interconnects to optimize the space-time efficiency of future fault-tolerant quantum computers.

</details>


### [26] [Fluctuation-induced quenching of chaos in quantum optics](https://arxiv.org/abs/2601.10147)
*Mei-Qi Gao,Song-hai Li,Xun Li,Xingli Li,Jiong Cheng,Wenlin Li*

Main category: quant-ph

TL;DR: Thermal fluctuations at room temperature suppress chaos in quantum optical systems, with noise threshold decreasing as nonlinearity increases, approaching vacuum fluctuation scales.


<details>
  <summary>Details</summary>
Motivation: Mean-field approximations ignore fluctuations, but chaos's sensitivity to initial conditions suggests even minute fluctuations could significantly affect chaotic dynamics, questioning the validity of these approximations in quantum optical systems.

Method: Analysis using stochastic Langevin equations and Lindblad master equation for systems operating at 10^5 to 10^7 Hz frequencies, examining effects of room-temperature thermal fluctuations on chaotic dynamics.

Result: Room-temperature thermal fluctuations suppress chaos at expectation value level even under weak nonlinearity; nonlinearity induces non-Gaussian phase-space distributions revealing attractor-like features in Wigner function; noise threshold for chaos suppression decreases with increasing nonlinearity, approaching vacuum fluctuation scales.

Conclusion: Results provide bidirectional validation of quantum mechanical suppression of chaos, demonstrating that realistic fluctuations fundamentally alter chaotic behavior predicted by mean-field approximations.

Abstract: Recent studies have extensively explored chaotic dynamics in quantum optical systems through the mean-field approximation, which corresponds to an ideal, fluctuation-free scenario. However, the inherent sensitivity of chaos to initial conditions implies that even minute fluctuations can be amplified, thereby questioning the applicability of this approximation. Here, we analyze these chaotic effects using stochastic Langevin equations or the Lindblad master equation. For systems operating at frequencies of $10^5$ to $10^7$ Hz, we demonstrate that room-temperature thermal fluctuations are sufficient to suppress chaos at the level of expectation values, even under weak nonlinearity. Furthermore, nonlinearity induces deviations from Gaussian phase-space distributions of the quantum state, revealing attractor-like features in the Wigner function. With increasing nonlinearity, the noise threshold for chaos suppression decreases, approaching the scale of vacuum fluctuations. These results provide a bidirectional validation of the quantum mechanical suppression of chaos.

</details>


### [27] [Computing Statistical Properties of Velocity Fields on Current Quantum Hardware](https://arxiv.org/abs/2601.10166)
*Miriam Goldack,Yosi Atia,Ori Alberton,Karl Jansen*

Main category: quant-ph

TL;DR: Quantum algorithms for CFD enable efficient encoding of spatial fields using qubits, with methods to extract statistical properties directly from quantum circuits without full tomography.


<details>
  <summary>Details</summary>
Motivation: Quantum CFD algorithms offer favorable scaling but face challenges in efficient readout of simulation results, which has received limited attention in literature.

Method: Develop methods to extract statistical properties (central moments, structure functions) of spatial velocity fields directly from parameterized ansatz circuits, avoiding full quantum state tomography.

Result: Implemented approach for 1D velocity fields encoding 16 spatial points with 4 qubits, analyzing sine wave and Burgers' equation snapshots using QESEM error mitigation on IBMQ Heron2 system, achieving high accuracy.

Conclusion: Demonstrated that quantum computation of statistical properties in CFD can achieve high accuracy on current quantum devices with proper error mitigation techniques.

Abstract: Quantum algorithms are gaining attention in Computational Fluid Dynamics (CFD) for their favorable scaling, as encoding physical fields into quantum probability amplitudes enables representation of two to the power of n spatial points with only n qubits. A key challenge in Quantum CFD is the efficient readout of simulation results, a topic that has received limited attention in literature. This work presents methods to extract statistical properties of spatial velocity fields, such as central moments and structure functions, directly from parameterized ansatz circuits, avoiding full quantum state tomography. As a proof of concept, we implement our approach for 1D velocity fields, encoding 16 spatial points with 4 qubits, and analyze both a sine wave signal and four snapshots from Burgers' equation evolution. Using Qedma's error mitigation software QESEM, we demonstrate that such computations achieve high accuracy on current quantum devices, specifically IBMQ's Heron2 system ibm_fez.

</details>


### [28] [Exponential Analysis for Entanglement Distillation](https://arxiv.org/abs/2601.10190)
*Zhiwen Lin,Ke Li,Kun Fang*

Main category: quant-ph

TL;DR: The paper studies the reliability function of entanglement distillation, extending from known states to black-box settings, connecting it to composite hypothesis testing and characterizing it via regularized quantum Hoeffding divergence.


<details>
  <summary>Details</summary>
Motivation: Traditional entanglement distillation focuses on distillable entanglement assuming complete knowledge of initial states. This work aims to study reliability functions (error exponents) when distillation rates are below distillable entanglement, and extends the framework to black-box settings where distillation is performed from a set of possible states for greater operational significance.

Method: Establishes exact finite blocklength results connecting to composite correlated hypothesis testing without redundant correction terms. Characterizes reliability function via regularized quantum Hoeffding divergence. Constructs concrete optimal distillation protocols for known states. Analyzes strong converse exponents. Investigates different free operation classes including PPT-preserving, dually non-entangling, and dually PPT-preserving operations.

Result: The reliability function of entanglement distillation is characterized by the regularized quantum Hoeffding divergence. For pure initial states, the result reduces to Hayashi et al.'s 2003 error exponent for entanglement concentration. Optimal distillation protocols are constructed for known states. Strong converse exponents are analyzed, and results are extended to various free operation classes.

Conclusion: The paper provides a comprehensive characterization of entanglement distillation reliability functions, bridging the gap between traditional distillable entanglement analysis and practical black-box scenarios, with connections to hypothesis testing and applications across different operation classes.

Abstract: Historically, the focus in entanglement distillation has predominantly been on the distillable entanglement, and the framework assumes complete knowledge of the initial state. In this paper, we study the reliability function of entanglement distillation, which specifies the optimal exponent of the decay of the distillation error when the distillation rate is below the distillable entanglement. Furthermore, to capture greater operational significance, we extend the framework from the standard setting of known states to a black-box setting, where distillation is performed from a set of possible states. We establish an exact finite blocklength result connecting to composite correlated hypothesis testing without any redundant correction terms. Based on this, the reliability function of entanglement distillation is characterized by the regularized quantum Hoeffding divergence. In the special case of a pure initial state, our result reduces to the error exponent for entanglement concentration derived by Hayashi et al. in 2003. Given full prior knowledge of the state, we construct a concrete optimal distillation protocol. Additionally, we analyze the strong converse exponent of entanglement distillation. While all the above results assume the free operations to be non-entangling, we also investigate other free operation classes, including PPT-preserving, dually non-entangling, and dually PPT-preserving operations.

</details>


### [29] [Autonomous Quantum Simulation through Large Language Model Agents](https://arxiv.org/abs/2601.10194)
*Weitang Li,Jiajun Ren,Lixue Cheng,Cunxi Gong*

Main category: quant-ph

TL;DR: LLM agents can autonomously perform tensor network simulations of quantum many-body systems with ~90% success rate, using in-context learning and multi-agent decomposition to overcome the expertise barrier typically requiring years of graduate training.


<details>
  <summary>Details</summary>
Motivation: Tensor network methods are powerful for quantum simulation but require specialized expertise typically acquired through years of graduate training, creating a significant barrier to their effective use.

Method: Combined in-context learning with curated documentation and multi-agent decomposition to create autonomous AI agents trainable in specialized computational domains within minutes. Benchmarked three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on quantum phase transitions, open quantum system dynamics, and photochemical reactions using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5.

Result: Achieved approximately 90% success rate across representative benchmark tasks. Both in-context learning and multi-agent architecture were essential for success. Multi-agent configuration substantially reduced implementation errors and hallucinations compared to simpler architectures, with systematic evaluation revealing characteristic failure patterns across models.

Conclusion: LLM agents can effectively perform complex tensor network simulations autonomously, with multi-agent architectures and in-context learning being critical components for reducing errors and hallucinations, potentially democratizing access to advanced quantum simulation techniques.

Abstract: We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Tensor network methods are powerful tools for quantum simulation, but their effective use requires expertise typically acquired through years of graduate training. By combining in-context learning with curated documentation and multi-agent decomposition, we create autonomous AI agents that can be trained in specialized computational domains within minutes. We benchmark three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on problems spanning quantum phase transitions, open quantum system dynamics, and photochemical reactions. Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.

</details>


### [30] [On the average-case complexity of learning states from the circular and Gaussian ensembles](https://arxiv.org/abs/2601.10197)
*Maxwell West*

Main category: quant-ph

TL;DR: The paper establishes average-case hardness of learning Born distributions from quantum states sampled from circular and Gaussian ensembles, complementing previous results for classical compact groups.


<details>
  <summary>Details</summary>
Motivation: To study the complexity of states sampled from various ensembles and establish the computational hardness of learning their Born distributions, which is central to quantum information theory.

Method: Uses the statistical query model to analyze learning hardness, employs an unconventional approach to integrating over compact groups, and focuses on states sampled uniformly from circular and fermionic Gaussian ensembles induced by compact symmetric spaces of types AI, AII, and DIII.

Result: Proves average-case hardness of learning Born distributions from these ensembles, and exactly evaluates total variation distances between output distributions of Haar random unitary/orthogonal circuits and constant distribution (previously known only approximately).

Conclusion: The work establishes computational hardness results for learning quantum states from important ensembles, provides technical integration methods for compact groups, and complements existing results for classical compact groups.

Abstract: Studying the complexity of states sampled from various ensembles is a central component of quantum information theory. In this work we establish the average-case hardness of learning, in the statistical query model, the Born distributions of states sampled uniformly from the circular and (fermionic) Gaussian ensembles. These ensembles of states are induced variously by the uniform measures on the compact symmetric spaces of type AI, AII, and DIII. This finding complements analogous recent results for states sampled from the classical compact groups. On the technical side, we employ a somewhat unconventional approach to integrating over the compact groups which may be of some independent interest. For example, our approach allows us to exactly evaluate the total variation distances between the output distributions of Haar random unitary and orthogonal circuits and the constant distribution, which were previously known only approximately.

</details>


### [31] [Topology-Aware Block Coordinate Descent for Qubit Frequency Calibration of Superconducting Quantum Processors](https://arxiv.org/abs/2601.10203)
*Zheng Zhao,Weifeng Zhuang,Yanwu Gu,Peng Qian,Xiao Xiao,Dong E. Liu*

Main category: quant-ph

TL;DR: The paper establishes that the Snake optimizer for qubit frequency calibration is equivalent to Block Coordinate Descent (BCD), formalizes the calibration objective, and proposes a topology-aware block ordering via Sequence-Dependent Traveling Salesman Problem (SD-TSP) solved with nearest-neighbor heuristic, achieving linear complexity per epoch while maintaining calibration quality.


<details>
  <summary>Details</summary>
Motivation: Pre-execution calibration, particularly qubit frequency allocation, is a major bottleneck for superconducting quantum processors due to crosstalk-coupled objectives. The widely-used Snake optimizer lacks rigorous theoretical foundation, and there's a need for scalable, efficient calibration methods for NISQ-era processors.

Method: 1) Establish mathematical equivalence between Snake optimizer and Block Coordinate Descent (BCD); 2) Formalize calibration objective and analyze when reduced experiments approximate full objective; 3) Cast block ordering as Sequence-Dependent Traveling Salesman Problem (SD-TSP) solved with nearest-neighbor heuristic; 4) Analyze convergence of inexact BCD with noisy measurements; 5) Evaluate on multi-qubit models with local crosstalk/bounded-degree assumptions.

Result: BCD-NNA ordering achieves same optimization accuracy at markedly lower runtime than graph-based heuristics (BFS, DFS) and random orders. Method achieves linear complexity in qubit count per epoch under local crosstalk assumptions. Robust to measurement noise and tolerant to moderate non-local crosstalk. Provides scalable, implementation-ready workflow for frequency calibration.

Conclusion: The paper provides rigorous theoretical foundation for Snake optimizer via BCD equivalence, introduces topology-aware block ordering via SD-TSP with nearest-neighbor heuristic, and demonstrates scalable frequency calibration workflow suitable for NISQ-era quantum processors with linear complexity per epoch and maintained calibration quality.

Abstract: Pre-execution calibration is a major bottleneck for operating superconducting quantum processors, and qubit frequency allocation is especially challenging due to crosstalk-coupled objectives. We establish that the widely-used Snake optimizer is mathematically equivalent to Block Coordinate Descent (BCD), providing a rigorous theoretical foundation for this calibration strategy. Building on this formalization, we present a topology-aware block ordering obtained by casting order selection as a Sequence-Dependent Traveling Salesman Problem (SD-TSP) and solving it efficiently with a nearest-neighbor heuristic. The SD-TSP cost reflects how a given block choice expands the reduced-circuit footprint required to evaluate the block-local objective, enabling orders that minimize per-epoch evaluation time. Under local crosstalk/bounded-degree assumptions, the method achieves linear complexity in qubit count per epoch, while retaining calibration quality. We formalize the calibration objective, clarify when reduced experiments are equivalent or approximate to the full objective, and analyze convergence of the resulting inexact BCD with noisy measurements. Simulations on multi-qubit models show that the proposed BCD-NNA ordering attains the same optimization accuracy at markedly lower runtime than graph-based heuristics (BFS, DFS) and random orders, and is robust to measurement noise and tolerant to moderate non-local crosstalk. These results provide a scalable, implementation-ready workflow for frequency calibration on NISQ-era processors.

</details>


### [32] [Noise-Resilient Quantum Evolution in Open Systems through Error-Correcting Frameworks](https://arxiv.org/abs/2601.10206)
*Nirupam Basak,Goutam Paul,Pritam Chattopadhyay*

Main category: quant-ph

TL;DR: The paper analyzes quantum error correction codes in realistic open quantum systems with bosonic thermal environments, finding the five-qubit code outperforms Steane and toric codes, especially in low-temperature regimes, and identifies critical evolution times for QEC effectiveness.


<details>
  <summary>Details</summary>
Motivation: To move beyond abstract quantum channel models and analyze quantum error correction codes in realistic microscopic system-bath environments, providing quantitative benchmarks for QEC performance under thermal noise conditions relevant to near-term quantum technologies.

Method: Embed QEC codes (five-qubit, Steane, toric) into multi-qubit registers coupled to bosonic thermal environments, derive second-order master equations for reduced dynamics, compute state fidelities as functions of coupling strength, bath temperature, and correction cycles, and analyze performance for both single logical qubits and two-qubit Werner states.

Result: Five-qubit code strongly suppresses decoherence/relaxation in low-temperature regimes; thermal excitations reduce QEC benefits in high-temperature regimes; critical evolution time exists before which QEC doesn't improve fidelity for Werner states (increasing with entanglement); five-qubit code consistently outperforms topological (toric) and concatenated (Steane) architectures.

Conclusion: Establishes quantitative framework for evaluating QEC under realistic noise, provides guidance for noise-resilient quantum architectures, and demonstrates that the smallest perfect code (five-qubit) offers superior performance in open-system settings compared to more complex topological and concatenated codes.

Abstract: We analyze quantum state preservation in open quantum systems using quantum error-correcting (QEC) codes that are explicitly embedded into microscopic system-bath models. Instead of abstract quantum channels, we consider multi-qubit registers coupled to bosonic thermal environments, derive a second-order master equation for the reduced dynamics, and use it to benchmark the five-qubit, Steane, and toric codes under local and collective noise. We compute state fidelities for logical qubits as functions of coupling strength, bath temperature, and the number of correction cycles. In the low-temperature regime, we find that repeated error-correction with the five-qubit code strongly suppresses decoherence and relaxation, while in the high-temperature regime, thermal excitations dominate the dynamics and reduce the benefit of all codes, though the five-qubit code still outperforms the Steane and toric codes. For two-qubit Werner states, we identify a critical evolution time before which QEC does not improve fidelity, and this time increases as entanglement grows. After this critical time, QEC does improve fidelity. Comparative analysis further reveals that the five-qubit code (the smallest perfect code) offers consistently higher fidelities than topological and concatenated architectures in these open-system settings. These findings establish a quantitative framework for evaluating QEC under realistic noise environments and provide guidance for developing noise-resilient quantum architectures in near-term quantum technologies.

</details>


### [33] [Coherence Limits in Interference-Based cos(2$\varphi$) Qubits](https://arxiv.org/abs/2601.10209)
*S. Messelot,A. Leblanc,J. -S. Tettekpoe,F. Lefloch,Q. Ficheux,J. Renard,É. Dumur*

Main category: quant-ph

TL;DR: Parity-protected cos(2φ) qubits face fundamental trade-off between charge and flux noise dephasing, limiting T_φ to microseconds despite millisecond T_1 lifetimes.


<details>
  <summary>Details</summary>
Motivation: To investigate coherence properties of parity-protected cos(2φ) qubits and understand practical limits of this approach for quantum computing applications.

Method: Numerical simulations examining relaxation and dephasing rates as functions of external flux and circuit parameters, analyzing various implementations (semiconducting junctions, rhombus circuits, flowermon, KITE structures) described by the same Hamiltonian as two multi-harmonic Josephson junctions in SQUID geometry.

Result: Despite parity protection suppressing single Cooper pair tunneling, fundamental trade-off exists between charge and flux noise dephasing channels. With current circuit parameters, T_1 can exceed milliseconds but T_φ remains limited to few microseconds due to either flux or charge noise.

Conclusion: Establishes practical limits on coherence of parity-protected cos(2φ) qubits and raises questions about long-term potential of this approach for quantum computing.

Abstract: We investigate the coherence properties of parity-protected $\cos(2\varphi)$ qubits based on interferences between two Josephson elements in a superconducting loop. We show that qubit implementations of a $\cos(2\varphi)$ potential using a single loop, such as those employing semiconducting junctions, rhombus circuits, flowermon and KITE structures, can be described by the same Hamiltonian as two multi-harmonic Josephson junctions in a SQUID geometry. We find that, despite the parity protection arising from the suppression of single Cooper pair tunneling, there exists a fundamental trade-off between charge and flux noise dephasing channels. Using numerical simulations, we examine how relaxation and dephasing rates depend on external flux and circuit parameters, and we identify the best compromise for maximum coherence. With currently existing circuit parameters, the qubit lifetime $T_1$ can exceed milliseconds while the dephasing time $T_\varphi$ remains limited to only a few microseconds due to either flux or charge noise. Our findings establish practical limits on the coherence of this class of qubits and raise questions about the long-term potential of this approach.

</details>


### [34] [Quantitative approach for the Dicke-Ising chain with an effective self-consistent matter Hamiltonian](https://arxiv.org/abs/2601.10210)
*J. Leibig,M. Hörmann,A. Langheld,A. Schellenberger,K. P. Schmidt*

Main category: quant-ph

TL;DR: The Dicke-Ising chain maps to an effective self-consistent matter Hamiltonian, enabling precise quantum phase diagram determination using NLCE+DMRG without needing photon-spin quantum correlations.


<details>
  <summary>Details</summary>
Motivation: To understand the quantum phase diagram of the Dicke-Ising chain in the thermodynamic limit without requiring complex photon-spin quantum correlations, and to provide more accurate determination of phase boundaries and multicritical points.

Method: Map the Dicke-Ising chain to an effective self-consistent matter Hamiltonian where photons act as an effective field, then solve using numerical linked-cluster expansions combined with density matrix renormalization group (NLCE+DMRG).

Result: Achieved significantly improved accuracy for magnetically ordered phases; refined multicritical point location for ferromagnetic couplings to 10^-4 relative accuracy; confirmed existence of narrow antiferromagnetic superradiant phase in thermodynamic limit; identified antiferromagnetic superradiant phase as ground state of antiferromagnetic transverse-field Ising model with longitudinal field.

Conclusion: The effective matter Hamiltonian framework enables precise determination of the Dicke-Ising phase diagram using NLCE+DMRG, revealing phase transitions including continuous polariton condensation and first-order transitions between different superradiant phases.

Abstract: In the thermodynamic limit, the Dicke-Ising chain maps exactly onto an effective self-consistent matter Hamiltonian with the photon field acting solely as a self-consistent effective field. As a consequence, no quantum correlations between photons and spins are needed to understand the quantum phase diagram. This enables us to determine the quantum phase diagram in the thermodynamic limit using numerical linked-cluster expansions combined with density matrix renormalization group calculations (NLCE+DMRG) to solve the resulting self-consistent matter Hamiltonian. This includes magnetically ordered phases with significantly improved accuracy compared to previous estimates. For ferromagnetic Ising couplings, we refine the location of the multicritical point governing the change in the order of the superradiant phase transition, reaching a relative accuracy of $10^{-4}$. For antiferromagnetic Ising couplings, we confirm the existence of the narrow antiferromagnetic superradiant phase in the thermodynamic limit. The effective matter Hamiltonian framework identifies the antiferromagnetic superradiant phase as the many-body ground state of an antiferromagnetic transverse-field Ising model with longitudinal field. This phase emerges through continuous Dicke-type polariton condensation from the antiferromagnetic normal phase, followed by a first-order transition to the paramagnetic superradiant phase. Thus, NLCE+DMRG provides a precise determination of the Dicke-Ising phase diagram in one dimension by solving the self-consistent effective matter Hamiltonian.

</details>


### [35] [Optimal control of a dissipative micromaser quantum battery in the ultrastrong coupling regime](https://arxiv.org/abs/2601.10281)
*Maristella Crotti,Luca Razzoli,Luigi Giannelli,Giuseppe A. Falci,Giuliano Benenti*

Main category: quant-ph

TL;DR: Micromaser quantum battery in ultrastrong coupling regime achieves enhanced charging performance and stability through optimized control strategies and controlled dissipation.


<details>
  <summary>Details</summary>
Motivation: Investigate open system dynamics of micromaser quantum batteries operating in ultrastrong coupling regime under environmental dissipation, addressing challenges of unbounded energy growth and mixed states in USC systems.

Method: Single-mode electromagnetic cavity sequentially interacts with stream of qubits via Rabi Hamiltonian; dissipative effects from weak coupling to thermal bath; employs optimal control on qubit preparation and interaction times with measurement-based passive-feedback strategy.

Result: Counter-rotating terms in USC regime improve charging speed but cause unbounded energy growth; dissipation mitigates detrimental effects yielding steady-state of finite energy and ergotropy; optimized protocols maximize stored ergotropy and stabilize against dissipative losses.

Conclusion: Interplay of ultrastrong light-matter coupling, controlled dissipation, and optimized control strategies enables micromaser quantum batteries to achieve enhanced charging performance and long-term stability under realistic conditions.

Abstract: We investigate the open system dynamics of a micromaser quantum battery operating in the ultrastrong coupling (USC) regime under environmental dissipation. The battery consists of a single-mode electromagnetic cavity sequentially interacting, via the Rabi Hamiltonian, with a stream of qubits acting as chargers. Dissipative effects arise from the weak coupling of the qubit-cavity system to a thermal bath. Non-negligible in the USC regime, the counter-rotating terms substantially improve the charging speed, but also lead, in the absence of dissipation, to unbounded energy growth and highly mixed cavity states. Dissipation during each qubit-cavity interaction mitigates these detrimental effects, yielding steady-state of finite energy and ergotropy. Optimal control on qubit preparation and interaction times enhances battery's performance in: (i) Maximizing the stored ergotropy trhough an optimized charging protocol; (ii) Stabilizing the stored ergotropy against dissipative losses through an optimized measurement-based passive-feedback strategy. Overall, our numerical results demonstrate that the interplay of ultrastrong light-matter coupling, controlled dissipation, and optimized control strategies enables micromaser quantum batteries to achieve both enhanced charging performance and long-term stability under realistic conditions.

</details>


### [36] [Exponential improvement in benchmarking multiphoton interference](https://arxiv.org/abs/2601.10289)
*Rodrigo M. Sanz,Emilio Annoni,Stephen C. Wein,Carmen G. Almudever,Shane Mansfield,Ellen Derbyshire,Rawad Mezher*

Main category: quant-ph

TL;DR: New protocol using quantum Fourier transform achieves constant sample complexity for estimating genuine n-photon indistinguishability, representing exponential improvement over previous exponential-scaling methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for benchmarking multi-photon indistinguishability have exponential sample complexity scaling with photon number, limiting scalability for photonic quantum technologies that require multiple indistinguishable photons.

Method: Introduces new theorems linking distinguishability to suppression laws of quantum Fourier transform interferometer, then proposes protocol using QFT for benchmarking genuine n-photon indistinguishability with constant sample complexity for prime photon numbers and sub-polynomial scaling otherwise.

Result: Protocol achieves constant sample complexity for prime photon numbers and sub-polynomial scaling otherwise, representing exponential improvement over state-of-the-art; validated experimentally on Quandela's photonic quantum processor showing runtime and precision advantages; optimality proven in many scenarios.

Conclusion: Establishes first scalable method for computing multi-photon indistinguishability applicable to current and near-term photonic quantum hardware, overcoming previous exponential scaling limitations.

Abstract: Several photonic quantum technologies rely on the ability to generate multiple indistinguishable photons. Benchmarking the level of indistinguishability of these photons is essential for scalability. The Hong-Ou-Mandel dip provides a benchmark for the indistinguishability between two photons, and extending this test to the multi-photon setting has so far resulted in a protocol that computes the genuine n-photon indistinguishability (GI). However, this protocol has a sample complexity that increases exponentially with the number of input photons for an estimation of GI up to a given additive error. To address this problem, we introduce new theorems that strengthen our understanding of the relationship between distinguishability and the suppression laws of the quantum Fourier transform interferometer (QFT). Building on this, we propose a protocol using the QFT for benchmarking GI that achieves constant sample complexity for the estimation of GI up to a given additive error for prime photon numbers, and sub-polynomial scaling otherwise, representing an exponential improvement over the state of the art. We prove the optimality of our protocol in many relevant scenarios and validate our approach experimentally on Quandela's reconfigurable photonic quantum processor, where we observe a clear advantage in runtime and precision over the state of the art. We therefore establish the first scalable method for computing multi-photon indistinguishability, which applies naturally to current and near-term photonic quantum hardware.

</details>


### [37] [Complex scalar relativistic field as a probability amplitude](https://arxiv.org/abs/2601.10302)
*Yu. M. Poluektov*

Main category: quant-ph

TL;DR: The paper proposes a relativistic equation for a neutral complex field as a probability amplitude, derives continuity equations, identifies two particle excitation types with positive energy and different dispersion laws, establishes conservation laws via Lagrangian formalism, and considers secondary quantization.


<details>
  <summary>Details</summary>
Motivation: To develop a relativistic framework for describing neutral complex fields as probability amplitudes, establishing proper continuity equations and conservation laws while identifying distinct particle excitations within this formalism.

Method: Proposes a relativistic equation for neutral complex field as probability amplitude, derives continuity equation for probability density, identifies two excitation types with positive energy and different dispersion laws, applies Lagrangian formalism to obtain conservation laws, and considers transition to secondary quantization.

Result: Successfully formulates relativistic equation for neutral complex probability amplitude field, obtains continuity equation for probability density, identifies two distinct particle excitation types with positive energy and different dispersion laws, derives conservation laws from Lagrangian formalism, and establishes framework for secondary quantization.

Conclusion: The paper presents a complete relativistic quantum mechanical framework for neutral complex fields as probability amplitudes, with proper continuity equations, conservation laws, and identification of distinct particle excitations, providing foundation for secondary quantization and further quantum field theory development.

Abstract: A relativistic equation for a neutral complex field as a probability amplitude is proposed. The continuity equation for the probability density is obtained. It is shown that there are two types of excitations of this field, which describe particles with positive energy and different dispersion laws. Based on the Lagrangian formalism, conservation laws are obtained. The transition to secondary quantization is considered.

</details>


### [38] [Addition to the dynamic Stark shift of the coherent population trapping resonance](https://arxiv.org/abs/2601.10319)
*Gavriil Voloshin,Konstantin Barantsev,Andrey Litvinov*

Main category: quant-ph

TL;DR: Theoretical study of light-induced shift in coherent population trapping resonance, showing an additional shift beyond conventional Stark shift due to off-resonant transitions in Λ-scheme atomic systems.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize light-induced shifts in coherent population trapping resonances, particularly those arising from off-resonant transitions, which are important for precision atomic devices like quantum frequency standards.

Method: Analytical modeling of two radiation components interacting with Λ-scheme atomic system including additional excited state level. Analysis covers both weak and strong coupling regimes with off-resonant transitions.

Result: Derived analytical expression for additional shift in weak-coupling limit, showing significant impact on resonance shape and sensitivity to laser intensities. Under strong coupling, shift deviates from linear intensity dependence.

Conclusion: The additional shift beyond conventional Stark shift offers new opportunities for controlling light shifts in precision atomic devices, potentially improving performance of quantum frequency standards.

Abstract: This paper presents a theoretical study of the light-induced shift of the coherent population trapping resonance. An analytical model is proposed that describes the interaction of two radiation components with an atomic system using a $Λ$ scheme and takes into account an additional level of excited state. Both weak and strong coupling regimes with off-resonant transitions are considered. It is shown that, in addition to the conventional dynamic Stark shift, an extra shift arises due to the distortion of the resonance line shape when bichromatic laser radiation interacts with off-resonant atomic transitions. An analytical expression for this additional shift is derived in the weak-coupling limit, and its significant impact on the resonance shape and sensitivity to the intensities of the laser field components is demonstrated. It is found that under strong coupling conditions, the additional shift can deviate substantially from a linear dependence on light intensity, suggesting new opportunities for controlling light shifts in precision atomic devices such as quantum frequency standards.

</details>


### [39] [Principles of Optics in the Fock Space: Scalable Manipulation of Giant Quantum States](https://arxiv.org/abs/2601.10325)
*Yifang Xu,Yilong Zhou,Ziyue Hua,Lida Sun,Jie Zhou,Weiting Wang,Weizhou Cai,Hongwei Huang,Lintao Xiao,Guangming Xue,Haifeng Yu,Ming Li,Chang-Ling Zou,Luyan Sun*

Main category: quant-ph

TL;DR: The paper introduces "Fock-space optics" - a framework treating photon number as a synthetic dimension to apply classical wave optics concepts to quantum state engineering in large Hilbert spaces.


<details>
  <summary>Details</summary>
Motivation: While classical optics provides scalable control over spatial/temporal degrees of freedom, quantum state engineering in Fock space has been limited to few-photon regimes due to computational and experimental challenges with large Hilbert spaces.

Method: Established Fock-space optics framework by treating photon number as a synthetic dimension, then experimentally demonstrated Fock-space analogues of optical phenomena (propagation, refraction, lensing, dispersion, interference) using a superconducting microwave resonator with up to 180 photons.

Result: Successfully demonstrated Fock-space analogues of classical optical phenomena and established fundamental correspondence between Schrödinger evolution in a single bosonic mode and classical paraxial wave propagation.

Conclusion: By mapping intuitive optical concepts onto high-dimensional quantum state engineering, this work opens a path toward scalable control of large-scale quantum systems with thousands of photons and advanced bosonic information processing.

Abstract: The manipulation of distinct degrees of freedom of photons plays a critical role in both classical and quantum information processing. While the principles of wave optics provide elegant and scalable control over classical light in spatial and temporal domains, engineering quantum states in Fock space has been largely restricted to few-photon regimes, hindered by the computational and experimental challenges of large Hilbert spaces. Here, we introduce ``Fock-space optics", establishing a conceptual framework of wave propagation in the quantum domain by treating photon number as a synthetic dimension. Using a superconducting microwave resonator, we experimentally demonstrate Fock-space analogues of optical propagation, refraction, lensing, dispersion, and interference with up to 180 photons. These results establish a fundamental correspondence between Schrödinger evolution in a single bosonic mode and classical paraxial wave propagation. By mapping intuitive optical concepts onto high-dimensional quantum state engineering, our work opens a path toward scalable control of large-scale quantum systems with thousands of photons and advanced bosonic information processing.

</details>


### [40] [Realistic prospects for testing a relativistic local quantum measurement inequality](https://arxiv.org/abs/2601.10354)
*Riccardo Falcone,Claudio Conti*

Main category: quant-ph

TL;DR: Experimental testing of a relativistic quantum measurement inequality for finite-size detectors, deriving explicit bounds for coherent states and modeling realistic photodetection scenarios.


<details>
  <summary>Details</summary>
Motivation: To investigate experimental prospects for testing a relativistic local quantum measurement inequality that quantifies the trade-off between vacuum insensitivity and responsiveness to excitations for finite-size detectors.

Method: Builds on Reeh-Schlieder approximation for coherent states to derive explicit bounds; models detection region as square prism operating over finite time window; considers normally incident single-mode coherent state; uses numerical analysis.

Result: Numerical results show expected qualitative behavior: suppressing dark counts necessarily tightens the achievable click probability, demonstrating the trade-off between vacuum insensitivity and excitation responsiveness.

Conclusion: The derived bound is practically applicable for arbitrary coherent states and connects with realistic photodetection scenarios, providing a framework for experimental testing of relativistic quantum measurement inequalities.

Abstract: We investigate the experimental prospects for testing a relativistic local quantum measurement inequality that quantifies the trade-off between vacuum insensitivity and responsiveness to excitations for finite-size detectors. Building on the Reeh--Schlieder approximation for coherent states, we derive an explicit and practically applicable bound for arbitrary coherent states. To connect with realistic photodetection scenarios, we model the detection region as a square prism operating over a finite time window and consider a normally incident single-mode coherent state. Numerical results exhibit the expected qualitative behavior: suppressing dark counts necessarily tightens the achievable click probability.

</details>


### [41] [Learning Hamiltonians in the Heisenberg limit with static single-qubit fields](https://arxiv.org/abs/2601.10380)
*Shrigyan Brahmachari,Shuchen Zhu,Iman Marvian,Yu Tong*

Main category: quant-ph

TL;DR: A protocol for Heisenberg-limited Hamiltonian learning using only static single-qubit control fields with strength independent of target precision, robust to SPAM errors.


<details>
  <summary>Details</summary>
Motivation: Existing Hamiltonian learning protocols require either noisy multi-qubit operations or single-qubit operations whose frequency/strength scales with precision, limiting applicability on near-term quantum platforms.

Method: Protocol learns quantum Hamiltonian with optimal Heisenberg-limited scaling using only single-qubit control in form of static fields with strengths independent of target precision; robust against SPAM errors.

Result: Method achieves Heisenberg-limited scaling through rigorous mathematical proof and numerical experiments; information-theoretic lower bound shows non-vanishing static field strength is necessary for Heisenberg limit without extensive discrete control operations.

Conclusion: Protocol overcomes limitations of existing methods, provides new tools for device characterization and quantum sensing on near-term quantum platforms.

Abstract: Learning the Hamiltonian governing a quantum system is a central task in quantum metrology, sensing, and device characterization. Existing Heisenberg-limited Hamiltonian learning protocols either require multi-qubit operations that are prone to noise, or single-qubit operations whose frequency or strength increases with the desired precision. These two requirements limit the applicability of Hamiltonian learning on near-term quantum platforms. We present a protocol that learns a quantum Hamiltonian with the optimal Heisenberg-limited scaling using only single-qubit control in the form of static fields with strengths that are independent of the target precision. Our protocol is robust against the state preparation and measurement (SPAM) error. By overcoming these limitations, our protocol provides new tools for device characterization and quantum sensing. We demonstrate that our method achieves the Heisenberg-limited scaling through rigorous mathematical proof and numerical experiments. We also prove an information-theoretic lower bound showing that a non-vanishing static field strength is necessary for achieving the Heisenberg limit unless one employs an extensive number of discrete control operations.

</details>


### [42] [Experimental Realization of Rabi-Driven Reset for Fast Cooling of a High-Q Cavity](https://arxiv.org/abs/2601.10385)
*Eliya Blumenthal,Natan Karaev,Shay Hacohen-Gourgy*

Main category: quant-ph

TL;DR: Demonstration of Rabi-Driven Reset (RDR) - a hardware-efficient, measurement-free cooling technique for superconducting cavity modes using dressed-state engineering to create tunable dissipation channels for fast bosonic memory reset.


<details>
  <summary>Details</summary>
Motivation: High-Q bosonic memories are essential for quantum error correction but face persistent bottlenecks in fast, high-fidelity reset due to their isolation. Existing approaches either use weak intermode cross-Kerr conversion or measurement-based sequences with substantial latency.

Method: Rabi-Driven Reset (RDR) uses a strong resonant Rabi drive on a transmon qubit combined with sideband drives on memory and readout modes detuned by the Rabi frequency. This converts the dispersive interaction into an effective Jaynes-Cummings coupling between qubit dressed states and each mode, creating a tunable dissipation channel from memory to the cold readout bath.

Result: Demonstrated RDR of a single photon with decay time of 1.2 μs (more than 200× faster than intrinsic lifetime). Reset about 30 thermal photons in about 80 μs to steady-state average photon number of n̄ = 0.045 ± 0.025.

Conclusion: RDR provides a hardware-efficient, continuous, measurement-free cooling method that overcomes limitations of existing approaches, enabling fast bosonic memory reset even in weakly coupled architectures that suppress direct mode-mode coupling.

Abstract: High-Q bosonic memories are central to hardware-efficient quantum error correction, but their isolation makes fast, high-fidelity reset a persistent bottleneck. Existing approaches either rely on weak intermode cross-Kerr conversion or on measurement-based sequences with substantial latency. Here we demonstrate a hardware-efficient Rabi-Driven Reset (RDR) that implements continuous, measurement-free cooling of a superconducting cavity mode. A strong resonant Rabi drive on a transmon, together with sideband drives on the memory and readout modes detuned by the Rabi frequency, converts the dispersive interaction into an effective Jaynes-Cummings coupling between the qubit dressed states and each mode. This realizes a tunable dissipation channel from the memory to the cold readout bath. Crucially, the engineered coupling scales with the qubit-mode dispersive interaction and the drive amplitude, rather than with the intermode cross-Kerr, enabling fast cooling even in very weakly coupled architectures that deliberately suppress direct mode-mode coupling. We demonstrate RDR of a single photon with a decay time of $1.2 μs$, more than two orders of magnitude faster than the intrinsic lifetime. Furthermore, we reset about 30 thermal photons in about $80 μs$ to a steady-state average photon number of $\bar{n} = 0.045 \pm 0.025$.

</details>


### [43] [A Collection of Pinsker-type Inequalities for Quantum Divergences](https://arxiv.org/abs/2601.10395)
*Kläre Wienecke,Gereon Koßmann,René Schwonnek*

Main category: quant-ph

TL;DR: The paper establishes lower bounds on various quantum and classical divergences in terms of trace distance, extending Pinsker's inequality to multiple divergence types and providing adaptation strategies for smoothed divergences.


<details>
  <summary>Details</summary>
Motivation: Pinsker's inequality provides a fundamental lower bound for Umegaki divergence in terms of trace distance, but similar bounds for other important quantum and classical divergences are needed for comprehensive analysis of state distinguishability and information-theoretic applications.

Method: The authors formulate corresponding estimates for a variety of divergences including f-divergences (Hellinger, χ²), Rényi divergences, and special cases (Umegaki, collision, max divergences), and develop strategies to adapt these bounds to smoothed divergences.

Result: The work establishes comprehensive lower bounds connecting various quantum and classical divergences to trace distance, extending the scope of Pinsker's inequality and providing adaptation methods for smoothed versions of these divergences.

Conclusion: The paper successfully generalizes Pinsker's inequality to multiple divergence types, providing a unified framework for bounding various quantum and classical divergences in terms of trace distance, with practical applications for smoothed divergence analysis.

Abstract: Pinsker's inequality sets a lower bound on the Umegaki divergence of two quantum states in terms of their trace distance. In this work, we formulate corresponding estimates for a variety of quantum and classical divergences including $f$-divergences like Hellinger and $χ^2$-divergences as well as Rényi divergences and special cases thereof like the Umegaki divergence, collision divergence, max divergence. We further provide a strategy on how to adapt these bounds to smoothed divergences.

</details>


### [44] [Bounding many-body properties under partial information and finite measurement statistics](https://arxiv.org/abs/2601.10408)
*Luke Mortimer,Leonardo Zambrano,Antonio Acín,Donato Farina*

Main category: quant-ph

TL;DR: Scalable certification of quantum many-body systems using moment-matrix relaxations and semidefinite programming to compute bounds from finite-shot measurements of incomplete observables.


<details>
  <summary>Details</summary>
Motivation: Bounds on quantum system properties are crucial for understanding emergent phenomena and complement estimation methods, but existing approaches need to scale to many qubits while handling experimental shot noise.

Method: Utilize moment-matrix relaxations within semidefinite programming framework to achieve scalability, adapting to system-specific knowledge (ground states, symmetries, Lindbladian steady states) while accounting for shot noise from finite measurements.

Result: Developed a scalable certification scheme that combines semidefinite programming relaxations with experimental estimations, enabling probabilistic bounds from informationally incomplete observables measured with finite shots.

Conclusion: The approach provides a practical, scalable framework for certifying quantum many-body systems in real-world experimental conditions with unavoidable shot noise, leveraging system-specific knowledge to enhance certification power.

Abstract: Calculating bounds of properties of many-body quantum systems is of paramount importance, since they guide our understanding of emergent quantum phenomena and complement the insights obtained from estimation methods. Recent semidefinite programming approaches enable probabilistic bounds from finite-shot measurements of easily accessible, yet informationally incomplete, observables. Here we render these methods scalable in the number of qubits by instead utilizing moment-matrix relaxations. After introducing the general formalism, we show how the approach can be adapted with specific knowledge of the system, such as it being the ground state of a given Hamiltonian, possessing specific symmetries or being the steady state of a given Lindbladian. Our approach defines a scalable real-world certification scheme leveraging semidefinite programming relaxations and experimental estimations which, unavoidably, contain shot noise.

</details>


### [45] [Tight bounds on recurrence time in closed quantum systems](https://arxiv.org/abs/2601.10409)
*Marcin Kotowski,Michał Oszmaniec*

Main category: quant-ph

TL;DR: The paper establishes rigorous upper bounds on quantum recurrence times, showing t_rec ≲ t_exit(ε)(1/ε)^d, and provides a partial solution for t_exit ≈ ε/√Δ(H²), demonstrating saturation for random Hamiltonians and analyzing initial state coherence effects.


<details>
  <summary>Details</summary>
Motivation: Despite the fundamental nature of quantum recurrence (Poincaré recurrence theorem), there has been a lack of rigorous quantitative understanding of recurrence times in isolated quantum systems. The authors aim to establish concrete upper bounds on recurrence times and understand the factors influencing them.

Method: The authors establish mathematical bounds on recurrence time t_rec in terms of escape time t_exit, Hilbert-space dimension d, and neighborhood size ε. They formulate the escape time problem as an inverse quantum speed limit problem and provide analytical solutions under mild assumptions. They use random Hamiltonian analysis to demonstrate bound saturation and analyze coherence effects in the Hamiltonian eigenbasis.

Result: 1) Upper bound: t_rec ≲ t_exit(ε)(1/ε)^d; 2) Partial solution for escape time: t_exit(ε) ≈ ε/√Δ(H²) where Δ(H²) is Hamiltonian variance; 3) The bound is generically saturated for random Hamiltonians; 4) Initial state coherence in H eigenbasis significantly impacts recurrence behavior.

Conclusion: The paper provides the first rigorous quantitative bounds on quantum recurrence times, connecting recurrence to escape times and Hamiltonian variance. The results establish fundamental limits on recurrence phenomena in isolated quantum systems and reveal the importance of initial state coherence in determining recurrence behavior.

Abstract: The evolution of an isolated quantum system inevitably exhibits recurrence: the state returns to the vicinity of its initial condition after finite time. Despite its fundamental nature, a rigorous quantitative understanding of recurrence has been lacking. We establish upper bounds on the recurrence time, $t_{\mathrm{rec}} \lesssim t_{\mathrm{exit}}(ε)(1/ε)^d$, where $d$ is the Hilbert-space dimension, $ε$ the neighborhood size, and $t_{\mathrm{exit}}(ε)$ the escape time from this neighborhood. For pure states evolving under a Hamiltonian $H$, estimating $t_{\mathrm{exit}}$ is equivalent to an inverse quantum speed limit problem: finding upper bounds on the time a time-evolved state $ψ_t$ needs to depart from the $ε$-vicinity of the initial state $ψ_0$. We provide a partial solution, showing that under mild assumptions $t_{\mathrm{exit}}(ε) \approx ε/\sqrt{ Δ(H^2)}$, with $Δ(H^2)$ the Hamiltonian variance in $ψ_0$. We show that our upper bound on $t_{\mathrm{rec}}$ is generically saturated for random Hamiltonians. Finally, we analyze the impact of coherence of the initial state in the eigenbasis of $H$ on recurrence behavior.

</details>


### [46] [Unifying Quantum and Classical Dynamics](https://arxiv.org/abs/2601.10423)
*Abdul Rahaman Shaikh,Tabish Qureshi*

Main category: quant-ph

TL;DR: The paper demonstrates exact equivalence between quantum and classical dynamics by showing Heisenberg equations can be cast in identical form to Newton's equations, with ħ absent, suggesting both are governed by the same equations with operators replacing classical observables.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deriving classical mechanics from quantum mechanics as a limiting case, and to explore unification of classical and quantum dynamics rather than emergence of classical behavior from quantum theory.

Method: Reformulating Heisenberg equations of motion to show they can be expressed in a form identical to Newton's equations of motion, with Planck's constant ħ absent from the formulation, thereby establishing equivalence between quantum operator dynamics and classical observable dynamics.

Result: Demonstrates that quantum and classical dynamics are governed by the same equations, with Heisenberg operators substituting classical observables, showing exact equivalence rather than emergence of classical behavior from quantum mechanics.

Conclusion: The work establishes that classical and quantum dynamics are unified through identical mathematical formulations, suggesting a deeper equivalence between the two theories rather than one emerging from the other under limiting conditions.

Abstract: Classical and quantum physics represent two distinct theories; however, quantum physics is regarded as the more fundamental of the two. It is posited that classical mechanics should arise from quantum mechanics under certain limiting conditions. Nevertheless, this remains a challenging objective. In this work, we explore the potential for unifying the dynamics of classical and quantum physics. This discussion does not suggest that classical behavior emerges from quantum mechanics; rather, it demonstrates the exact equivalence between the dynamics of quantum observables and their classical counterparts. It is shown that the Heisenberg equations of motion can be cast in a form that is identical to Newton's equations of motion, with $\hbar$ being absent from the formulation. This implies that both quantum and classical dynamics are governed by the same equations, with the Heisenberg operators substituting the classical observables.

</details>


### [47] [Reduction of thermodynamic uncertainty by a virtual qubit](https://arxiv.org/abs/2601.10429)
*Yang Li,Fu-Lin Zhang*

Main category: quant-ph

TL;DR: Quantum thermal machines with coherent coupling can violate classical thermodynamic uncertainty relations through negative quantum corrections to current fluctuations, enabling optimization beyond classical bounds.


<details>
  <summary>Details</summary>
Motivation: Quantum violations of classical thermodynamic uncertainty relations (TUR) reveal genuinely quantum thermodynamic effects essential for improving performance and enabling optimization in quantum technologies. Understanding how quantum coherence affects TUR in practical quantum thermal machines is crucial.

Method: Analysis of TUR in paradigmatic quantum thermal-machine models with coherent coupling between two energy levels forming a virtual qubit. The system maintains steady-state coherences in the virtual-qubit subspace while satisfying detailed balance without coherent coupling. The approach involves decomposing thermodynamic uncertainty into classical (diagonal) and coherent contributions.

Result: Steady-state currents and entropy production can be fully reproduced by an effective classical Markov process, but current fluctuations acquire an additional purely quantum correction from coherence. The coherent contribution becomes negative under resonant conditions and reaches its minimum at the coupling strength maximizing steady-state coherence. Optimization conditions and criteria for surpassing classical TUR bounds near the reversible limit are identified.

Conclusion: Quantum coherence enables violation of classical thermodynamic uncertainty relations in quantum thermal machines, with negative quantum corrections to current fluctuations. This provides pathways for optimizing quantum technologies beyond classical thermodynamic constraints, particularly near reversible operation limits.

Abstract: The thermodynamic uncertainty relation (TUR) imposes a fundamental constraint between current fluctuations and entropy production, providing a refined formulation of the second law for micro- and nanoscale systems. Quantum violations of the classical TUR reveal genuinely quantum thermodynamic effects, which are essential for improving performance and enabling optimization in quantum technologies. In this work, we analyze the TUR in a class of paradigmatic quantum thermal-machine models whose operation is enabled by coherent coupling between two energy levels forming a virtual qubit. Steady-state coherences are confined to this virtual-qubit subspace, while in the absence of coherent coupling the system satisfies detailed balance with the thermal reservoirs and supports no steady-state heat currents. We show that the steady-state currents and entropy production can be fully reproduced by an effective classical Markov process, whereas current fluctuations acquire an additional purely quantum correction originating from coherence. As a result, the thermodynamic uncertainty naturally decomposes into a classical (diagonal) contribution and a coherent contribution. The latter becomes negative under resonant conditions and reaches its minimum at the coupling strength that maximizes steady-state coherence. We further identify the optimization conditions and the criteria for surpassing the classical TUR bound in the vicinity of the reversible limit.

</details>


### [48] [The SpinPulse library for transpilation and noise-accurate simulation of spin qubit quantum computers](https://arxiv.org/abs/2601.10435)
*Benoît Vermersch,Oscar Gravier,Nathan Miscopein,Julia Guignon,Carlos Ramos Marimón,Jonathan Durandau,Matthieu Dartiailh,Tristan Meunier,Valentin Savin*

Main category: quant-ph

TL;DR: SpinPulse is an open-source Python package for pulse-level simulation of spin qubit quantum computers with realistic noise modeling.


<details>
  <summary>Details</summary>
Motivation: To provide realistic simulations of spin qubit quantum computers at the pulse level, including classical non-Markovian noise, to support hardware development and circuit optimization.

Method: SpinPulse transpiles quantum circuits into native gate sets, converts them to pulse sequences, and numerically integrates these sequences in simulated noisy experimental environments with classical non-Markovian noise modeling.

Result: The package enables workflows including transpilation, pulse-level compilation, hardware benchmarking, quantum error mitigation, and large-scale simulations via integration with quimb tensor-network library.

Conclusion: SpinPulse is expected to be a valuable open-source tool for the quantum computing community, fostering development of high-fidelity quantum circuits and improved error mitigation/correction strategies.

Abstract: We introduce SpinPulse, an open-source python package for simulating spin qubit-based quantum computers at the pulse-level. SpinPulse models the specific physics of spin qubits, particularly through the inclusion of classical non-Markovian noise. This enables realistic simulations of native gates and quantum circuits, in order to support hardware development. In SpinPulse, a quantum circuit is first transpiled into the native gate set of our model and then converted to a pulse sequence. This pulse sequence is subsequently integrated numerically in the presence of a simulated noisy experimental environment. We showcase workflows including transpilation, pulse-level compilation, hardware benchmarking, quantum error mitigation, and large-scale simulations via integration with the tensor-network library quimb. We expect SpinPulse to be a valuable open-source tool for the quantum computing community, fostering efforts to devise high-fidelity quantum circuits and improved strategies for quantum error mitigation and correction.

</details>


### [49] [Minimal-Energy Optimal Control of Tunable Two-Qubit Gates in Superconducting Platforms Using Continuous Dynamical Decoupling](https://arxiv.org/abs/2601.10446)
*Adonai Hilário da Silva,Octávio da Motta,Leonardo Kleber Castelano,Reginaldo de Jesus Napolitano*

Main category: quant-ph

TL;DR: A unified scheme combining continuous dynamical decoupling with variational minimal-energy optimal control for generating high-fidelity entangling gates in superconducting platforms, achieving virtually unit fidelity with noise resilience.


<details>
  <summary>Details</summary>
Motivation: To develop a practical and noise-resilient scheme for designing superconducting entangling gates that addresses residual couplings, calibration drifting, and quasistatic noise while achieving high fidelity with experimentally realistic control fields.

Method: Combines continuous dynamical decoupling (CDD) to suppress noise and stabilize the effective Hamiltonian, followed by variational geodesic optimization to calculate smooth low-energy single-qubit control functions that directly minimize gate infidelity in the stable SU(4) manifold.

Result: Achieves virtually unit fidelity for CZ, CX, and generic entangling gates with robustness under restricted single-qubit action, using experimentally realistic control fields. Demonstrates noise resilience and practical applicability.

Conclusion: CDD-enhanced variational geometric optimal control establishes a practical and noise-resilient scheme for designing superconducting entangling gates, offering high fidelity and robustness with realistic experimental constraints.

Abstract: We present a unified scheme for generating high-fidelity entangling gates in superconducting platforms by continuous dynamical decoupling (CDD) combined with variational minimal-energy optimal control. During the CDD stage, we suppress residual couplings, calibration drifting, and quasistatic noise, resulting in a stable effective Hamiltonian that preserves the designed ZZ interaction intended for producing tunable couplers. In this stable $\mathrm{SU}(4)$ manifold, we calculate smooth low-energy single-quibt control functions using a variational geodesic optimization process that directly minimizes gate infidelity. We illustrate the methodology by applying it to CZ, CX, and generic engangling gates, achieving virtually unit fidelity and robustness under restricted single-qubit action, with experimentally realistic control fields. These results establish CDD-enhanced variational geometric optimal control as a practical and noise-resilient scheme for designing superconducting entangling gates.

</details>


### [50] [Localization Landscape in Non-Hermitian and Floquet quantum systems](https://arxiv.org/abs/2601.10451)
*David Guéry-Odelin,François Impens*

Main category: quant-ph

TL;DR: Generalized localization landscape theory extends beyond static, elliptic, Hermitian systems to predict localization in non-Hermitian, Floquet, and topological systems without eigenstate computation.


<details>
  <summary>Details</summary>
Motivation: The original Filoche-Mayboroda localization landscape theory is limited to static, elliptic, and Hermitian settings. There is a need to extend this powerful geometric approach to broader physical contexts including non-Hermitian systems, Floquet (periodically driven) systems, and topological systems while preserving its interpretability.

Method: The method generalizes the localization landscape by using the positive operator H†H instead of the original elliptic operator. This allows the theory to handle non-Hermitian operators. The approach incorporates: 1) singular-value collapse to reveal spectral instabilities and skin effects, 2) Sambe formulation to capture coherent destruction of tunneling in driven systems, and 3) topological zero modes that emerge directly from the landscape structure.

Result: The generalized landscape successfully predicts localization in diverse systems: 1) Hatano-Nelson chains (non-Hermitian), 2) driven two-level systems (Floquet), and 3) driven Aubry-André-Harper models (topological). Quantitative accuracy is confirmed across these applications, demonstrating the unified predictor works for both equilibrium and driven quantum matter.

Conclusion: The proposed generalization establishes a unified predictor for localization that extends well beyond the original theory's limitations. It preserves geometric interpretability while capturing phenomena in non-Hermitian, Floquet, and topological systems without requiring eigenstate computation, providing a powerful tool for studying localization in diverse quantum systems.

Abstract: We propose a generalization of the Filoche--Mayboroda localization landscape that extends the theory well beyond the static, elliptic and Hermitian settings while preserving its geometric interpretability. Using the positive operator $H^\dagger H$, we obtain a landscape that predicts localization across non-Hermitian, Floquet, and topological systems without computing eigenstates. Singular-value collapse reveals spectral instabilities and skin effects, the Sambe formulation captures coherent destruction of tunneling, and topological zero modes emerge directly from the landscape. Applications to Hatano--Nelson chains, driven two-level systems, and driven Aubry--André--Harper models confirm quantitative accuracy, establishing a unified predictor for localization in equilibrium and driven quantum matter.

</details>


### [51] [Erasure conversion for singlet-triplet spin qubits enables high-performance shuttling-based quantum error correction](https://arxiv.org/abs/2601.10461)
*Adam Siegel,Simon Benjamin*

Main category: quant-ph

TL;DR: Singlet-triplet (dual-spin) qubits enable high-fidelity shuttling and serve as natural erasure qubits in semiconductor quantum dot devices, with a hardware-efficient leakage-detection protocol and XZZX surface code achieving significantly improved fault-tolerant quantum error correction performance.


<details>
  <summary>Details</summary>
Motivation: To establish singlet-triplet (dual-spin) qubits as optimal for high-fidelity shuttling in semiconductor quantum dot devices and develop a fault-tolerant framework that leverages their natural erasure qubit properties for improved quantum error correction.

Method: 1) Propose singlet-triplet qubits as natural erasure qubits in semiconductor architectures; 2) Introduce hardware-efficient leakage-detection protocol that automatically projects leaked qubits back to computational subspace without measurement feedback or increased classical control overhead; 3) Combine with XZZX surface code and leakage-aware decoding.

Result: Demonstrates twofold increase in error correction threshold and achieves orders-of-magnitude reductions in logical error rates compared to conventional approaches.

Conclusion: Singlet-triplet encoding provides a practical route toward high-fidelity shuttling and erasure-based fault-tolerant quantum computation in semiconductor devices, establishing dual-spin qubits as optimal for shuttling-based architectures.

Abstract: Fast and high fidelity shuttling of spin qubits has been demonstrated in semiconductor quantum dot devices. Several architectures based on shuttling have been proposed; it has been suggested that singlet-triplet (dual-spin) qubits could be optimal for the highest shuttling fidelities. Here we present a fault-tolerant framework for quantum error correction based on such dual-spin qubits, establishing them as a natural realisation of erasure qubits within semiconductor architectures. We introduce a hardware-efficient leakage-detection protocol that automatically projects leaked qubits back onto the computational subspace, without the need for measurement feedback or increased classical control overheads. When combined with the XZZX surface code and leakage-aware decoding, we demonstrate a twofold increase in the error correction threshold and achieve orders-of-magnitude reductions in logical error rates. This establishes the singlet-triplet encoding as a practical route toward high-fidelity shuttling and erasure-based, fault-tolerant quantum computation in semiconductor devices.

</details>


### [52] [Nonlinear quantum Kibble-Zurek ramps in open systems at finite temperature](https://arxiv.org/abs/2601.10465)
*Johannes N. Kriel,Emma C. King,Michael Kastner*

Main category: quant-ph

TL;DR: The paper analyzes quantum systems undergoing simultaneous temperature and Hamiltonian parameter ramps toward quantum critical points, showing these protocols can probe universality classes of equilibrium quantum phase transitions in out-of-equilibrium, finite-temperature settings.


<details>
  <summary>Details</summary>
Motivation: To develop protocols that allow probing of quantum critical exponents (ν and z) in experimentally realistic finite-temperature situations, overcoming limitations of fixed-temperature protocols that cannot access universality classes of zero-temperature quantum phase transitions.

Method: Using an open-system version of a Kitaev quantum wire as an example, analyzing simultaneous nonlinear ramps of temperature and Hamiltonian control parameters toward quantum critical points, identifying protocols where both coherent and incoherent dynamics significantly affect excitation density.

Result: These protocols enable probing of universality classes in out-of-equilibrium, finite-temperature settings, with identification of specific ramps that suppress subleading corrections to asymptotic scaling laws, providing guidance for experimental dynamic probing of quantum critical exponents.

Conclusion: Simultaneous temperature and parameter ramps offer a powerful approach to dynamically probe quantum critical exponents in realistic experimental conditions, bridging equilibrium quantum phase transitions with out-of-equilibrium finite-temperature scenarios.

Abstract: We analyze quantum systems under a broad class of protocols in which the temperature and a Hamiltonian control parameter are ramped simultaneously and, in general, in a nonlinear fashion toward a quantum critical point. Using an open-system version of a Kitaev quantum wire as an example, we show that, unlike finite-temperature protocols at fixed temperature, these protocols allow us to probe, in an out-of-equilibrium situation and at finite temperature, the universality class (characterized by the critical exponents $ν$ and $z$) of an equilibrium quantum phase transition at zero temperature. Key to this is the identification of ramps in which both coherent and incoherent parts of the open-system dynamics affect the excitation density in a non-negligible way. We also identify the specific ramps for which subleading corrections to the asymptotic scaling laws are suppressed, which serves as a guide to dynamically probing quantum critical exponents in experimentally realistic finite-temperature situations.

</details>


### [53] [Analysis and Experimental Demonstration of Amplitude Amplification for Combinatorial Optimization](https://arxiv.org/abs/2601.10473)
*Daniel Koch,Brian Pardo,Kip Nieman*

Main category: quant-ph

TL;DR: The paper extends Quantum Amplitude Amplification (QAA) beyond Grover's algorithm to handle oracles encoding cost functions like QUBO, develops exact formulas for linear cost functions, demonstrates performance via simulations up to 40 qubits, and validates experimentally on IBMQ and IonQ quantum hardware.


<details>
  <summary>Details</summary>
Motivation: To extend Quantum Amplitude Amplification (QAA) beyond the conventional 2-dimensional Grover representation to handle oracles encoding combinatorial optimization cost functions like QUBO, enabling optimal solutions with high probabilities for practical optimization problems.

Method: Extends the 2-dimensional orthogonal collective states representation of Grover's algorithm to oracles encoding cost functions; develops exact formulas for determining optimal oracle parameter settings for linear cost functions; performs simulations up to 40 qubits to analyze algorithmic performance across all solutions; conducts experimental demonstrations on IBMQ (superconducting) and IonQ (trapped ion) quantum hardware.

Result: Shows that linear cost functions allow exact formulas for optimal oracle parameter settings; simulations demonstrate QAA's performance across all possible solutions with emphasis on near-optimal solutions; experimental results on IBMQ and IonQ hardware show observed probabilities match theoretical equations as a function of varying oracle and diffusion operator parameters.

Conclusion: Successfully extends QAA to handle cost-function-encoding oracles, provides exact parameter formulas for linear cases, validates performance through extensive simulations, and demonstrates practical implementation on current quantum hardware with matching experimental results.

Abstract: Quantum Amplitude Amplification (QAA), the generalization of Grover's algorithm, is capable of yielding optimal solutions to combinatorial optimization problems with high probabilities. In this work we extend the conventional 2-dimensional representation of Grover's (orthogonal collective states) to oracles which encode cost functions such as QUBO, and show that linear cost functions are a special case whereby an exact formula exists for determining optimal oracle parameter settings. Using simulations of problem sizes up to 40 qubits we demonstrate QAA's algorithmic performance across all possible solutions, with an emphasis on the closeness in Grover-like performance for solutions near the global optimum. We conclude with experimental demonstrations of generalized QAA on both IBMQ (superconducting) and IonQ (trapped ion) qubits, showing that the observed probabilities of each basis state match our equations as a function of varying the free parameters in the oracle and diffusion operators.

</details>


### [54] [H-EFT-VA: An Effective-Field-Theory Variational Ansatz with Provable Barren Plateau Avoidance](https://arxiv.org/abs/2601.10479)
*Eyad I. B Hamid*

Main category: quant-ph

TL;DR: H-EFT-VA ansatz prevents barren plateaus via hierarchical initialization inspired by effective field theory, maintaining volume-law entanglement while guaranteeing polynomial gradient variance lower bounds.


<details>
  <summary>Details</summary>
Motivation: Barren plateaus critically threaten variational quantum algorithms by causing exponentially vanishing gradients, making optimization infeasible for large systems. Existing approaches that avoid barren plateaus often sacrifice entanglement or expressibility.

Method: H-EFT-VA (H-EFT Variational Ansatz) uses effective field theory-inspired hierarchical initialization with a "UV-cutoff" that restricts state exploration to prevent formation of approximate unitary 2-designs, theoretically guaranteeing polynomial gradient variance.

Result: Theoretical proof shows inverse-polynomial lower bound on gradient variance: Var[∂θ] ∈ Ω(1/poly(N)). Extensive benchmarking across 16 experiments shows 109x improvement in energy convergence and 10.7x increase in ground-state fidelity over standard HEA with p < 10^{-88} significance.

Conclusion: H-EFT-VA successfully prevents barren plateaus while maintaining volume-law entanglement and near-Haar purity, offering a promising approach for scalable variational quantum algorithms without sacrificing expressibility.

Abstract: Variational Quantum Algorithms (VQAs) are critically threatened by the Barren Plateau (BP) phenomenon. In this work, we introduce the H-EFT Variational Ansatz (H-EFT-VA), an architecture inspired by Effective Field Theory (EFT). By enforcing a hierarchical "UV-cutoff" on initialization, we theoretically restrict the circuit's state exploration, preventing the formation of approximate unitary 2-designs. We provide a rigorous proof that this localization guarantees an inverse-polynomial lower bound on the gradient variance: $Var[\partial θ] \in Ω(1/poly(N))$. Crucially, unlike approaches that avoid BPs by limiting entanglement, we demonstrate that H-EFT-VA maintains volume-law entanglement and near-Haar purity, ensuring sufficient expressibility for complex quantum states. Extensive benchmarking across 16 experiments -- including Transverse Field Ising and Heisenberg XXZ models -- confirms a 109x improvement in energy convergence and a 10.7x increase in ground-state fidelity over standard Hardware-Efficient Ansatze (HEA), with a statistical significance of $p < 10^{-88}$.

</details>


### [55] [Optimized readout strategies for neutral atom quantum processors](https://arxiv.org/abs/2601.10492)
*Liang Chen,Wen-Yi Zhu,Zi-Jie Chen,Zhu-Bo Wang,Ya-Dong Hu,Qing-Xuan Jie,Guang-Can Guo,Chang-Ling Zou*

Main category: quant-ph

TL;DR: Theoretical framework for optimizing readout strategies in neutral atom quantum processors by balancing fidelity and retention to maximize quantum circuit iteration rate.


<details>
  <summary>Details</summary>
Motivation: Neutral atom quantum processors offer scalability but face challenges in efficiently extracting readout outcomes while maintaining high system throughput for practical applications.

Method: Developed theoretical framework quantifying trade-off between readout fidelity and atomic retention, introduced quantum circuit iteration rate (qCIR) metric, used normalized quantum Fisher information to characterize performance, and demonstrated optimized readout strategy balancing fidelity and retention.

Result: Achievable qCIRs of 197.2Hz with single photon detectors and 154.5Hz with cameras using experimentally feasible parameters for 87Rb atoms.

Conclusion: Provides practical guidance for constructing scalable, high-throughput neutral atom quantum processors for sensing, simulation, and near-term algorithm implementation.

Abstract: Neutral atom quantum processors have emerged as a promising platform for scalable quantum information processing, offering high-fidelity operations and exceptional qubit scalability. A key challenge in realizing practical applications is efficiently extracting readout outcomes while maintaining high system throughput, i.e., the rate of quantum task executions. In this work, we develop a theoretical framework to quantify the trade-off between readout fidelity and atomic retention. Moreover, we introduce a metric of quantum circuit iteration rate (qCIR) and employ normalized quantum Fisher information to characterize system overall performance. Further, by carefully balancing fidelity and retention, we demonstrate a readout strategy for optimizing information acquisition efficiency. Considering the experimentally feasible parameters for 87Rb atoms, we demonstrate that qCIRs of 197.2Hz and 154.5Hz are achievable using single photon detectors and cameras, respectively. These results provide practical guidance for constructing scalable and high-throughput neutral atom quantum processors for applications in sensing, simulation, and near-term algorithm implementation.

</details>


### [56] [A Mirror-Descent Algorithm for Computing the Petz-Rényi Capacity of Classical-Quantum Channels](https://arxiv.org/abs/2601.10558)
*Yu-Hong Lai,Hao-Chung Cheng*

Main category: quant-ph

TL;DR: Proposes exponentiated-gradient algorithm for computing α-Rényi capacity of classical-quantum channels, with global sublinear and local linear convergence guarantees under certain conditions.


<details>
  <summary>Details</summary>
Motivation: The α-Rényi capacity of classical-quantum channels is important in quantum information theory but lacks efficient computational algorithms. The Blahut-Arimoto algorithm for classical channels doesn't directly extend to quantum settings, motivating a new optimization approach.

Method: Proposes an exponentiated-gradient (mirror descent) iteration that generalizes the Blahut-Arimoto algorithm. Uses entropy geometry and relative smoothness analysis. Analyzes convergence under tangent-space nondegeneracy conditions and spectral bounds.

Result: Establishes global sublinear convergence of objective values via relative smoothness. Proves local linear (geometric) convergence in Kullback-Leibler divergence on truncated probability simplex under nondegeneracy conditions, with explicit contraction factor.

Conclusion: The proposed algorithm provides an efficient computational method for α-Rényi capacity of classical-quantum channels with rigorous convergence guarantees, extending classical optimization techniques to quantum information settings.

Abstract: We study the computation of the $α$-Rényi capacity of a classical-quantum (c-q) channel for $α\in(0,1)$. We propose an exponentiated-gradient (mirror descent) iteration that generalizes the Blahut-Arimoto algorithm. Our analysis establishes relative smoothness with respect to the entropy geometry, guaranteeing a global sublinear convergence of the objective values. Furthermore, under a natural tangent-space nondegeneracy condition (and a mild spectral lower bound in one regime), we prove local linear (geometric) convergence in Kullback-Leibler divergence on a truncated probability simplex, with an explicit contraction factor once the local curvature constants are bounded.

</details>


### [57] [Deterministic and scalable generation of large Fock states](https://arxiv.org/abs/2601.10559)
*Mo Xiong,Jize Han,Chuanzhen Cao,Jinbin Li,Qi Liu,Zhiguo Huang,Ming Xue*

Main category: quant-ph

TL;DR: Scalable protocol generates large Fock states (up to ~100 photons) with >0.9 fidelity using hybrid Genetic-Adam optimization of native control operations (Jaynes-Cummings interactions and displacements).


<details>
  <summary>Details</summary>
Motivation: Large Fock-number states are crucial for quantum metrology, communication, and simulation, but generating them with high fidelity at scale remains challenging despite small-scale progress.

Method: Hybrid Genetic-Adam optimization framework combines global search (genetic algorithms) with adaptive convergence (Adam) to optimize multi-pulse control sequences using native operations: Jaynes-Cummings interactions and displacement operations, optionally enhanced by post-selection.

Result: Achieves fidelities exceeding 0.9 for Fock states up to photon numbers on the order of 100, with shallow circuit depths and strong robustness against parameter variations.

Conclusion: Establishes efficient and scalable pathway for high-fidelity non-classical state generation, enabling applications in precision metrology and fault-tolerant quantum technologies.

Abstract: The scalable and deterministic preparation of large Fock-number states represents a long-standing frontier in quantum science, with direct implications for quantum metrology, communication, and simulation. Despite significant progress in small-scale implementations, extending such state generation to large excitation numbers while maintaining high fidelity remains a formidable challenge. Here, we present a scalable protocol for generating large Fock states with fidelities exceeding 0.9 up to photon numbers on the order of 100, achieved using only native control operations and, when desired, further enhanced by an optional post-selection step. Our method employs a hybrid Genetic-Adam optimization framework that combines the global search efficiency of genetic algorithms with the adaptive convergence of Adam to optimize multi-pulse control sequences comprising Jaynes-Cummings interactions and displacement operations, both of which are native to leading experimental platforms. The resulting control protocols achieve high fidelities with shallow circuit depths and strong robustness against parameter variations. These results establish an efficient and scalable pathway toward high-fidelity non-classical state generation for precision metrology and fault-tolerant quantum technologies.

</details>


### [58] [Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders](https://arxiv.org/abs/2601.10588)
*I. K. Kominis,C. Xie,S. Li,M. Skotiniotis,G. P. Tsironis*

Main category: quant-ph

TL;DR: Proposes an information-theoretic test for nonclassicality in neural information processing using autoencoders and Bell-type consistency tests in latent space.


<details>
  <summary>Details</summary>
Motivation: To determine whether neural information processing involves quantum-mechanical elements, bypassing microscopic assumptions and focusing on the structure of neural representations.

Method: Uses autoencoders as a transparent model system and introduces a Bell-type consistency test in latent space to examine whether decoding statistics under multiple readout contexts can be explained by a single positive latent-variable distribution.

Result: The paper proposes a model-agnostic, information-theoretic framework that shifts the search for quantum-like signatures from microscopic dynamics to experimentally testable constraints on information processing.

Conclusion: This work opens a new route for probing the fundamental physics of neural computation by focusing on information processing constraints rather than microscopic assumptions.

Abstract: Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts can be jointly explained by a single positive latent-variable distribution. By shifting the search for quantum-like signatures in neural systems from microscopic dynamics to experimentally testable constraints on information processing, this work opens a new route for probing the fundamental physics of neural computation.

</details>


### [59] [Quantum solver for single-impurity Anderson models with particle-hole symmetry](https://arxiv.org/abs/2601.10594)
*Mariia Karabin,Tanvir Sohail,Dmytro Bykov,Eduardo Antonio Coello Pérez,Swarnava Ghosh,Murali Gopalakrishnan Meena,Seongmin Kim,Amir Shehata,In-Saeng Suh,Hanna Terletska,Markus Eisenbach*

Main category: quant-ph

TL;DR: Quantum-classical hybrid solver for DMFT using VQE to solve Anderson impurity model, enabling Green's function reconstruction on near-term quantum devices.


<details>
  <summary>Details</summary>
Motivation: Solving the Anderson impurity model (AIM) is computationally expensive for large bath sizes in DMFT, and quantum computers offer potential advantages for this strongly correlated problem.

Method: VQE with shallow quantum circuits to prepare AIM ground state, unified ansatz for particle/hole excitations via parameter-shifted circuits, continued-fraction expansion for Green's function reconstruction, comparison of optimization routines (COBYLA, Adam, L-BFGS-B), quantum-computed moment correction.

Result: Feasibility demonstration of Green's function reconstruction under noisy, shot-limited conditions, comparison of optimization methods, benefits of QCM correction, benchmark against classical pipeline for density of states.

Conclusion: Quantum impurity solvers are feasible for near-term devices and establish practical benchmarks for embedding within self-consistent DMFT loops.

Abstract: Quantum embedding methods, such as dynamical mean-field theory (DMFT), provide a powerful framework for investigating strongly correlated materials. A central computational bottleneck in DMFT is in solving the Anderson impurity model (AIM), whose exact solution is classically intractable for large bath sizes. In this work, we develop and benchmark a quantum-classical hybrid solver tailored for DMFT applications, using the variational quantum eigensolver (VQE) to prepare the ground state of the AIM with shallow quantum circuits. The solver uses a unified ansatz framework to prepare the particle and hole excitations of the ground-state from parameter-shifted circuits, enabling the reconstruction of the impurity Green's function through a continued-fraction expansion. We evaluate the performance of this approach across a few bath sizes and interaction strengths under noisy, shot-limited conditions. We compare three optimization routines (COBYLA, Adam, and L-BFGS-B) in terms of convergence and fidelity, assess the benefits of estimating a quantum-computed moment (QCM) correction to the variational energies, and benchmark the approach by comparing the reconstructed density of states (DOS) against that obtained using a classical pipeline. Our results demonstrate the feasibility of Green's function reconstruction on near-term devices and establish practical benchmarks for quantum impurity solvers embedded within self-consistent DMFT loops.

</details>


### [60] [Quantifying the properties of evolutionary quantum states of the XXZ spin model using quantum computing](https://arxiv.org/abs/2601.10650)
*M. P. Tonne,Kh. P. Gnatenko*

Main category: quant-ph

TL;DR: Study of entanglement distance and evolution speed in a two-spin XXZ system using analytical methods and quantum computing, showing agreement between theory and quantum computations.


<details>
  <summary>Details</summary>
Motivation: To investigate quantum entanglement dynamics and evolution speed in a two-spin XXZ model system, bridging theoretical predictions with quantum computing implementations.

Method: Combined analytical approach and quantum computing simulations of a two-spin system with XXZ Hamiltonian, analyzing entanglement distance and evolution speed as functions of coupling constants and initial state parameters.

Result: Obtained explicit analytical dependencies of entanglement distance and evolution speed on coupling constants and initial state parameters, with quantum computing results showing good agreement with theoretical predictions.

Conclusion: The study successfully demonstrates the relationship between entanglement dynamics, evolution speed, and system parameters in a two-spin XXZ model, validating theoretical models through quantum computing experiments.

Abstract: The entanglement distance of evolutionary quantum states of a two-spin system with the XXZ model has been studied. The analysis has been conducted both analytically and using quantum computing. An analytical dependence of the entanglement distance on the values of the model coupling constants and the parameters of the initial states has been obtained. The speed of evolution of a two-spin system has been investigated. The analysis has been performed analytically and using quantum computing. An explicit dependence of the speed of evolution on the coupling constants and on the parameters of the initial state has been obtained. The results of quantum computations are in good agreement with the theoretical predictions.

</details>


### [61] [Symmetry-based Perspectives on Hamiltonian Quantum Search Algorithms and Schrodinger's Dynamics between Orthogonal States](https://arxiv.org/abs/2601.10655)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: Constant Hamiltonian quantum search between orthogonal states is time-optimal in 2D subspace; breaking optimality requires time-dependent Hamiltonians or higher-dimensional subspaces.


<details>
  <summary>Details</summary>
Motivation: To understand why continuous-time Grover's search fails when source and target states are orthogonal, and to investigate fundamental limitations on time-optimal evolution between orthogonal states with constant Hamiltonians.

Method: Employ normalization, orthogonality, and energy constraints to analyze quantum evolution between orthogonal states. Use quantitative analysis of time-optimal unitary evolutions with constant Hamiltonians confined to the 2D subspace spanned by initial and final states.

Result: It's impossible to break time-optimality between orthogonal states with constant Hamiltonians when evolution is limited to the 2D subspace. Time-optimality deviations require either time-dependent Hamiltonians or constant Hamiltonians in higher-dimensional subspaces.

Conclusion: The failure of analog quantum search with orthogonal states and constant Hamiltonians is fundamentally linked to inherent system symmetry, explaining why time-optimal evolution constraints apply to both scenarios.

Abstract: It is known that the continuous-time variant of Grover's search algorithm is characterized by quantum search frameworks that are governed by stationary Hamiltonians, which result in search trajectories confined to the two-dimensional subspace of the complete Hilbert space formed by the source and target states. Specifically, the search approach is ineffective when the source and target states are orthogonal. In this paper, we employ normalization, orthogonality, and energy limitations to demonstrate that it is unfeasible to breach time-optimality between orthogonal states with constant Hamiltonians when the evolution is limited to the two-dimensional space spanned by the initial and final states. Deviations from time-optimality for unitary evolutions between orthogonal states can only occur with time-dependent Hamiltonian evolutions or, alternatively, with constant Hamiltonian evolutions in higher-dimensional subspaces of the entire Hilbert space. Ultimately, we employ our quantitative analysis to provide meaningful insights regarding the relationship between time-optimal evolutions and analog quantum search methods. We determine that the challenge of transitioning between orthogonal states with a constant Hamiltonian in a sub-optimal time is closely linked to the shortcomings of analog quantum search when the source and target states are orthogonal and not interconnected by the search Hamiltonian. In both scenarios, the fundamental cause of the failure lies in the existence of an inherent symmetry within the system.

</details>


### [62] [Counterdiabatic driving for random-gap Landau-Zener transitions](https://arxiv.org/abs/2601.10659)
*Georgios Theologou,Mikkel F. Andersen,Sandro Wimberger*

Main category: quant-ph

TL;DR: Researchers develop a single control field to drive an ensemble of Landau-Zener systems with varying energy gaps, minimizing average transition probability through statistical optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of controlling multiple quantum systems with different parameters simultaneously, specifically an ensemble of Landau-Zener-type Hamiltonians with distributed energy gaps, where traditional counterdiabatic driving requires system-specific control fields.

Method: Construct a single control field H₁ that works statistically for the ensemble, restricting to a special class motivated by counterdiabatic driving H_CD. Use analytical treatment for limiting cases with linear sweeps (including Dirac δ(t) function) and comprehensive numerical simulations to validate and extend results.

Result: Found systematic trade-off between instantaneous adiabaticity and final transition probability. The control field minimizes average transition probability across the ensemble, with analytical solutions for special cases and numerical confirmation of the approach's effectiveness.

Conclusion: A single statistically optimized control field can effectively drive ensembles of quantum systems with parameter variations, revealing fundamental trade-offs in quantum control that balance instantaneous adiabaticity against final transition probabilities.

Abstract: The Landau--Zener (LZ) model describes a two-level quantum system that undergoes an avoided crossing. In the adiabatic limit, the transition probability vanishes. An auxiliary control field $H_\text{CD}$ can be reverse-engineered so that the full Hamiltonian $H_0 + H_\text{CD}$ reproduces adiabaticity for all parameter values. Our aim is to construct a single control field $H_1$ that drives an ensemble of LZ-type Hamiltonians with a distribution of energy gaps. $H_1$ works best statistically, minimizing the average transition probability. We restrict our attention to a special class of $H_1$ controls, motivated by $H_\text{CD}$. We found a systematic trade-off between instantaneous adiabaticity and the final transition probability. Certain limiting cases with a linear sweep can be treated analytically; one of them being the LZ system with Dirac $δ(t)$ function. Comprehensive and systematic numerical simulations support and extend the analytic results.

</details>


### [63] [Geometric Aspects of Entanglement Generating Hamiltonian Evolutions](https://arxiv.org/abs/2601.10662)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: Geometric analysis of entanglement evolution from separable to maximally entangled two-qubit states reveals time-optimal trajectories have high geodesic efficiency, zero curvature, no energy waste, and lower average path entanglement than suboptimal evolutions, with nonlocality patterns differing between orthogonal and nonorthogonal state transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric characteristics of entanglement evolution in two-qubit systems under stationary Hamiltonian dynamics, specifically examining how time-optimal versus time-suboptimal trajectories differ in terms of geometric efficiency, energy utilization, curvature, and entanglement metrics during transitions from separable to maximally entangled states.

Method: Analysis of stationary Hamiltonian evolutions using geometric metrics (geodesic efficiency, speed efficiency, curvature coefficient) and entanglement metrics (concurrence, entanglement power, entangling capability) to characterize transitions from separable to maximally entangled two-qubit states, comparing time-optimal versus time-suboptimal trajectories for both orthogonal and nonorthogonal state pairs.

Result: Time-optimal evolutions show high geodesic efficiency, zero curvature, no energy waste, and lower average path entanglement than suboptimal evolutions. For nonorthogonal states, time-optimal evolutions exhibit greater short-time nonlocality than suboptimal ones, while for orthogonal states, the reverse is generally true due to suboptimal trajectories having longer path lengths with smaller curvature and higher energy waste.

Conclusion: The geometric approach reveals fundamental differences between optimal and suboptimal entanglement generation: time-optimal trajectories are characterized by straight paths (zero curvature) with efficient energy use, while higher initial nonlocality in unitary propagators appears essential for achieving maximal entanglement from separable states, with distinct patterns emerging for orthogonal versus nonorthogonal state transitions.

Abstract: We examine the pertinent geometric characteristics of entanglement that arise from stationary Hamiltonian evolutions transitioning from separable to maximally entangled two-qubit quantum states. From a geometric perspective, each evolution is characterized by means of geodesic efficiency, speed efficiency, and curvature coefficient. Conversely, from the standpoint of entanglement, these evolutions are quantified using various metrics, such as concurrence, entanglement power, and entangling capability. Overall, our findings indicate that time-optimal evolution trajectories are marked by high geodesic efficiency, with no energy resource wastage, no curvature (i.e., zero bending), and an average path entanglement that is less than that observed in time-suboptimal evolutions. Additionally, when analyzing separable-to-maximally entangled evolutions between nonorthogonal states, time-optimal evolutions demonstrate a greater short-time degree of nonlocality compared to time-suboptimal evolutions between the same initial and final states. Interestingly, the reverse is generally true for separable-to-maximally entangled evolutions involving orthogonal states. Our investigation suggests that this phenomenon arises because suboptimal trajectories between orthogonal states are characterized by longer path lengths with smaller curvature, which are traversed with a higher energy resource wastage compared to suboptimal trajectories between nonorthogonal states. Consequently, a higher initial degree of nonlocality in the unitary time propagators appears to be essential for achieving the maximally entangled state from a separable state. Furthermore, when assessing optimal and suboptimal evolutions...

</details>


### [64] [Efficiency, Curvature, and Complexity of Quantum Evolutions for Qubits in Nonstationary Magnetic Fields](https://arxiv.org/abs/2601.10672)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: The paper provides exact analytical expressions for curvature in quantum evolutions of two-level systems under time-dependent magnetic fields, finding that efficient evolutions generally have lower complexity but complexity depends on curvature, not just path length.


<details>
  <summary>Details</summary>
Motivation: Realistic quantum evolutions often deviate from ideal optimal paths, exhibiting suboptimal efficiency, nonzero curvature, and high complexity. The paper aims to quantify these deviations by deriving exact curvature expressions for two-level systems to better understand the relationship between curvature, efficiency, and complexity in quantum dynamics.

Method: The authors derive exact analytical expressions for curvature in quantum evolutions of two-level systems subjected to various time-dependent magnetic fields. They examine dynamics produced by a two-parameter nonstationary Hermitian Hamiltonian with unit speed efficiency. They analyze curvature behavior in relation to geodesic efficiency, speed efficiency, and complexity (measured as the ratio of difference between accessible and accessed Bloch-sphere volumes to accessible volume).

Result: Efficient quantum evolutions generally exhibit lower complexity compared to inefficient ones. However, complexity is not solely determined by path length - longer paths with sufficient curvature can demonstrate lower complexity than shorter paths with lower curvature coefficients. The exact curvature expressions provide quantitative tools for analyzing these relationships.

Conclusion: The derived curvature expressions provide a framework for understanding the interplay between curvature, efficiency, and complexity in quantum evolutions. Complexity transcends mere path length and depends crucially on curvature, with implications for designing optimal quantum control protocols and understanding fundamental limits in quantum dynamics.

Abstract: In optimal quantum-mechanical evolutions, motion can take place along paths of minimal length within an optimal time frame. Alternatively, optimal evolutions may occur along established paths without any waste of energy resources and achieving 100% speed efficiency. Unfortunately, realistic physical scenarios often lead to less-than-ideal evolutions that demonstrate suboptimal efficiency, nonzero curvature, and a high level of complexity. In this paper, we provide an exact analytical expression for the curvature of a quantum evolution pertaining to a two-level quantum system subjected to various time-dependent magnetic fields. Specifically, we examine the dynamics produced by a two-parameter nonstationary Hermitian Hamiltonian with unit speed efficiency. To enhance our understanding of the physical implications of the curvature coefficient, we analyze the curvature behavior in relation to geodesic efficiency, speed efficiency, and the complexity of the quantum evolution (as described by the ratio of the difference between accessible and accessed Bloch-sphere volumes for the evolution from initial to final state to the accessible volume for the given quantum evolution). Our findings indicate that, generally, efficient quantum evolutions exhibit lower complexity compared to inefficient ones. However, we also note that complexity transcends mere length. In fact, longer paths that are sufficiently curved can demonstrate a complexity that is less than that of shorter paths with a lower curvature coefficient.

</details>


### [65] [Optimal lower bound for quantum channel tomography in away-from-boundary regime](https://arxiv.org/abs/2601.10683)
*Kean Chen,Zhicheng Zhang,Nengkun Yu*

Main category: quant-ph

TL;DR: Optimal quantum channel tomography lower bound: Ω(rd₁d₂/ε²) in away-from-boundary regime (rd₂ ≥ 2d₁), matching existing upper bound and settling query complexity for equal dimensions with r ≥ 2.


<details>
  <summary>Details</summary>
Motivation: To establish fundamental limits on quantum channel tomography, particularly understanding how query complexity scales with channel parameters (input dimension d₁, output dimension d₂, Kraus rank r) and diamond norm error ε, and to contrast with the unitary case where Heisenberg scaling is achievable.

Method: The paper proves a query lower bound using information-theoretic techniques, likely involving minimax lower bounds via Fano's method or Le Cam's method, analyzing the parameter regime where rd₂ ≥ 2d₁ (away-from-boundary regime). The proof contrasts with the boundary regime rd₂ = d₁ where different scaling may occur.

Result: Shows optimal query lower bound Ω(rd₁d₂/ε²) for quantum channel tomography to within diamond norm error ε in the away-from-boundary regime rd₂ ≥ 2d₁, matching existing upper bound O(rd₁d₂/ε²). This settles query complexity for equal dimensions d₁ = d₂ = d with r ≥ 2, contrasting sharply with unitary case r = 1 where Θ(d²/ε) scaling is achievable.

Conclusion: The paper establishes the fundamental query complexity of quantum channel tomography in the away-from-boundary regime, showing that the standard linear scaling in rd₁d₂/ε² is optimal and cannot be improved, unlike the unitary case which admits Heisenberg scaling. This provides a complete characterization for practical scenarios where r ≥ 2 and dimensions are equal.

Abstract: Consider quantum channels with input dimension $d_1$, output dimension $d_2$ and Kraus rank at most $r$. Any such channel must satisfy the constraint $rd_2\geq d_1$, and the parameter regime $rd_2=d_1$ is called the boundary regime. In this paper, we show an optimal query lower bound $Ω(rd_1d_2/\varepsilon^2)$ for quantum channel tomography to within diamond norm error $\varepsilon$ in the away-from-boundary regime $rd_2\geq 2d_1$, matching the existing upper bound $O(rd_1d_2/\varepsilon^2)$. In particular, this lower bound fully settles the query complexity for the commonly studied case of equal input and output dimensions $d_1=d_2=d$ with $r\geq 2$, in sharp contrast to the unitary case $r=1$ where Heisenberg scaling $Θ(d^2/\varepsilon)$ is achievable.

</details>


### [66] [Mitigating nonlinear transduction noise in high-cooperativity cavity optomechanics](https://arxiv.org/abs/2601.10689)
*Daniel Allepuz-Requena,Zohran Ali,Dennis Høj,Yingxuan Chen,Luiz Couto Correa Pinto Filho,Alexander Huck,Ulrik L. Andersen*

Main category: quant-ph

TL;DR: A nonlinear transform method removes all orders of thermal intermodulation noise in high-cooperativity room-temperature optomechanical systems, improving signal-to-noise ratio by nearly 10 dB.


<details>
  <summary>Details</summary>
Motivation: Thermal intermodulation noise (TIN) from nonlinear mixing of thermomechanical motion increases measurement imprecision above the standard quantum limit in optomechanical systems, and existing methods only cancel TIN up to second order.

Method: Record output of a membrane-in-the-middle microcavity system operating at room temperature with high cooperativity (C > nth), then apply a nonlinear transform that removes all orders of TIN.

Result: The nonlinear transform improves mechanical signal-to-noise ratio by nearly 10 dB and can handle third-order TIN, which is expected to be the dominating intrinsic noise source in high-cooperativity room-temperature systems.

Conclusion: A nonlinear transform technique effectively eliminates all orders of thermal intermodulation noise, enabling improved displacement measurements in high-cooperativity room-temperature cavity optomechanical systems where third-order TIN dominates.

Abstract: Coupling mechanical motion to an optical resonator enables displacement measurements approaching the standard quantum limit (SQL). However, increasing the optomechanical coupling strength will inevitably lead to probing of the nonlinear response of the optical resonator. Thermal intermodulation noise (TIN) arising from the nonlinear mixing of thermomechanical motion can further increase the imprecision well above the SQL and has hitherto been canceled up to second order of nonlinearity via operation at the "magic detuning". In this work, we record the output of a membrane-in-the-middle microcavity system operating at room temperature and achieving high cooperativity, $C>n_\text{th}$, and apply a nonlinear transform that removes all orders of TIN, improving the mechanical signal-to-noise ratio by nearly 10 dB. Our results can be applied to experiments affected by third-order TIN, which we expect to be the dominating intrinsic source of noise in high-cooperativity room-temperature cavity optomechanical systems.

</details>


### [67] [Constant-Depth Unitary Preparation of Dicke States](https://arxiv.org/abs/2601.10693)
*Francisca Vasconcelos,Malvika Raj Joshi*

Main category: quant-ph

TL;DR: First unitary constant-depth protocols for exact Dicke state preparation using global interactions beyond standard circuit models, overcoming logarithmic-depth barriers.


<details>
  <summary>Details</summary>
Motivation: Dicke states are crucial for quantum metrology, communication, and computation, but existing unitary preparation methods are limited to logarithmic depth in standard circuit models, while constant-depth protocols require measurement and feed-forward operations.

Method: Move beyond standard circuit model by leveraging global interactions native to architectures like neutral atoms and trapped ions. Use unbounded CZ gates (QAC⁰ circuit class) for exact constant-weight Dicke states with polynomial ancillae, and approximate weight-1 Dicke states (W states) with constant ancillae. With additional quantum FAN-OUT operation (QAC_f⁰ circuit class), achieve exact preparation of arbitrary-weight Dicke states with polynomial ancillae.

Result: First unitary constant-depth protocols for exact Dicke state preparation, distinguishing constant-depth capabilities of quantum architectures based on connectivity, and offering a path toward resolving a long-standing quantum complexity conjecture.

Conclusion: The work demonstrates that moving beyond standard circuit models and leveraging global interactions enables unitary constant-depth Dicke state preparation, advancing quantum state preparation capabilities and providing insights into quantum complexity.

Abstract: Dicke states serve as a critical resource in quantum metrology, communication, and computation. However, unitary preparation of Dicke states is limited to logarithmic depth in standard circuit models and existing constant-depth protocols require measurement and feed-forward. In this work, we present the first unitary, constant-depth protocols for exact Dicke state preparation. We overcome the logarithmic-depth barrier by moving beyond the standard circuit model and leveraging global interactions (native to architectures such as neutral atoms and trapped ions). Specifically, utilizing unbounded CZ gates (i.e. within the QAC$^0$ circuit class), we offer circuits for exact computation of constant-weight Dicke states, using polynomial ancillae, and approximation of weight-1 Dicke states (i.e. $W$ states), using only constant ancillae. Granted additional access to the quantum FAN-OUT operation (i.e. upgrading to the QAC$_f^0$ circuit class), we also achieve exact preparation of arbitrary-weight Dicke states, with polynomial ancillae. These protocols distinguish the constant-depth capabilities of quantum architectures based on connectivity and offer a novel path toward resolving a long-standing quantum complexity conjecture.

</details>


### [68] [Madelung hydrodynamics of spin-orbit coupling: action principles, currents, and correlations](https://arxiv.org/abs/2601.10698)
*Cesare Tronci*

Main category: quant-ph

TL;DR: The paper develops a quantum hydrodynamic framework with spin-orbit coupling, identifying distinct SOC-induced orbital forces and spin-hydrodynamic forces, revealing two mechanisms for quantum spin-orbit correlations, and demonstrating applications to spin transport phenomena.


<details>
  <summary>Details</summary>
Motivation: To understand the correlation and torque mechanisms accompanying spin-orbit coupling in electronic motion by exploiting the variational and Hamiltonian structures of quantum hydrodynamics with spin, distinguishing between different force terms that were previously overlooked or conflated.

Method: Using Hamilton's action principle for the Pauli equation to derive quantum hydrodynamic equations with spin, isolating SOC-induced quantum forces acting on orbital Madelung-Bohm trajectories, distinguishing them from spin-hydrodynamic forces related to the quantum geometric tensor, and leveraging the Hamiltonian structure to analyze spin transport.

Result: Identified two fundamentally different mechanisms generating quantum spin-orbit correlations: SOC-induced orbital forces from a particular current operator contributing to spin current, and spin-hydrodynamic forces related to the quantum geometric tensor. Elucidated spin transport features including current shift in spin Hall effect and correlation-induced quantum torques, illustrated via Madelung-Rashba equations for planar SOC.

Conclusion: The variational and Hamiltonian approach to quantum hydrodynamics with spin provides a comprehensive framework for understanding SOC mechanisms, distinguishing previously overlooked force contributions, explaining spin transport phenomena, and enabling numerical implementation through particle-based schemes.

Abstract: We exploit the variational and Hamiltonian structures of quantum hydrodynamics with spin to unfold the correlation and torque mechanisms accompanying spin-orbit coupling (SOC) in electronic motion. Using Hamilton's action principle for the Pauli equation, we isolate SOC-induced quantum forces that act on the orbital Madelung--Bohm trajectories and complement the usual force terms known to appear in quantum hydrodynamics with spin. While the latter spin-hydrodynamic forces relate to the quantum geometric tensor (QGT), SOC-induced orbital forces originate from a particular current operator that contributes prominently to the spin current and whose contribution was overlooked in the past. The distinction between different force terms reveals two fundamentally different mechanisms generating quantum spin-orbit correlations. Leveraging the Hamiltonian structure of the hydrodynamic system, we also elucidate spin transport features such as the current shift in the spin Hall effect and the correlation-induced quantum torques. Finally, we illustrate the framework via the Madelung--Rashba equations for planar SOC configurations and propose a particle-based scheme for numerical implementation.

</details>


### [69] [Scalable Spin Squeezing in Power-Law Interacting XXZ Models with Disorder](https://arxiv.org/abs/2601.10703)
*Samuel E. Begg,Bishal K. Ghosh,Chong Zu,Chuanwei Zhang,Michael Kolodrubetz*

Main category: quant-ph

TL;DR: Spin squeezing in 2D lattices with power-law interactions remains scalable up to a disorder threshold despite positional disorder, explaining limitations in NV center experiments.


<details>
  <summary>Details</summary>
Motivation: Recent experiments with NV centers in diamond showed that positional disorder severely limits spin squeezing scalability, contradicting expectations from all-to-all interacting models and raising questions about practical applications in disordered quantum systems.

Method: Semi-classical modeling of two-dimensional lattices with power-law interacting XXZ models containing a fraction of unoccupied lattice sites to study disorder effects on spin squeezing.

Result: Demonstrated existence of scalable squeezing up to a disorder threshold, beyond which squeezing loses scalability. Produced phase diagram for scalable squeezing and explained absence of scalable squeezing in NV center experiments due to exceeding the disorder threshold.

Conclusion: Identified maximum disorder tolerance for scalable spin squeezing in quantum simulators, highlighted regimes with substantial disorder tolerance, and proposed controlled defect creation as a promising approach for achieving scalable squeezing in solid-state systems.

Abstract: While spin squeezing has been traditionally considered in all-to-all interacting models, recent works have shown that spin squeezing can occur in systems with power-law interactions, leading to direct testing in Rydberg atoms, trapped ions, ultracold atoms and nitrogen vacancy (NV) centers in diamond. For the latter, Wu. et al. Nature 646 (2025) demonstrated that spin squeezing is heavily affected by positional disorder, reducing any capacity for a practical squeezing advantage, which requires scalability with the system size. In this Letter we explore the robustness of spin-squeezing in two-dimensional lattices with a fraction of unoccupied lattice sites. Using semi-classical modeling, we demonstrate the existence of scalable squeezing in power-law interacting XXZ models up to a disorder threshold, above which squeezing is not scalable. We produce a phase diagram for scalable squeezing, and explain its absence in the aforementioned NV experiment. Our work illustrates the maximum disorder allowed for realizing scalable spin squeezing in a host of quantum simulators, highlights a regime with substantial tolerance to disorder, and identifies controlled defect creation as a promising route for scalable squeezing in solid-state systems.

</details>


### [70] [Quantum Maxwell Erasure Decoder for qLDPC codes](https://arxiv.org/abs/2601.10713)
*Bruno Costa Alves Freire,François-Marie Le Régent,Anthony Leverrier*

Main category: quant-ph

TL;DR: Quantum Maxwell erasure decoder for CSS qLDPC codes extends peeling with bounded symbolic guessing, offering tunable complexity-performance tradeoff via guessing budget.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient decoder for CSS quantum LDPC codes that bridges the gap between simple linear-time decoding and optimal Maximum-Likelihood performance, addressing the need for practical decoders with theoretical guarantees.

Method: Extends peeling decoder with bounded symbolic guessing where guesses are tracked symbolically and can be eliminated by restrictive checks; introduces guessing budget parameter to control complexity-performance tradeoff.

Result: Unconstrained budget recovers ML performance; constant budget yields linear-time decoding approximating ML; provides theoretical asymptotic performance guarantees; demonstrates strong performance on bivariate bicycle and quantum Tanner codes.

Conclusion: The quantum Maxwell erasure decoder offers a tunable framework for CSS qLDPC code decoding, enabling practical linear-time approximations of ML decoding with provable performance guarantees.

Abstract: We introduce a quantum Maxwell erasure decoder for CSS quantum low-density parity-check (qLDPC) codes that extends peeling with bounded guessing. Guesses are tracked symbolically and can be eliminated by restrictive checks, giving a tunable tradeoff between complexity and performance via a guessing budget: an unconstrained budget recovers Maximum-Likelihood (ML) performance, while a constant budget yields linear-time decoding and approximates ML. We provide theoretical guarantees on asymptotic performance and demonstrate strong performance on bivariate bicycle and quantum Tanner codes.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [71] [Emergent Nonperturbative Universal Floquet Localization](https://arxiv.org/abs/2601.09793)
*Soumadip Pakrashi,Atanu Rajak,Sambuddha Sanyal*

Main category: cond-mat.dis-nn

TL;DR: Robust nonperturbative localization plateau emerges in driven quasiperiodic lattices, independent of static properties and drive protocol, with fine-tuned amplitude-to-frequency ratio causing all Floquet states to localize despite dense resonances.


<details>
  <summary>Details</summary>
Motivation: To understand localization phenomena in periodically driven quasiperiodic systems and identify conditions where robust localization emerges independent of static properties and drive protocols.

Method: Exact Floquet dynamics, Floquet perturbation theory, and optimal-order van Vleck analysis to study driven quasiperiodic lattices and identify fine-tuned amplitude-to-frequency ratios.

Result: A robust nonperturbative localization plateau emerges at specific amplitude-to-frequency ratios where all Floquet states become localized despite dense resonances; van Vleck expansion achieves superasymptotic accuracy but ultimately breaks down due to resonant hybridization.

Conclusion: The observed localization is nonperturbative in nature, emerging from fine-tuned drive parameters and persisting despite the presence of dense resonances in the quasiperiodic potential.

Abstract: We show that a robust, nonperturbative localization plateau emerges in periodically driven quasiperiodic lattices, independent of the static localization properties and drive protocol. Using exact Floquet dynamics, Floquet perturbation theory, and optimal-order van Vleck analysis, we identify a fine-tuned amplitude-to-frequency ratio where all Floquet states become localized despite dense resonances. The van Vleck expansion achieves superasymptotic accuracy up to an optimal orde; it ultimately breaks down due to resonant hybridization at a weak quasiperiodic potential, revealing that the observed localization is nonperturbative.

</details>


### [72] [Integral Variable Range Hopping for Modeling Electrical Transport in Disordered Systems](https://arxiv.org/abs/2601.10226)
*Chenxin Qin,Chenyan Wang,Mouyang Cheng,Ji Chen*

Main category: cond-mat.dis-nn

TL;DR: The paper introduces an Integral Variable Range Hopping (IVRH) model that replaces empirical temperature power-law dependencies in standard VRH theories with a physics-inspired integral formulation, providing more stable and physically meaningful fitting for hopping transport in disordered systems.


<details>
  <summary>Details</summary>
Motivation: Standard VRH models rely on oversimplified assumptions that restrict applicability and cause problematic fitting behaviors, yet they are overused. There's a need for a more robust framework that can better describe electrical transport in disordered systems across different temperature regimes.

Method: Developed an integral variable range hopping (IVRH) model that replaces empirical temperature power-law dependencies with an integral formulation based on hopping probability ω(R) and an effective volume function V(R) that incorporates system geometry. Applied the model to 2D, 3D, and multi-layered systems and validated with Monte Carlo simulations.

Result: IVRH inherently reproduces both Mott behavior at low temperatures and Arrhenius behavior at high temperatures with smooth transition between regimes. Monte Carlo simulations validate predictions and yield consistent fitting parameters with substantially reduced variances compared to standard VRH. Improved robustness demonstrated in monolayer MoS₂ and WS₂ systems.

Conclusion: IVRH offers a more stable and physically sound framework for interpreting hopping transport in low-dimensional amorphous materials, providing deeper insights into universal geometric scaling factors governing charge transport in disordered systems.

Abstract: The variable range hopping (VRH) model has been widely applied to describe electrical transport in disordered systems, providing theoretical formulas to fit temperature-dependent electric conductivity. These models rely on oversimplified assumptions that restrict their applicability and result in problematic fitting behaviors, yet their overusing situation is becoming increasingly serious. In this work we formulate an integral variable range hopping (IVRH) model, which replaces the empirical temperature power-law dependence in standard VRH theories with a physics-inspired integral formulation. The model builds upon the standard hopping probability $ω(R)$ w.r.t. hopping distance $R$ and incorporates the density of accessible electronic states through an effective volume function $V(R)$, which reflects the influence of system geometry. The IVRH formulation inherently reproduces both the Mott behavior at low temperatures and the Arrhenius behavior at high temperatures, respectively, and enables a smooth transition between the two regimes. We apply the IVRH model to two-dimensional, three-dimensional, and multi-layered systems. Monte Carlo simulations validate the model's predictions and yield consistent values for the fitting parameters, with substantially reduced variances compared to fitting using the standard VRH model. Furthermore, the improved robustness of IVRH also extends to the transport measurements in monolayer MoS$_2$ system and monolayer WS$_2$ system, enabling more physically meaningful interpretation.IVRH model offers a more stable and physically sound framework for interpreting hopping transport in low-dimensional amorphous materials, providing deeper insights into the universal geometric scaling factors that govern charge transport in disordered systems.

</details>


### [73] [Computer Generation of Disordered Networks with Targeted Structural Properties](https://arxiv.org/abs/2601.10333)
*Florin Hemmann,Vincent Glauser,Ullrich Steiner,Matthias Saba*

Main category: cond-mat.dis-nn

TL;DR: Extended Wooten-Weaire-Winer algorithm to generate disordered spatial networks with arbitrary coordination numbers using bond repulsion in Keating strain energy, enabling targeted disorder generation for studying complex wave phenomena.


<details>
  <summary>Details</summary>
Motivation: Need for efficient numerical methods to generate disordered networks with targeted structural properties for studying complex wave phenomena like structural phase transitions, localization, diffusion, and band gaps. Existing methods limited to 3D networks with coordination numbers ≤4.

Method: Extended Wooten-Weaire-Winer algorithm by introducing bond repulsion in Keating strain energy to handle arbitrary coordination numbers. Tuned disorder by varying bond-bending force constant and temperature profile. Used order metrics to analyze structural properties. Trained feedforward neural network to predict structural characteristics from algorithm inputs.

Result: Successfully generated disordered networks with tailored structural properties. Demonstrated capability by statistically reproducing four disordered biophotonic networks exhibiting structural color. Neural network enabled targeted network generation.

Conclusion: Developed versatile method for generating disordered networks with tailored structural properties, enabling new insights into structure-property relations such as photonic band gaps in disordered networks.

Abstract: Disordered spatial networks are model systems that describe structures and interactions across multiple length scales. Scattering and interference of waves in these networks can give rise to structural phase transitions, localization, diffusion, and band gaps. The study of these complex phenomena requires efficient numerical methods to computer-generate disordered networks with targeted structural properties. In the established Wooten-Weaire-Winer algorithm, a series of bond switch moves introduces disorder into an initial network. Conventional strain energies that govern this evolution are limited to 3D networks with coordination numbers of no more than four. We extend the algorithm to arbitrary coordination number statistics by introducing bond repulsion in the Keating strain energy. We tune the degree and type of disorder introduced into initially crystalline networks by varying the bond-bending force constant in the strain energy and the temperature profile. The effects of these variables are analyzed using a list of order metrics that capture both direct and reciprocal space. A feedforward neural network is trained to predict the structural characteristics from the algorithm inputs, enabling targeted network generation. As a case study, we statistically reproduce four disordered biophotonic networks exhibiting structural color. This work presents a versatile method for generating disordered networks with tailored structural properties. It will enable new insights into structure-property relations, such as photonic band gaps in disordered networks.

</details>


### [74] [The eigenvalues and eigenvectors of finite-rank normal perturbations of large rotationally invariant non-Hermitian matrices](https://arxiv.org/abs/2601.10427)
*Pierre Bousseyroux,Marc Potters*

Main category: cond-mat.dis-nn

TL;DR: Finite-rank normal deformations of rotationally invariant non-Hermitian random matrices: characterization of outlier eigenvalues and eigenvector behavior, generalizing BBP framework.


<details>
  <summary>Details</summary>
Motivation: Extend the classical Baik-Ben Arous-Péché (BBP) framework for outlier eigenvalues to non-Hermitian random matrices with finite-rank normal perturbations, providing a unified theory that encompasses both Hermitian and non-Hermitian settings.

Method: Study models of the form A + T, where A is a large rotationally invariant non-Hermitian random matrix and T is a finite-rank normal perturbation. Analyze the emergence and fluctuations of outlier eigenvalues and corresponding eigenvector behavior.

Result: Characterization of outlier eigenvalue emergence and fluctuations in non-Hermitian random matrices with finite-rank normal deformations. Description of eigenvector behavior. Provides a unified framework generalizing several known cases.

Conclusion: The paper establishes a comprehensive theory for finite-rank normal deformations of rotationally invariant non-Hermitian random matrices, extending the BBP framework and unifying Hermitian and non-Hermitian outlier eigenvalue analysis.

Abstract: We study finite-rank normal deformations of rotationally invariant non-Hermitian random matrices. Extending the classical Baik-Ben Arous-Péché (BBP) framework, we characterize the emergence and fluctuations of outlier eigenvalues in models of the form $\mathbf{A} + \mathbf{T}$, where $\mathbf{A}$ is a large rotationally invariant non-Hermitian random matrix and $\mathbf{T}$ is a finite-rank normal perturbation. We also describe the corresponding eigenvector behavior. Our results provide a unified framework encompassing both Hermitian and non-Hermitian settings, thereby generalizing several known cases.

</details>
