<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 28]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Minimal length: a source of non-Hermiticity and non-locality in quantum mechanics](https://arxiv.org/abs/2601.02413)
*H. Moradpour,S. Jalalzadeh*

Main category: quant-ph

TL;DR: Study explores connections between quantum momentum measurements, minimal length effects, complex numbers in quantum mechanics, and generation of entangled states, suggesting minimal length theories enrich understanding of non-locality.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between quantum momentum measurements (canonical vs. generalized with minimal length effects), justify the existence of complex numbers in quantum mechanics as a consequence of minimal length, and explore how minimal length theories can enhance understanding of non-locality.

Method: Theoretical analysis examining connections between purely quantum mechanical momentum measurements and generalized momentum operators with minimal length effects. Development of a novel method for generating quantum entangled states with complex quantum numbers inspired by minimal length considerations.

Result: Established relationships between different momentum measurement frameworks, provided justification for complex numbers in quantum mechanics as arising from minimal length effects, and developed a method for generating entangled states with complex quantum numbers based on minimal length principles.

Conclusion: Theories incorporating minimal length, such as certain quantum gravity scenarios, can enrich current understanding of non-locality by providing new insights into quantum momentum measurements, the origin of complex numbers in quantum mechanics, and novel methods for generating entangled states.

Abstract: First, the study tries to shed light on the relationship between purely quantum mechanical momentum measurements (canonical momentum space) and measurements of the generalized momentum operator, including minimal length effects. Additionally, the existence of complex numbers in quantum mechanics seems justifiable as a consequence of minimal length. Finally, a novel method for generating quantum entangled states with complex quantum numbers inspired by the minimal length is also reported. Therefore, theories including a minimal length, like some quantum scenarios of gravity, seem to be able to enrich the current understanding of non-locality.

</details>


### [2] [How Alice, long before her time, derived the principles of quantum mechanics](https://arxiv.org/abs/2601.02419)
*Marcello Poletti*

Main category: quant-ph

TL;DR: Quantum mechanics principles may not describe new physics but emerge from formalizing measurement within algebraic frameworks, independent of quantum particle properties.


<details>
  <summary>Details</summary>
Motivation: To challenge conventional interpretations of quantum mechanics by proposing that its foundational principles arise from logical necessities of formalizing measurement rather than describing fundamentally new physical phenomena.

Method: Philosophical dialogue exploring extreme perspective that quantum structures can be derived independently of quantum particle properties through algebraic formalization of measurement, considering limitations of observability and measurement context.

Result: Core structures of quantum mechanics can be formulated within classical theory when measurement limitations and context are properly accounted for, suggesting quantum principles emerge from measurement formalization rather than new physics.

Conclusion: Quantum mechanics may not represent fundamentally new physics but rather logical consequences of formalizing measurement within coherent algebraic frameworks, with quantum structures derivable from classical theory when measurement limitations are seriously considered.

Abstract: This philosophical dialogue explores the idea that the foundational principles of quantum mechanics need not be interpreted as describing a new physics, but may instead arise from the logical necessity of formalising the act of measurement within a coherent algebraic framework. By pushing this perspective to its extreme, the dialogue argues that the core structures of quantum mechanics can be derived independently of any specifically quantum properties of atomic particles, and can be formulated within an otherwise classical theory once limitations of observability and measurement context are taken seriously.

</details>


### [3] [Formal Modeling and Verification of Grover's Algorithm](https://arxiv.org/abs/2601.02435)
*H. Sun,Z. Shi,S. Chen,G. Wang,X. Li,Y. Guan,Q. Zhang,Z. Shao*

Main category: quant-ph

TL;DR: Formal verification of Grover's quantum search algorithm using HOL Light theorem prover, proving key properties and demonstrating practicality through integer factorization application.


<details>
  <summary>Details</summary>
Motivation: Quantum algorithms like Grover's are difficult to verify through traditional simulation due to quantum mechanics complexity; formal verification using logical reasoning provides rigorous correctness guarantees.

Method: Formally model Grover's algorithm in HOL Light theorem prover, using higher-order logical reasoning to verify key properties including operator unitarity, probability monotonicity, and optimal iteration count.

Result: Successfully proved: unitarity of oracle and diffusion operators, monotonicity of success probability with iteration count, exact expression for optimal iteration count, and demonstrated practicality through integer factorization application.

Conclusion: Formal verification of quantum algorithms using theorem proving is feasible and practical, providing rigorous correctness guarantees that complement traditional simulation methods.

Abstract: Grover's algorithm relies on the superposition and interference of quantum mechanics, which is more efficient than classical computing in specific tasks such as searching an unsorted database. Due to the high complexity of quantum mechanics, the correctness of quantum algorithms is difficult to guarantee through traditional simulation methods. By contrast, the fundamental concepts and mathematical structure of Grover's algorithm can be formalized into logical expressions and verified by higher-order logical reasoning. In this paper, we formally model and verify Grover's algorithm in the HOL Light theorem prover. We focus on proving key properties such as the unitarity of its oracle and diffusion operators, the monotonicity of the success probability with respect to the number of iterations, and an exact expression for the optimal iteration count. By analyzing a concrete application to integer factorization, we demonstrate the practicality and prospects of our work.

</details>


### [4] [Gravitational time dilation in quantum clock interferometry with entangled multi-photon states and quantum memories](https://arxiv.org/abs/2601.02470)
*Mustafa Gündoğan,Roy Barzel,Dennis Rätzel*

Main category: quant-ph

TL;DR: A quantum clock interferometer using frequency-bin photonic clocks stored in vertically separated quantum memories achieves gravitational time dilation effects amplified by N for 2N-photon inputs, enabling observation with height differences of 10-100 m using current Rb/Cs memory technology.


<details>
  <summary>Details</summary>
Motivation: To establish near-term laboratory conditions for observing entanglement dynamics driven by gravitational time dilation in a photonic platform, overcoming the challenge that gravitational effects are typically extremely small and difficult to measure.

Method: A memory-assisted quantum clock interferometer where a frequency-bin photonic clock is stored in two vertically separated quantum memories for controllable durations, creating quantum superposition of proper times. After retrieval, photonic modes interfere in a Hong-Ou-Mandel interferometer, with analysis extended from entangled photon pairs to frequency-entangled 2N-photon inputs.

Result: For 2N-photon inputs, the proper-time dependent phase is amplified by factor N, leading to N-times faster collapse and revival of interference signal compared to two-photon case. With Rb/Cs memories and achievable optical frequency separations, first collapse occurs for height differences of 10-100 m with subsecond to few-second storage times. Rare-earth ion and alkali memory combinations can reduce required height to few-metre scale.

Conclusion: The proposed scheme establishes feasible near-term laboratory conditions for observing gravitational time dilation effects on quantum entanglement using existing quantum memory technology and manageable height differences, enabling experimental study of entanglement dynamics in gravitational fields.

Abstract: Gravitational time dilation implies that clocks held at different heights accumulate different proper times. We analyze a memory-assisted quantum clock interferometer in which a frequency-bin photonic clock is stored in two vertically separated quantum memories for a controllable duration, such that the joint state evolves in a quantum superposition of two proper times. After retrieval, the photonic modes interfere in a Hong-Ou-Mandel (HOM) interferometer, for which we derive analytic expressions for the resulting multiphoton detection statistics. Extending this HOM-based scheme from entangled photon pairs to frequency-entangled 2N-photon inputs, we show that the proper-time dependent phase is amplified by a factor N, leading to an N-times faster collapse and revival of the interference signal compared with the two-photon case. Incorporating finite memory efficiency and lifetime, we identify regimes where this modulation remains observable. For parameters compatible with demonstrated Rb and Cs memories and achievable optical frequency separations, the first collapse occurs for height differences in the order of 10-100 m with subsecond to few-second storage times, while suitable rare-earth ion and alkali memory combinations can reduce the required height to the few-metre scale. These results establish near-term laboratory conditions for observing entanglement dynamics driven by gravitational time dilation in a photonic platform.

</details>


### [5] [Gaussian time-translation covariant operations: structure, implementation, and thermodynamics](https://arxiv.org/abs/2601.02471)
*Xueyuan Hu,Lea Lautenbacher,Giovanni Spaventa,Martin B. Plenio,Nelly H. Y. Ng,Jeongrak Son*

Main category: quant-ph

TL;DR: Classification of Gaussian quantum operations covariant under time translation reveals key differences from discrete-variable systems in physical implementation, thermodynamics, asymmetry extensivity, and catalytic advantages.


<details>
  <summary>Details</summary>
Motivation: Time-translation symmetry strongly constrains physical dynamics, but systematic characterization for continuous-variable systems lags behind discrete-variable systems. The paper aims to close this gap by providing rigorous classification of Gaussian covariant operations.

Method: The authors provide comprehensive mathematical and operational toolkits for Gaussian covariant operations, including analysis of asymmetry measures and examination of the interplay among symmetry, Gaussianity, and thermodynamic constraints.

Result: Several key results known for discrete-variable covariant operations break down in Gaussian optical setting: discrepancies arise in physical/thermodynamic implementation, extensivity of asymmetry, and catalytic advantages. The paper identifies a peculiar pair of completely non-extensive asymmetry measures.

Conclusion: The findings reveal surprising consequences of the interplay among symmetry, Gaussianity, and thermodynamic constraints, suggesting that real-world scenarios with multiple constraints have rich structure not accessible from examining individual constraints separately.

Abstract: Time-translation symmetry strongly constrains physical dynamics, yet systematic characterization for continuous-variable systems lags behind its discrete-variable counterpart. We close this gap by providing a rigorous classification of Gaussian quantum operations that are covariant under time translations, termed Gaussian covariant operations. We show that several key results known for discrete-variable covariant operations break down in the Gaussian optical setting: discrepancies arise in physical and thermodynamic implementation, in the extensivity of asymmetry, and in catalytic advantages. Our results provide comprehensive mathematical and operational toolkits for Gaussian covariant operations, including a peculiar pair of asymmetry measures that are completely non-extensive. Our findings also reveal surprising consequences of the interplay among symmetry, Gaussianity, and thermodynamic constraints, suggesting that real-world scenarios with multiple constraints have a rich structure not accessible from examining individual constraints separately.

</details>


### [6] [Minimization of AND-XOR Expressions with Decoders for Quantum Circuits](https://arxiv.org/abs/2601.02515)
*Sonia Yang,Ali Al-Bayaty,Marek Perkowski*

Main category: quant-ph

TL;DR: A new three-level reversible quantum circuit synthesis method using Multi-Valued Input Fixed Polarity Reed-Muller forms with decoders to reduce quantum cost by decreasing input qubits in Toffoli gates.


<details>
  <summary>Details</summary>
Motivation: To minimize quantum cost in reversible quantum circuits by addressing the problem of high input qubit counts in Toffoli gates from traditional two-level ESOP-based methods, which significantly impacts quantum circuit efficiency.

Method: Introduces Multi-Valued Input Fixed Polarity Reed-Muller (MVI-FPRM) forms and decoder-based three-level circuits. Presents two algorithms: products-matching and butterfly diagrams for MVI-FPRM synthesis, followed by factorization and reduction to approximate Multi-Valued Input Generalized Reed-Muller (MVI-GRM) forms.

Result: The decoder-based approach reduces the number of input qubits in Toffoli gates compared to traditional two-level ESOP methods, thereby decreasing quantum cost while maintaining binary circuit implementation despite using multi-valued input functions as mathematical concepts.

Conclusion: The proposed three-level synthesis method using MVI-FPRM forms with decoders offers a more efficient alternative to conventional two-level ESOP approaches for reversible quantum circuit synthesis, effectively reducing quantum cost through decreased input qubit requirements in Toffoli gates.

Abstract: This paper introduces a new logic structure for reversible quantum circuit synthesis. Our synthesis method aims to minimize the quantum cost of reversible quantum circuits with decoders. In this method, multi-valued input, binary output (MVI) functions are utilized as a mathematical concept only, but the circuits are binary. We introduce the new concept of ``Multi-Valued Input Fixed Polarity Reed-Muller (MVI-RM)" forms. Our decoder-based circuit uses three logical levels in contrast to commonly-used methods based on Exclusive-or Sum of Products (ESOP) with two levels (AND-XOR expressions), realized by Toffoli gates. In general, the high number of input qubits in the resulting Toffoli gates is a problem that greatly impacts the quantum cost. Using decoders decreases the number of input qubits in these Toffoli gates. We present two practical algorithms for three-level circuit synthesis by finding the MVI-FPRM: products-matching and the newly developed butterfly diagrams. The best MVI-FPRM forms are factorized and reduced to approximate Multi-Valued Input Generalized Reed-Muller (MVI-GRM) forms.

</details>


### [7] [Compressed Qubit Noise Spectroscopy: Piecewise-Linear Modeling and Rademacher Measurements](https://arxiv.org/abs/2601.02516)
*Kaixin Huang,Demitry Farfurnik,Dror Baron,Yi-Kai Liu*

Main category: quant-ph

TL;DR: Random pulse sequences enhanced with TGV regularization for piecewise-linear noise spectra and simplified via Rademacher measurements for sparse spectra, improving quantum noise spectroscopy efficiency and practicality.


<details>
  <summary>Details</summary>
Motivation: To advance random pulse sequence methods for qubit noise spectroscopy by addressing limitations in reconstructing realistic noise spectra and reducing experimental complexity.

Method: Two complementary approaches: 1) Extend random pulse sequences with total generalized variation (TGV) norm regularization to reconstruct piecewise-linear noise spectra; 2) Introduce Rademacher measurements using pseudorandom pulse sequences generated from short random seeds for sparse noise spectra.

Result: TGV-based method resolves finer spectral features while maintaining order-of-magnitude speedup over conventional approaches; Rademacher measurements reduce experimental complexity without compromising reconstruction accuracy.

Conclusion: These developments broaden the applicability of random pulse sequences for accurate and efficient noise characterization in realistic quantum systems.

Abstract: Random pulse sequences are a powerful method for qubit noise spectroscopy, enabling efficient reconstruction of sparse noise spectra. Here, we advance this method in two complementary directions. First, we extend the method using a regularizer based on the total generalized variation (TGV) norm, in order to reconstruct a larger class of noise spectra, namely piecewise-linear noise spectra, which more realistically model many physical systems. We show through numerical simulations that the new method resolves finer spectral features, while maintaining an order-of-magnitude speedup over conventional approaches to noise spectroscopy. Second, we simplify the experimental implementation of the method, by introducing Rademacher measurements for reconstructing sparse noise spectra. These measurements use pseudorandom pulse sequences that can be generated in real time from a short random seed, reducing experimental complexity without compromising reconstruction accuracy. Together, these developments broaden the reach of random pulse sequences for accurate and efficient noise characterization in realistic quantum systems.

</details>


### [8] [Deep learning parameter estimation and quantum control of single molecule](https://arxiv.org/abs/2601.02517)
*Juan M. Scarpetta,Omar Calderón-Losada,Morten Hjorth-Jensen,John H. Reina*

Main category: quant-ph

TL;DR: Researchers demonstrate parameter estimation methods for coherent control of single molecules at room temperature using two-photon absorption photoluminescence signals and computational approaches.


<details>
  <summary>Details</summary>
Motivation: Coherent control requires precise characterization of dissipative dynamics, especially at high temperatures, but learning system-bath parameters and driving coupling strengths in quantum control experiments remains challenging.

Method: Developed and compared two computational approaches based on two-photon absorption photoluminescence signals: an optimization-based minimization scheme and a feed-forward neural network to infer key physical parameters of a single molecule driven by spectrally modulated pulses at room temperature.

Result: Demonstrated successful parameter inference for coherent control of single molecules at room temperature, with robust approaches highlighting the importance of reliable parameter estimation for designing effective coherent control protocols.

Conclusion: The developed parameter estimation methods have direct applications in ultrafast spectroscopy, quantum materials, and quantum technology, enabling more effective coherent control protocols.

Abstract: Coherent control, a central concept in physics and chemistry, has sparked significant interest due to its ability to fine-tune interference effects in atoms and individual molecules for applications ranging from light-harvesting complexes to molecular qubits. However, precise characterization of the system's dissipative dynamics is required for its implementation, especially at high temperature. In a quantum control experiment, this means learning system-bath parameters and driving coupling strengths. Here, we demonstrate how to infer key physical parameters of a single molecule driven by spectrally modulated pulses at room temperature. We develop and compare two computational approaches based on two-photon absorption photoluminescence signals: an optimization-based minimization scheme and a feed-forward neural network. The robustness of our approach highlights the importance of reliable parameter estimation in designing effective coherent control protocols. Our results have direct applications in ultrafast spectroscopy, quantum materials and technology.

</details>


### [9] [Further Improving the Decoy State Quantum Key Distribution Protocol with Advantage Distillation](https://arxiv.org/abs/2601.02565)
*Walter O. Krawec*

Main category: quant-ph

TL;DR: Improved security proof for advantage distillation in decoy-state BB84 protocol with tighter bounds on Eve's uncertainty in vacuum/single-photon events, leading to better key rates and longer distances.


<details>
  <summary>Details</summary>
Motivation: Previous security proofs for classical advantage distillation (CAD) in decoy-state BB84 assumed trivial zero entropy bounds for vacuum-state CAD blocks, resulting in sub-optimal key-rate bounds despite CAD's known benefits for distance and noise tolerance.

Method: Derived new security proof for CAD applied to decoy-state BB84 protocol by computing tighter bounds on Eve's uncertainty in all possible single-photon and vacuum photon events, then used this to derive improved asymptotic key-rate bound.

Result: New key-rate bound outperforms prior work, enabling increased maximal distances and improved noise tolerances for practical decoy-state quantum key distribution.

Conclusion: The improved security analysis provides more accurate bounds on Eve's information in vacuum/single-photon events during CAD, leading to enhanced practical performance of decoy-state BB84 with advantage distillation.

Abstract: In this paper, we revisit the application of classical advantage distillation (CAD) to the decoy-state BB84 protocol. Prior work has shown that CAD can greatly improve maximal distances and noise tolerances of the practical decoy state protocol. However, past work in deriving key-rate bounds for this protocol with CAD have assumed a trivial bound on the quantum entropy, whenever Alice sends a vacuum state in a CAD block (i.e., the entropy of such blocks is taken to be zero). Since such rounds contribute, negatively, to the error correction leakage, this results in a correct, though sub-optimal bound. Here, we derive a new proof of security for CAD applied to the decoy state BB84 protocol, computing a bound on Eve's uncertainty in all possible single and vacuum photon events. We use this to derive a new asymptotic key-rate bound which, we show, outperforms prior work, allowing for increased distances and noise tolerances.

</details>


### [10] [Localization of joint quantum measurements on $\mathbb{C}^d \otimes \mathbb{C}^d$ by entangled resources with Schmidt number at most $d$](https://arxiv.org/abs/2601.02660)
*Seiseki Akibue,Jisho Miyazaki*

Main category: quant-ph

TL;DR: Characterization of localizable quantum measurements (PVMs) that can be implemented using only non-adaptive local operations and shared entanglement, revealing strong limitations compared to adaptive settings.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limitations of implementing joint quantum measurements using only non-adaptive local operations and shared entanglement, and to characterize which projection-valued measures (PVMs) can be localized under these constraints.

Method: Exploits algebraic structures that localizable measurements must satisfy. First analyzes rank-1 PVMs on bipartite systems containing maximal Schmidt rank elements, then characterizes two-qubit rank-1 PVMs localizable with two-qubit entanglement, and extends to ideal two-qudit measurements.

Result: Shows that rank-1 PVMs with maximal Schmidt rank elements can be localized only if they form maximally entangled bases corresponding to nice unitary error bases. Completely characterizes two-qubit rank-1 PVMs localizable with two-qubit entanglement, resolving Gisin and Del Santo's conjecture. Extends characterization to ideal two-qudit measurements.

Conclusion: Reveals strong limitations imposed by non-adaptive local operations compared to adaptive settings, provides complete characterizations for important cases, and demonstrates that only specific algebraic structures (nice unitary error bases) allow localization under these constraints.

Abstract: Localizable measurements are joint quantum measurements that can be implemented using only non-adaptive local operations and shared entanglement. We provide a protocol-independent characterization of localizable projection-valued measures (PVMs) by exploiting algebraic structures that any such measurement must satisfy. We first show that a rank-1 PVM on $\mathbb{C}^d\otimes\mathbb{C}^d$ containing an element with the maximal Schmidt rank can be localized using entanglement of a Schmidt number at most $d$ if and only if it forms a maximally entangled basis corresponding to a nice unitary error basis. This reveals strong limitations imposed by non-adaptive local operations, in contrast to the adaptive setting where any joint measurement is implementable. We then completely characterize two-qubit rank-1 PVMs that can be localized with two-qubit entanglement, resolving a conjecture of Gisin and Del Santo, and finally extend our characterization to ideal two-qudit measurements, strengthening earlier results.

</details>


### [11] [Multiparameter quantum estimation with a uniformly accelerated Unruh-DeWitt detector](https://arxiv.org/abs/2601.02689)
*Shoukang Chang,Yashu Yang,Wei Ye,Yawen Tang,Hui Cao,Huan Zhang,Zunlue Zhu,Shaoming Fei,Xingdong Zhao*

Main category: quant-ph

TL;DR: Multiparameter estimation for uniformly accelerated Unruh-DeWitt detector shows quantum Cramér-Rao bound is not tight; Nagaoka bound provides tightest error bound, with boundaries improving estimation precision.


<details>
  <summary>Details</summary>
Motivation: Previous studies focused on single-parameter estimation via quantum Cramér-Rao bound, leaving multiparameter estimation for uniformly accelerated Unruh-DeWitt detectors significantly underexplored, particularly in both bounded and unbounded Minkowski vacuum scenarios.

Method: Investigate multiparameter estimation for uniformly accelerated Unruh-DeWitt detector coupled to vacuum scalar field. Numerically compute tighter error bounds (Holevo Cramér-Rao bound and Nagaoka bound) using semidefinite programming when quantum Cramér-Rao bound fails to provide tight error bound for two-parameter estimation involving initial phase and weight parameters.

Result: Nagaoka bound yields tightest error bound among all considered bounds, consistent with general hierarchy of multiparameter quantum estimation. In bounded case, boundary systematically reduces values of both Holevo Cramér-Rao and Nagaoka bounds, indicating improvement in attainable estimation precision.

Conclusion: Results provide valuable insights and practical guidance for advancing multiparameter estimation in relativistic quantum metrology, demonstrating superiority of Nagaoka bound and beneficial effects of boundaries on estimation precision.

Abstract: The uniformly accelerated Unruh-DeWitt detector serves as a fundamental model in relativistic quantum metrology. While previous studies have mainly concentrated on single-parameter estimation via quantum Cramér-Rao bound, the multi-parameter case remains significantly underexplored. In this paper, we investigate the multiparameter estimation for a uniformly accelerated Unruh-DeWitt detector coupled to a vacuum scalar field in both bounded and unbounded Minkowski vacuum. Our analysis reveals that quantum Cramér-Rao bound fails to provide a tight error bound for the two-parameter estimation involving the initial phase and weight parameters. For this reason, we numerically compute two tighter error bounds, Holevo Cramér-Rao bound and Nagaoka bound, based on a semidefinite program. Notably, our results demonstrate that Nagaoka bound yields the tightest error bound among all the considered error bounds, consistent with the general hierarchy of multiparameter quantum estimation. In the case with a boundary, we observe the introduction of boundary systematically reduces the values of both Holevo Cramér-Rao bound and Nagaoka bound, indicating an improvement on the attainable estimation precision. These results offer valuable insights on and practical guidance for advancing multiparameter estimation in relativistic context.

</details>


### [12] [Q-based, objective-field model for wave-function collapse: Analyzing measurement on a macroscopic superposition state](https://arxiv.org/abs/2601.02767)
*Channa Hatharasinghe,Ashleigh Willis,Run Yan Teh,P. D. Drummond,M. D. Reid*

Main category: quant-ph

TL;DR: The paper analyzes quantum measurement using a Q-based objective-field model, showing that measurement outcomes are determined when meter-system coupling completes, with wavefunction collapse occurring as a two-stage process involving amplification and information loss.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved measurement problem in quantum mechanics by examining measurement processes using a Q-based objective-field model, specifically analyzing how microscopic superpositions become macroscopic measurement outcomes.

Method: Uses Q-based objective-field model for quantum mechanics, solving forward-backward stochastic differential equations for real amplitudes x(t) and p(t) corresponding to phase-space variables of the Q function. Models system and meter as single-mode fields, with measurement of x̂ achieved through amplification of amplitude x(t). Analyzes entangled meter-system dynamics and evaluates postselected amplitude distributions.

Result: Measurement outcomes are determined at time t_m when meter-system coupling completes, with meter states becoming macroscopically distinguishable. The Q-based model provides a more complete description where postselected x and p variances are too narrow to satisfy uncertainty principle, indicating the distribution doesn't represent a quantum state. Wavefunction collapse occurs as a two-stage process: amplification creates branches of x(t) amplitudes associated with eigenvalues, then information loss about complementary variable p determines final collapse.

Conclusion: The Q-based objective-field model offers a resolution to the measurement problem by showing that measurement outcomes are determined when macroscopic distinguishability is achieved, with wavefunction collapse occurring through a two-stage process of amplification and information loss, consistent with macroscopic realism and explaining Born's rule.

Abstract: The measurement problem remains unaddressed in modern physics, with an array of proposed solutions but as of yet no agreed resolution. In this paper, we examine measurement using the Q-based, objective-field model for quantum mechanics. Schrodinger considered a microscopic system prepared in a superposition of states which is then coupled to a macroscopic meter. We analyze the entangled meter and system, and measurements on it, by solving forward-backward stochastic differential equations for real amplitudes $x(t)$ and $p(t)$ that correspond to the phase-space variables of the Q function of the system at a time $t$. We model the system and meter as single-mode fields, and measurement of $\hat{x}$ by amplification of the amplitude $x(t)$. Our conclusion is that the outcome for the measurement is determined at (or by) the time $t_{m}$, when the coupling to the meter is complete, the meter states being macroscopically distinguishable. There is consistency with macroscopic realism. By evaluating the distribution of the amplitudes $x$ and $p$ postselected on a given outcome of the meter, we show how the $Q$-based model represents a more complete description of quantum mechanics: The variances associated with amplitudes $x$ and $p$ are too narrow to comply with the uncertainty principle, ruling out that the distribution represents a quantum state. We conclude that the collapse of the wavefunction occurs as a two-stage process: First there is an amplification that creates branches of amplitudes $x(t)$ of the meter, associated with distinct eigenvalues. The outcome of measurement is determined by $x(t)$ once amplified, explaining Born's rule. Second, the distribution that determines the final collapse is the state inferred for the system conditioned on the outcome of the meter: information is lost about the meter, in particular, about the complementary variable $p$.

</details>


### [13] [Quantum key distribution without authentication and information leakage](https://arxiv.org/abs/2601.02846)
*Zixuan Hu,Zhenyu Li*

Main category: quant-ph

TL;DR: New QKD variant eliminates separate authentication requirement and classical post-processing, achieving higher key rates with reusable protocol keys and near-perfect information-theoretic security.


<details>
  <summary>Details</summary>
Motivation: Conventional QKD requires separate authentication mechanisms and has public classical post-processing steps that create information leakage, making it vulnerable to attacks and reducing key rates.

Method: Proposes a new QKD variant that uses two additional protocol keys and eliminates all public classical steps, enabling authentication without external mechanisms and preventing information leakage.

Result: Achieves substantially higher key rates, eliminates the need for separate authentication, removes information leakage vulnerabilities, and enables reusable protocol keys with near-perfect information-theoretic security.

Conclusion: The proposed QKD variant overcomes fundamental limitations of conventional QKD by integrating authentication, eliminating classical post-processing vulnerabilities, and achieving superior security and efficiency with reusable keys.

Abstract: Quantum key distribution (QKD) is the most widely studied quantum cryptographic model that exploits quantum effects to achieve information-theoretically secure key establishment. Conventional QKD contains public classical post-processing steps that require authentication to prevent impersonation and maintain security. However, a major limitation of QKD is it cannot perform authentication by itself, and thus requires a separate authentication mechanism. In addition, these public classical steps also have information leakage which subjects QKD to additional attack strategies and reduces the final key rate. In this work, we propose a new QKD variant that removes the need for a separate authentication mechanism, eliminates information leakage, and achieves a substantially higher key rate. By having two more protocol keys than conventional QKD and no public classical steps, our design achieves (almost) perfect information-theoretic security with the protocol keys reusable.

</details>


### [14] [Entanglement Entropy for Screened Interactions via Dimensional Mapping to Harmonic Oscillators](https://arxiv.org/abs/2601.02877)
*Akshay Kulkarni,Rahul Nigam*

Main category: quant-ph

TL;DR: The paper develops an oscillator-based framework to compute interaction-induced corrections to entanglement entropy for Yukawa-type interactions using perturbative methods.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze how interactions beyond Gaussian approximations affect entanglement entropy in quantum systems, particularly for Yukawa-type potentials, and to establish a controlled perturbative framework for such calculations.

Method: Map a screened Yukawa interaction to an effective 4D radial oscillator system with polynomial perturbations, then apply Rayleigh-Schrodinger perturbation theory to compute ground-state wavefunction corrections and reduced density matrix eigenvalues.

Result: Obtained closed analytic expressions for leading non-Gaussian corrections to entanglement entropy at order α², showing contributions from both explicit anharmonic state-mixing and implicit Gaussian width renormalization.

Conclusion: The oscillator mapping provides a systematic framework for computing entanglement entropy in interacting systems, clarifies distinct roles of harmonic renormalization vs. genuine non-Gaussian effects, and establishes power-counting for higher-order perturbations.

Abstract: We investigate interaction-induced corrections to entanglement entropy by mapping a screened Yukawa-type interaction to an effective harmonic oscillator system with controlled anharmonic perturbations. Starting from a one-dimensional interaction $V(x) = -g^2 e^{-αm x}/x$, we reformulate the problem in terms of a four-dimensional radial oscillator, where the finite screening length generates a systematic hierarchy of polynomial interactions in the radial coordinate. This mapping enables a controlled Rayleigh-Schrodinger perturbative treatment of the ground-state wavefunction and an explicit spectral analysis of the reduced density matrix. Working in the weak-screening regime, we compute the leading non-Gaussian correction arising from the quartic interaction $ρ^4$, which appears at order $α^2$ in the expansion of the Yukawa-like potential. We obtain closed analytic expressions for the resulting small eigenvalues of the reduced density matrix and evaluate their contribution to the von Neumann entanglement entropy. We show that the entropy receives analytic corrections at order $α^2$, originating both from explicit anharmonic state-mixing effects and from the implicit $α$ dependence of the Gaussian width parameter. Our results clarify the distinct roles of harmonic renormalization and genuinely non-Gaussian interactions in generating entanglement, establish a systematic power-counting and normalization scheme for higher-order $ρ^{2n}$ perturbations, and provide a transparent oscillator-based framework for computing entanglement entropy in weakly interacting low-dimensional and field-theoretic systems.

</details>


### [15] [Trading symmetry for Hilbert-space dimension in Bell-inequality violation](https://arxiv.org/abs/2601.02893)
*Hsin-Yu Hsu,Gelo Noel M. Tabia,Kai-Siang Chen,Mu-En Liu,Tamás Vértesi,Nicoals Brunner,Yeong-Cherng Liang*

Main category: quant-ph

TL;DR: The paper explores whether symmetric quantum strategies in minimal Hilbert space dimensions can achieve maximal Bell inequality violations, finding that some symmetric inequalities require asymmetric strategies for optimal violation, while some asymmetric inequalities can be maximally violated by symmetric correlations.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between symmetry in quantum strategies and achieving maximal Bell inequality violations, specifically exploring when symmetry must be sacrificed for lower-dimensional strategies to achieve optimal violation.

Method: Focuses on symmetry in party exchange, analyzes symmetric Collins-Gisin-Linden-Massar-Popescu inequalities and other Bell inequalities with small numbers of dichotomic measurement settings, comparing symmetric vs. asymmetric quantum strategies in minimal Hilbert space dimensions.

Result: For symmetric CGLMP inequalities, no trade-off between symmetry and dimension is needed. However, for several other Bell inequalities, symmetric quantum strategies in minimal dimensions lead to suboptimal violations, requiring asymmetric strategies for maximal violation. Some asymmetric inequalities can be maximally violated by symmetric correlations.

Conclusion: Symmetry in quantum strategies and achieving maximal Bell violations involves complex trade-offs; some symmetric inequalities require asymmetric strategies for optimal violation, while some asymmetric inequalities can be maximally violated by symmetric correlations, with implications for quantum correlation geometry and self-testing.

Abstract: In quantum information, asymmetry, i.e., the lack of symmetry, is a resource allowing one to accomplish certain tasks that are otherwise impossible. Similarly, in a Bell test using any given Bell inequality, the maximum violation achievable using quantum strategies respecting or disregarding a certain symmetry can be different. In this work, we focus on the symmetry involved in the exchange of parties and explore when we have to trade this symmetry for a lower-dimensional quantum strategy in achieving the maximal violation of given Bell inequalities. For the family of symmetric Collins-Gisin-Linden-Massar-Popescu inequalities, we provide evidence showing that there is no such trade-off. However, for several other Bell inequalities with a small number of dichotomic measurement settings, we show that symmetric quantum strategies in the minimal Hilbert space dimension can only lead to a suboptimal Bell violation. In other words, there exist symmetric Bell inequalities that can only be maximally violated by asymmetric quantum strategies of minimal dimension. In contrast, one can also find examples of asymmetric Bell inequalities that are maximally violated by symmetric correlations. The implications of these findings on the geometry of the set of quantum correlations and the possibility of performing self-testing therefrom are briefly discussed.

</details>


### [16] [Violation of Bell Monogamy Relations](https://arxiv.org/abs/2601.02925)
*Abhisek Panda,Chandan Datta,Pankaj Agrawal*

Main category: quant-ph

TL;DR: Bell monogamy relations can be violated using local filtering operations on permutation-symmetric multipartite pure states like W states.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Bell monogamy relations, which suggest limited sharing of nonlocality across subsystems, can be violated through appropriate operations, similar to how entanglement monogamy relations can be circumvented.

Method: Use local filtering operations on permutation-symmetric multipartite pure states, specifically focusing on W states, to demonstrate violation of Bell monogamy relations.

Result: Demonstration that Bell monogamy relations can indeed be violated through the application of local filtering operations on appropriate quantum states.

Conclusion: Bell monogamy relations are not absolute constraints and can be circumvented using local filtering operations, revealing that nonlocality sharing across subsystems can be manipulated beyond the limitations suggested by standard Bell monogamy relations.

Abstract: The entangled multipartite systems, specially in pure states, exhibit the phenomenon entanglement monogamy. Such systems also display the phenomenon of Bell nonlocality. Like entanglement monogamy relations, there are Bell monogamy relations. These relations suggest a sharing of nonlocality across the subsystems. The nonlocality, as characterized by Bell inequalities, of one subsystem limits the nonlocality exhibited by another subsystem. We show that the Bell monogamy relations can be violated by using local filtering operations. We consider permutation-symmetric multipartite pure states, in particular $W$ states, to demonstrate the violation.

</details>


### [17] [Nonseparability as Time-Averaged Dynamic States](https://arxiv.org/abs/2601.02977)
*Mathieu Padlewski,Tim Tuuva,Benjamin Apffel,Hervé Lissek,Romain Fleury*

Main category: quant-ph

TL;DR: The paper proposes a novel framework that interprets quantum nonseparability (entanglement) as a time-averaged effect of underlying oscillatory dynamics, mediated by auxiliary angular frequencies acting as coherence channels.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative theoretical understanding of quantum nonseparability and entanglement, and to enable practical simulation of multipartite entanglement in classical wave systems.

Method: Introduces auxiliary angular frequencies that modulate temporal evolution of composite states, creating coherence channels through which nonseparability is mediated, framing entanglement as time-averaged oscillatory behavior.

Result: Develops a formalism that reinterprets quantum nonseparability as emerging from underlying oscillatory processes, potentially enabling entanglement simulation in classical systems.

Conclusion: The proposed framework offers both theoretical insights into entanglement mechanisms and practical pathways for simulating multipartite entanglement in controlled classical wave systems.

Abstract: Nonseparability - multipartite states that cannot be factorized - is one of the most striking features of quantum mechanics, as it gives rise to entanglement and non-causal correlations. In quantum computing, it also contributes directly to the computational advantage of quantum computers over its digital counterparts. In this work, we introduce a simple mechanism that frames nonseparability as a time-averaged manifestation of an underlying oscillatory process within state space. The central idea is the inclusion of auxiliary angular frequencies that modulate the temporal evolution of composite states. These additional dynamical degrees of freedom act as coherence channels through which nonseparability is mediated. While the proposed formalism could eventually serve as an alternative theoretical handle on the mechanisms of quantum entanglement, its greater significance lies in opening practical routes for simulating multipartite entanglement in controlled classical wave systems.

</details>


### [18] [Entanglement signatures of quantum criticality in Floquet non-Hermitian topological systems](https://arxiv.org/abs/2601.03002)
*Siyuan Cheng,Rui Xie,Xiaosen Yang,Yuee Xie,Yuanping Chen*

Main category: quant-ph

TL;DR: Entanglement entropy serves as an effective diagnostic for topological phase transitions in the 1D Floquet SSH model, exhibiting logarithmic scaling with central charge c=1 at critical points and revealing topological phase boundaries through entanglement spectrum analysis.


<details>
  <summary>Details</summary>
Motivation: To establish entanglement entropy as a reliable and universal diagnostic tool for probing topological phase transitions in periodically driven systems, specifically in the 1D Floquet Su-Schrieffer-Heeger model, and to understand how entanglement properties can reveal topological phase boundaries and edge mode hybridization.

Method: Investigation of topological phase transitions in the 1D Floquet SSH model using entanglement entropy analysis, including construction of phase diagrams based on entanglement entropy, examination of logarithmic scaling behavior at critical points, and analysis of entanglement spectra to distinguish different topological phases and detect hybridization between zero and π modes.

Result: Entanglement entropy shows pronounced peaks at phase transition points following logarithmic scaling law with extracted central charge c=1. Entanglement spectrum analysis accurately distinguishes different topological phases and reveals characteristic splittings indicating hybridization between zero and π modes under periodic driving. Results remain robust in non-Hermitian regimes and with next-nearest-neighbor hopping.

Conclusion: Entanglement entropy provides a reliable and universal diagnostic for topological phase transitions in Floquet systems, capable of identifying critical points through logarithmic scaling behavior and distinguishing topological phases via entanglement spectrum analysis, with robustness extending to non-Hermitian and extended hopping scenarios.

Abstract: The entanglement entropy can be an effective diagnostic tool for probing topological phase transitions. In one-dimensional single particle systems, the periodic driving generates a variety of topological phases and edge modes. In this work, we investigate the topological phase transition of the one-dimensional Floquet Su-Schrieffer-Heeger model using entanglement entropy, and construct the phase diagram based on entanglement entropy. The entanglement entropy exhibits pronounced peaks and follows the logarithmic scaling law at the phase transition points, from which we extract the central charge $c=1$. We further investigate the entanglement spectrum to accurately distinguish the different topological phases. In addition, the coupling between zero and $π$ modes leads to characteristic splittings in the entanglement spectrum, signaling their hybridization under periodic driving. These results remain robust in non-Hermitian regimes and in the presence of next-nearest-neighbor hopping, demonstrating the reliability and universality of entanglement entropy as a diagnostic for topological phase transitions.

</details>


### [19] [Who can compete with quantum computers? Lecture notes on quantum inspired tensor networks computational techniques](https://arxiv.org/abs/2601.03035)
*Xavier Waintal,Chen-How Huang,Christoph W. Groth*

Main category: quant-ph

TL;DR: Tensor network lectures focusing on MPS/MPO algorithms as linear algebra tools for exponentially large matrices/vectors, covering DMRG, linear solvers, TCI learning, and quantics representation for calculus and applications.


<details>
  <summary>Details</summary>
Motivation: To present tensor network algorithms (particularly MPS/MPO) as general linear algebra tools independent of quantum many-body physics, making them accessible for diverse applications beyond their traditional quantum physics context.

Method: Detailed presentation of core tensor network algorithms including: 1) MPO/MPS as linear algebra frameworks for exponential-sized matrices/vectors, 2) DMRG for finding MPO eigenvectors, 3) linear problem solvers, 4) Tensor Cross Interpolation (TCI) for function-to-MPS mapping, 5) "quantics" representation for calculus with tensor networks, and 6) analytical construction of key MPOs (differentiation, integration, convolution, QFT).

Result: Comprehensive lecture series providing: 1) Detailed proofs of all statements, 2) Analytical constructions of fundamental MPOs, 3) Three concrete applications: quantum computer simulation (exact/compressed), quantum annealer simulation, and PDE solving (Poisson, diffusion, Gross-Pitaevskii) using quantics representation.

Conclusion: Tensor networks (MPS/MPO) provide powerful general-purpose linear algebra frameworks for exponentially large systems, with applications spanning quantum simulation, PDE solving, and function representation through the quantics approach, accessible to first-year PhD students.

Abstract: This is a set of lectures on tensor networks with a strong emphasis on the core algorithms involving Matrix Product States (MPS) and Matrix Product Operators (MPO). Compared to other presentations, particular care has been given to disentangle aspects of tensor networks from the quantum many-body problem: MPO/MPS algorithms are presented as a way to deal with linear algebra on extremely (exponentially) large matrices and vectors, regardless of any particular application. The lectures include well-known algorithms to find eigenvectors of MPOs (the celebrated DMRG), solve linear problems, and recent learning algorithms that allow one to map a known function into an MPS (the Tensor Cross Interpolation, or TCI, algorithm). The lectures end with a discussion of how to represent functions and perform calculus with tensor networks using the "quantics" representation. They include the detailed analytical construction of important MPOs such as those for differentiation, indefinite integration, convolution, and the quantum Fourier transform. Three concrete applications are discussed in detail: the simulation of a quantum computer (either exactly or with compression), the simulation of a quantum annealer, and techniques to solve partial differential equations (e.g. Poisson, diffusion, or Gross-Pitaevskii) within the "quantics" representation. The lectures have been designed to be accessible to a first-year PhD student and include detailed proofs of all statements.

</details>


### [20] [Collective dynamics versus entanglement in quantum battery performance](https://arxiv.org/abs/2601.03119)
*Rohit Kumar Shukla,Sunil K. Mishra,Ujjwal Sen*

Main category: quant-ph

TL;DR: Enhanced quantum battery charging performance stems from coherent collective dynamics rather than quantum correlations, with peak power occurring before entanglement buildup, and fully collective interactions providing genuine advantages over partially extended schemes.


<details>
  <summary>Details</summary>
Motivation: To identify whether improved charging performance in quantum batteries originates from genuine quantum correlations or from coherent collective dynamics that are not intrinsically quantum, distinguishing between classical scaling effects and genuine collective enhancements.

Method: Compare time evolution of energetic quantities with a hierarchy of information-theoretic measures probing bipartite, tripartite, and further-partite correlations across different battery-charger configurations. Examine charging protocols based on k-local interactions under both unconstrained and norm-constrained (fair) settings.

Result: Instantaneous power peaks before buildup of strong quantum correlations, indicating peak charging is dominated by coherent transport while entanglement and scrambling develop later. Increasing interaction order or participation number doesn't automatically increase charging power. Performance is dictated by how many particles become mutually correlated and contribute to entanglement.

Conclusion: Fully collective interactions provide genuine advantage because all particles participate coherently, whereas partially extended interaction schemes fail to monotonically increase effectively interacting particles and don't guarantee improved charging efficiency.

Abstract: Identifying the physical origin of enhanced charging performance in many-body quantum batteries is a key challenge in quantum thermodynamics. We investigate whether improvements in stored energy and instantaneous charging power arise from genuine quantum correlations or from coherent collective dynamics that are not intrinsically quantum. We compare the time evolution of energetic quantities with a hierarchy of information-theoretic measures probing bipartite, tripartite, and further-partite correlations. Across different battery charger configurations, we find a consistent temporal ordering in which the instantaneous power peaks before the buildup of strong quantum correlations, indicating that peak charging is dominated by coherent transport, while entanglement and scrambling develop at later times. Furthermore, charging protocols based on k local interactions are examined under both unconstrained and norm-constrained (fair) settings, enabling a clear distinction between classical scaling effects and genuine collective enhancements. Increasing the interaction order or the participation number does not automatically translate into higher charging power. Instead, the performance is primarily dictated by how many particles actually become mutually correlated and contribute to entanglement. Fully collective interactions provide a genuine advantage because all particles participate coherently, whereas partially extended interaction schemes fail to monotonically increase the number of effectively interacting particles, and therefore do not guarantee improved charging efficiency.

</details>


### [21] [Gradient descent reliably finds depth- and gate-optimal circuits for generic unitaries](https://arxiv.org/abs/2601.03123)
*Janani Gomathi,Alex Meiburg*

Main category: quant-ph

TL;DR: Gradient descent can find optimal quantum circuits for generic unitaries, contrary to previous beliefs requiring combinatorial search, by avoiding parameter-deficient circuit skeletons.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of efficiently finding minimal quantum circuits for unitary operators with continuous parameters, particularly contrasting compiled unitaries (which have short circuits) with generic unitaries (which typically require maximal circuit size). Previous evidence suggested optimal synthesis required combinatorial search, creating an efficiency problem.

Method: The authors use simple gradient descent to find depth- and gate-optimal circuits for generic unitaries. A key insight is avoiding the random selection of certain parameter-deficient circuit skeletons that previously led to the perception that combinatorial search was necessary.

Result: The research shows that gradient descent reliably finds optimal circuits for generic unitaries, including in scenarios with restricted chip connectivity. This demonstrates that optimal synthesis doesn't require combinatorial search as previously believed.

Conclusion: The discrepancy between earlier evidence (requiring combinatorial search) and the current findings can be explained by avoiding parameter-deficient circuit skeletons. This suggests gradient descent is a viable approach for optimal quantum circuit synthesis, potentially improving efficiency in quantum compilation.

Abstract: When the gate set has continuous parameters, synthesizing a unitary operator as a quantum circuit is always possible using exact methods, but finding minimal circuits efficiently remains a challenging problem. The landscape is very different for compiled unitaries, which arise from programming and typically have short circuits, as compared with generic unitaries, which use all parameters and typically require circuits of maximal size. We show that simple gradient descent reliably finds depth- and gate-optimal circuits for generic unitaries, including in the presence of restricted chip connectivity. This runs counter to earlier evidence that optimal synthesis required combinatorial search, and we show that this discrepancy can be explained by avoiding the random selection of certain parameter-deficient circuit skeletons.

</details>


### [22] [Operational modes of a Raman-coupled two-qubit quantum thermal machine](https://arxiv.org/abs/2601.03139)
*Alonso Alcalá,Charlie Oncebay,Onofre Rojas,Moises Rojas*

Main category: quant-ph

TL;DR: Quantum thermal machine with two qubits using Raman-induced exchange interaction operates as heat engine, refrigerator, thermal accelerator, or heater across Carnot, Otto, and Stirling cycles with regeneration.


<details>
  <summary>Details</summary>
Motivation: To investigate thermodynamic behavior of a quantum thermal machine with controllable asymmetry through Raman-type interaction and frequency tuning, exploring different thermodynamic cycles and operational modes.

Method: Analyze a two-qubit system coupled via Raman-induced exchange interaction with inhomogeneous transition frequencies. Study thermodynamic cycles: Carnot, Otto, and Stirling (with/without regeneration). Identify operational conditions for different thermodynamic functions.

Result: Carnot cycle shows sharp engine-refrigerator transitions; Otto cycle displays richer structure with coexistence of all modes; Stirling cycle offers enhanced versatility and performance, achieving near-ideal efficiencies with regenerator. Operational boundaries governed by frequency ratio r=ω̄/ω, coupling strength g, and thermal gradient.

Conclusion: Raman-type interaction introduces controllable left-right asymmetry enabling nontrivial thermodynamic manipulation through frequency tuning. Stirling cycle with regeneration provides optimal performance with near-ideal efficiencies.

Abstract: We investigate a quantum thermal machine composed of two qubits coupled through a Raman-induced exchange interaction and driven by inhomogeneous transition frequencies. The system is analyzed within Carnot, Otto, and Stirling thermodynamic cycles, including the Stirling cycle with and without regeneration. We identify the conditions under which the device operates as a heat engine, refrigerator, thermal accelerator, or heater. Efficiency maps and operational-mode diagrams reveal well-defined boundaries in parameter space, governed by the frequency ratio $r=\barω/ω$, the coupling strength $g$, and the thermal gradient between reservoirs. The Carnot cycle exhibits sharp transitions between engine and refrigerator regimes, while the Otto cycle displays a richer structure with the coexistence of all operational modes. The Stirling cycle shows enhanced versatility and performance, particularly when assisted by a regenerator, where near-ideal efficiencies are achieved. Overall, the Raman-type interaction introduces a controllable left-right asymmetry that enables nontrivial manipulation of thermodynamic behavior through frequency tuning.

</details>


### [23] [Energetics of Rydberg-atom Quantum Computing](https://arxiv.org/abs/2601.03141)
*Óscar Alves,Marco Pezzutto,Yasser Omar*

Main category: quant-ph

TL;DR: Analysis of energy efficiency in Rydberg atom quantum computers, focusing on Quantum Phase Estimation and Quantum Fourier Transform algorithms, with comparisons to classical supercomputers.


<details>
  <summary>Details</summary>
Motivation: While quantum computing promises computational speedups, its energy efficiency remains understudied. As Rydberg atoms emerge as a promising quantum computing platform, understanding their energy consumption is crucial for determining their viability as alternatives to classical computers.

Method: Analyzes energy consumption of Rydberg atom quantum computers by: 1) Examining experimental implementation of Quantum Phase Estimation algorithm, 2) Deriving scaling of energy cost for Quantum Fourier Transform, 3) Comparing energy consumption across different computer elements (atom preparation, algorithm execution, measurement), 4) Benchmarking against classical supercomputers executing Discrete Fourier Transform.

Result: Provides first estimates of energy consumption for Rydberg atom quantum computing platform, identifies energy expenditure patterns across different system components, and establishes comparative energy scaling between quantum and classical approaches to Fourier transforms.

Conclusion: This work establishes foundational analysis of energy efficiency in Rydberg atom quantum computers, enabling evaluation of the platform's energy performance and identification of optimization opportunities for more energy-efficient quantum computing implementations.

Abstract: Quantum computing exploits the properties of Quantum Mechanics to solve problems faster than classical computers. The potential applications of this technology have been widely explored, and extensive research over the past decades has been dedicated to developing scalable quantum computers. However, the question of the energetic performance of quantum computation has only gained attention more recently, and its importance is now recognized. In fact, quantum computers can only be a viable alternative if their energy cost scales favorably, and some research has shown that there is even a potential quantum energy advantage. Rydberg atoms have emerged recently as one of the most promising platforms to implement a large-scale quantum computer, with significant advances made in recent years. This work aims at contributing first steps to understand the energy efficiency of this platform, namely by investigating the energy consumption of the different elements of a Rydberg atom quantum computer. First, an experimental implementation of the Quantum Phase Estimation algorithm is analyzed, and an estimation of the energetic cost of executing this algorithm is calculated. Then, a potential scaling of the energy cost of performing the Quantum Fourier Transform with Rydberg atoms is derived. This analysis facilitates a comparison of the energy consumption of different elements within a Rydberg atom quantum computer, from the preparation of the atoms to the execution of the algorithm, and the measurement of the final state, enabling the evaluation of the energy expenditure of the Rydberg platform and the identification of potential improvements. Finally, we used the Quantum Fourier Transform as an energetic benchmark, comparing the scaling we obtained to that of the execution of the Discrete Fourier Transform in two state-of-the-art classical supercomputers.

</details>


### [24] [A Unified Frequency Principle for Quantum and Classical Machine Learning](https://arxiv.org/abs/2601.03169)
*Rundi Lu,Ruiqi Zhang,Weikang Li,Zhaohui Wei,Dong-Ling Deng,Zhengwei Liu*

Main category: quant-ph

TL;DR: Quantum neural networks exhibit spectral bias toward low-frequency components during training, similar to classical networks, with noise exponentially suppressing high-frequency features while preserving low-frequency learnability.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics of quantum neural networks (QNNs) and establish a unified theoretical framework that connects classical and quantum learning through the frequency principle, while clarifying the impact of noise on trainability.

Method: Developed a unified theoretical framework for the frequency principle, proving QNNs exhibit spectral bias toward low-frequency components. Analyzed noise impact by modeling single-qubit noise as Pauli channels aligned with rotation axes, deriving suppression factors for Fourier components. Established conditions for efficient classical simulation of noisy circuits. Conducted numerical experiments to validate theoretical predictions.

Result: Proved QNNs learn low-frequency components first during training, similar to classical networks. Showed noise suppresses Fourier component ω by factor (1-2γ)^‖ω‖₁, causing exponential attenuation of high-frequency terms while preserving low-frequency structure. Demonstrated noisy circuits admit efficient classical simulation up to average-case error. Numerical experiments confirmed QNNs primarily learn low-frequency features early and maintain robustness against dephasing/depolarizing noise on encoding layer.

Conclusion: The frequency principle provides a unifying framework for classical and quantum learning dynamics. QNNs exhibit spectral bias toward low frequencies, and noise selectively suppresses high-frequency components, guiding design of noise-resilient quantum neural networks while establishing connections to classical simulation.

Abstract: Quantum neural networks constitute a key class of near-term quantum learning models, yet their training dynamics remain not fully understood. Here, we present a unified theoretical framework for the frequency principle (F-principle) that characterizes the training dynamics of both classical and quantum neural networks. Within this framework, we prove that quantum neural networks exhibit a spectral bias toward learning low-frequency components of target functions, mirroring the behavior observed in classical deep networks. We further analyze the impact of noise and show that, when single-qubit noise is applied after encoding-layer rotations and modeled as a Pauli channel aligned with the rotation axis, the Fourier component labeled by $\boldsymbolω$ is suppressed by a factor $(1-2γ)^{\|\boldsymbolω\|_1}$. This leads to exponential attenuation of high-frequency terms while preserving the learnability of low-frequency structure. In the same setting, we establish that the resulting noisy circuits admit efficient classical simulation up to average-case error. Numerical experiments corroborate our theoretical predictions: Quantum neural networks primarily learn low-frequency features during early optimization and maintain robustness against dephasing and depolarizing noise acting on the encoding layer. Our results provide a frequency-domain lens that unifies classical and quantum learning dynamics, clarifies the role of noise in shaping trainability, and guides the design of noise-resilient quantum neural networks.

</details>


### [25] [FTCircuitBench: A Benchmark Suite for Fault-Tolerant Quantum Compilation and Architecture](https://arxiv.org/abs/2601.03185)
*Adrian Harkness,Shuwen Kan,Chenxu Liu,Meng Wang,John M. Martyn,Shifan Xu,Diana Chamaki,Ethan Decker,Ying Mao,Luis F. Zuluaga,Tamás Terlaky,Ang Li,Samuel Stein*

Main category: quant-ph

TL;DR: FTCircuitBench is an open-source benchmark suite and toolkit for evaluating fault-tolerant quantum compilation, featuring pre-compiled algorithms, modular compilation pipelines, and detailed performance analysis tools.


<details>
  <summary>Details</summary>
Motivation: Quantum error correction is essential for large-scale quantum advantage, requiring specialized compilation tools distinct from NISQ-era approaches. The complexity of fault-tolerant quantum computing stacks necessitates standardized benchmarks and toolkits to evaluate compilation progress and co-design factors like gate decomposition precision and computational models.

Method: Developed FTCircuitBench as a three-component system: (1) benchmark suite of impactful quantum algorithms pre-compiled in Clifford+T and Pauli Based Computation models; (2) modular end-to-end compilation pipeline supporting various fault-tolerant architectures with prebuilt and custom optimization passes; (3) toolkit for evaluating algorithm and optimization impact across the full compilation stack with detailed numerical analysis at each stage.

Result: Created a fully open-sourced, maintained benchmark suite on GitHub that provides standardized evaluation tools for fault-tolerant quantum compilation, enabling systematic assessment of compilation techniques across different computational models and architectures.

Conclusion: FTCircuitBench addresses the critical need for standardized benchmarks in fault-tolerant quantum compilation, providing researchers with comprehensive tools to evaluate and optimize logical operations for quantum error correction, thereby supporting progress toward large-scale quantum advantage.

Abstract: Realizing large-scale quantum advantage is expected to require quantum error correction (QEC), making the compilation and optimization of logical operations a critical area of research. Logical computation imposes distinct constraints and operational paradigms that differ from those of the Noisy Intermediate-Scale Quantum (NISQ) regime, motivating the continued evolution of compilation tools. Given the complexity of this emerging stack, where factors such as gate decomposition precision and computational models must be co-designed, standardized benchmarks and toolkits are valuable for evaluating progress. To support this need, we introduce FTCircuitBench, which serves as: (1) a benchmark suite of impactful quantum algorithms, featuring pre-compiled instances in both Clifford+T and Pauli Based Computation models; (2) a modular end-to-end pipeline allowing users to compile and decompose algorithms for various fault-tolerant architectures, supporting both prebuilt and custom optimization passes; and (3) a toolkit for evaluating the impact of algorithms and optimization across the full compilation stack, providing detailed numerical analysis at each stage. FTCircuitBench is fully open-sourced and maintained on Github.

</details>


### [26] [Restoring Bloch's Theorem for Cavity Exciton Polaron-Polaritons](https://arxiv.org/abs/2601.03230)
*Michael A. D. Taylor,Yu Zhang*

Main category: quant-ph

TL;DR: Symmetry-informed representation restores Bloch's theorem in hybrid photon-exciton-phonon quantum electrodynamics Hamiltonians, enabling efficient computation of observables without approximations.


<details>
  <summary>Details</summary>
Motivation: Strong coupling between photons, excitons, and phonons breaks translational symmetry of crystalline excitons due to momentum interchange between fermions and bosons, preventing application of Bloch's theorem and complicating computation of experimentally accessible observables.

Method: Introduces a symmetry-informed representation for hybrid photon-exciton-phonon quantum electrodynamics Hamiltonians that restores Bloch's theorem by properly accounting for momentum exchange between fermionic and bosonic degrees of freedom.

Result: Enables efficient computation of experimentally accessible observables without introducing approximations to the Hamiltonian, facilitating investigations of material properties in strong coupling regimes.

Conclusion: The restored symmetry framework enhances coherent transport studies and unlocks symmetry-forbidden matter transitions, providing a foundation for applications in quantum electrodynamics and materials science.

Abstract: We introduce a symmetry-informed representation for hybrid photon--exciton--phonon quantum electrodynamics Hamiltonians to restore Bloch's theorem. The interchange of momenta between fermions and bosons breaks crystalline excitons' translational symmetry under strong coupling. Restoring said symmetry, we efficiently compute experimentally accessible observables without introducing approximations to the Hamiltonian, enabling investigations that elucidate material properties in strong coupling with applications enhancing coherent transport and unlocking symmetry-forbidden matter transitions.

</details>


### [27] [Shallow-circuit Supervised Learning on a Quantum Processor](https://arxiv.org/abs/2601.03235)
*Luca Candelori,Swarnadeep Majumder,Antonio Mezzacapo,Javier Robledo Moreno,Kharen Musaelian,Santhanam Nagarajan,Sunil Pinnamaneni,Kunal Sharma,Dario Villani*

Main category: quant-ph

TL;DR: A quantum machine learning method using linear Hamiltonian-based representation of classical data via ground state problems, trained with sample-based Krylov diagonalization and demonstrated on IBM quantum hardware up to 50 qubits.


<details>
  <summary>Details</summary>
Motivation: Overcome fundamental obstacles in practical quantum machine learning, including high quantum cost for classical data loading and poor trainability of near-term quantum algorithms.

Method: Linear Hamiltonian-based machine learning method providing compact quantum representation of classical data via ground state problems for k-local Hamiltonians, using sample-based Krylov quantum diagonalization to compute low-energy states, with parameters trained through local gradients.

Result: Demonstrated efficacy and scalability by performing experiments on benchmark datasets using up to 50 qubits of an IBM Heron quantum processor.

Conclusion: The approach overcomes key obstacles in practical quantum machine learning by providing efficient data representation and trainability on near-term quantum hardware.

Abstract: Quantum computing has long promised transformative advances in data analysis, yet practical quantum machine learning has remained elusive due to fundamental obstacles such as a steep quantum cost for the loading of classical data and poor trainability of many quantum machine learning algorithms designed for near-term quantum hardware. In this work, we show that one can overcome these obstacles by using a linear Hamiltonian-based machine learning method which provides a compact quantum representation of classical data via ground state problems for k-local Hamiltonians. We use the recent sample-based Krylov quantum diagonalization method to compute low-energy states of the data Hamiltonians, whose parameters are trained to express classical datasets through local gradients. We demonstrate the efficacy and scalability of the methods by performing experiments on benchmark datasets using up to 50 qubits of an IBM Heron quantum processor.

</details>


### [28] [Grand-Canonical Typicality](https://arxiv.org/abs/2601.03253)
*Cedric Igelspacher,Roderich Tumulka,Cornelia Vogel*

Main category: quant-ph

TL;DR: The paper extends canonical typicality to grand-canonical ensembles, showing that typical wavefunctions in generalized micro-canonical subspaces yield grand-canonical density matrices after tracing out a bath, and that conditional wavefunctions follow GAP distributions.


<details>
  <summary>Details</summary>
Motivation: To establish the foundation and justification for grand-canonical density matrices and wavefunction distributions in quantum statistical mechanics, extending beyond the well-known canonical typicality to include particle exchange and chemical reactions.

Method: Extends canonical typicality framework to grand-canonical ensembles by considering generalized micro-canonical Hilbert subspaces defined by energy intervals and particle number sectors, analyzing reduced density matrices after tracing out bath degrees of freedom.

Result: Shows that: (a) tracing out bath from generalized micro-canonical subspace yields grand-canonical density matrix; (b) typical wavefunctions have reduced density matrices close to grand-canonical; (c) conditional wavefunctions follow GAP distributions; extends to generalized Gibbs ensembles.

Conclusion: Provides rigorous foundation for grand-canonical ensembles in quantum statistical mechanics, justifying both density matrix formalism and wavefunction distributions for systems with particle exchange and chemical reactions, with extensions to systems with additional conserved quantities.

Abstract: We study how the grand-canonical density matrix arises in macroscopic quantum systems. ``Canonical typicality'' is the known statement that for a typical wave function $Ψ$ from a micro-canonical energy shell of a quantum system $S$ weakly coupled to a large but finite quantum system $B$, the reduced density matrix $\hatρ^S_Ψ=\mathrm{tr}^B |Ψ\rangle\langle Ψ|$ is approximately equal to the canonical density matrix $\hatρ_\mathrm{can}=Z^{-1}_\mathrm{can} \exp(-β\hat{H}^S)$. Here, we discuss the analogous statement and related questions for the \emph{grand-canonical} density matrix $\hatρ_\mathrm{gc}=Z^{-1}_\mathrm{gc} \exp(-β(\hat{H}^S-μ_1 \hat{N}_{1}^S-\ldots-μ_r\hat{N}_{r}^S))$ with $\hat{N}_{i}^S$ the number operator for molecules of type $i$ in the system $S$. This includes (i) the case of chemical reactions and (ii) that of systems $S$ defined by a spatial region which particles may enter or leave. It includes the statements (a) that the density matrix of the appropriate (generalized micro-canonical) Hilbert subspace $H_\mathrm{gmc} \subset H^S \otimes H^B$ (defined by a micro-canonical interval of total energy and suitable particle number sectors), after tracing out $B$, yields $\hatρ_\mathrm{gc}$; (b) that typical $Ψ$ from $H_\mathrm{gmc}$ have reduced density matrix $\hatρ^S_Ψ$ close to $\hatρ_\mathrm{gc}$; and (c) that the conditional wave function $ψ^S$ of $S$ has probability distribution $\mathrm{GAP}_{\hatρ_\mathrm{gc}}$ if a typical orthonormal basis of $H^B$ is used. That is, we discuss the foundation and justification of both the density matrix and the distribution of the wave function in the grand-canonical case. We also extend these considerations to the so-called generalized Gibbs ensembles, which apply to systems for which some macroscopic observables are conserved.

</details>
