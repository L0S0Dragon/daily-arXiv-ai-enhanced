<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 60]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Eigenstate condensation in quantum systems with finite-dimensional Hilbert spaces](https://arxiv.org/abs/2601.18869)
*Christopher David White,Michael Winer,Noam Bernstein*

Main category: quant-ph

TL;DR: Random quantum states constrained by energy expectation value exhibit eigenstate condensation with three phases: ground-state phase (macroscopic overlap with ground state), high-temperature phase (exponentially small overlap), and anti-ground-state phase (macroscopic overlap with highest excited state).


<details>
  <summary>Details</summary>
Motivation: To understand the phenomenon of eigenstate condensation in finite-dimensional quantum systems, where energy-constrained random states develop macroscopic overlap with extremal eigenstates, and to characterize the phase structure and transitions between different condensation regimes.

Method: Study random quantum states drawn from Haar ensemble with constraint on energy expectation value E_av = ⟨ψ|H|ψ⟩ in systems with finite-dimensional Hilbert spaces. Analyze overlap with energy eigenstates as function of E_av and system size, focusing on local spin systems.

Result: Three distinct phases identified: (1) ground-state phase for E_av < E_c with macroscopic overlap with ground state, (2) high-temperature phase with exponentially small overlap with each eigenstate, and (3) anti-ground-state phase with macroscopic overlap with most highly excited state. In local spin systems, ground-state and anti-ground-state phases approach middle of spectrum as 1/[system size], but phase transitions exhibit exponential finite-size scaling making crossover exponentially sharp.

Conclusion: Eigenstate condensation in finite-dimensional systems reveals rich phase structure with sharp transitions. The high-temperature phase should be understood as an extended phase due to exponential scaling of transitions, despite extremal phases approaching middle of spectrum polynomially with system size.

Abstract: Random quantum states drawn from the Haar ensemble with a constraint on the energy expectation value $E_{\mathrm{av}} = \langle ψ| H | ψ\rangle$ display \textit{eigenstate condensation}: for $E_{\mathrm{av}}$ below a critical value $E_c$, they develop macroscopic overlap with the ground state. We study eigenstate condensation in systems with finite-dimensional Hilbert spaces. These systems display three phases: a ground-state phase, in which energy-constrained random states have macroscopic overlap with the ground state; a high-temperature phase, in which they have exponentially small overlap with each energy eigenstate; and an anti-ground-state phase, in which they have macroscopic overlap with the most highly excited state. In local spin systems the ground-state and anti-ground-state phases approach the middle of the spectrum as $1/[\text{system size}]$, but -- because the condensation phase transitions have exponential, rather than polynomial, finite-size scaling -- the crossover becomes exponentially sharp in system size and the high-temperature phase is best understood as an extended phase.

</details>


### [2] [Quantum Light Detection with Enhanced Photonic Neural Network](https://arxiv.org/abs/2601.19721)
*Stanisław Świerczewski,Dogyun Ko,Amir Rahmani,Juan Camilo López Carreño,Wouter Verstraelen,Piotr Deuar,Barbara Piętka,Timothy C. H. Liew,Michał Matuszewski,Andrzej Opala*

Main category: quant-ph

TL;DR: Hybrid quantum-classical detection protocol combining quantum reservoir computing with analog neural networks enhances photonic quantum sensors, achieving improved performance with minimal nonlinearity and small network size.


<details>
  <summary>Details</summary>
Motivation: Address limitations in practical realization of photonic quantum reservoirs due to weak optical nonlinearities in available materials and challenges in fabricating densely coupled quantum networks, while meeting demand for high-precision, versatile, and scalable optical quantum state sensors.

Method: Introduce hybrid quantum-classical detection protocol integrating quantum reservoirs with adaptive learning capabilities of analog neural networks, using synergistic architecture to enhance information-extraction accuracy and robustness.

Result: Achieved significant improvements in quantum state classification, tomography, and feature regression with reservoirs having small nonlinearity-to-losses ratio (U/γ≈0.02) in only five-node networks, enabling low-cost performance improvements.

Conclusion: Reduces reliance on material nonlinearity and reservoir size, facilitating practical deployment of high-fidelity photonic quantum sensors on existing integrated platforms, paving way toward chip-scale quantum processors and photonic sensing technologies.

Abstract: Advances in quantum technologies are accelerating the demand for optical quantum state sensors that combine high precision, versatility, and scalability within a unified hardware platform. Quantum reservoir computing offers a powerful route toward this goal by exploiting the nonlinear dynamics of quantum systems to process and interpret quantum information efficiently. Photonic neural networks are particularly well suited for such implementations, owing to their intrinsic sensitivity to photon-encoded quantum information. However, the practical realisation of photonic quantum reservoirs remains constrained by the inherently weak optical nonlinearities of available materials and the technological challenges of fabricating densely coupled quantum networks. To address these limitations, we introduce a hybrid quantum-classical detection protocol that integrates the advantages of quantum reservoirs with the adaptive learning capabilities of analogue neural networks. This synergistic architecture substantially enhances information-extraction accuracy and robustness, enabling low-cost performance improvements of quantum light sensors. Based on the proposed approach, we achieved significant improvements in quantum state classification, tomography, and feature regression, even for reservoirs with a relatively small nonlinearity-to-losses ratio $U/γ\approx 0.02$ in a network of only five nodes. By reducing reliance on material nonlinearity and reservoir size, the proposed approach facilitates the practical deployment of high-fidelity photonic quantum sensors on existing integrated platforms, paving the way toward chip-scale quantum processors and photonic sensing technologies.

</details>


### [3] [Interaction-Conditional Semantics and the Dissolution of Quantum Paradoxes](https://arxiv.org/abs/2601.18810)
*Jonathon Sendall*

Main category: quant-ph

TL;DR: The paper argues that quantum puzzles arise from semantic errors in treating measurement outcomes as intrinsic properties rather than configuration-relative relations, and proposes relational objectivity principles that dissolve these puzzles without new physics.


<details>
  <summary>Details</summary>
Motivation: To resolve persistent puzzles in quantum mechanics (spin measurement, double slit, entanglement, Wigner's friend) by identifying their common origin in semantic errors and the illicit promotion of interaction-conditional outcomes to intrinsic properties.

Method: Introduces four principles that license only configuration-relative predication, grounding outcomes in physical measurement geometry while preserving objectivity. Applies these principles uniformly to canonical quantum puzzles.

Result: Dissolves each quantum puzzle without requiring new physics or ad hoc interpretive machinery. Reframes Bell's theorem and Kochen-Specker theorem as constraints on permissible explanatory structure rather than dynamical mysteries.

Conclusion: Achieves relational objectivity that avoids both naive property realism and observer subjectivism, showing that intrinsic-outcome semantics is incompatible with empirical reality.

Abstract: This paper argues that several canonical puzzles in quantum mechanics, including spin measurement, the double slit, entanglement correlations, and Wigner's friend, share a common origin in a semantic error and the illicit promotion of interaction conditional outcomes to intrinsic properties. I introduce four principles that license only configuration relative predication, grounding outcomes in physical measurement geometry while preserving objectivity. Applying these principles uniformly dissolves each puzzle without new physics or ad hoc interpretive machinery. Bell's theorem and the Kochen-Specker theorem are reframed not as dynamical mysteries but as constraints on permissible explanatory structure, evidence that intrinsic-outcome semantics is incompatible with empirical reality. The result is a relational objectivity that avoids both naive property realism and observer subjectivism.

</details>


### [4] [A framework to evaluate the performance of Variational Quantum Algorithms](https://arxiv.org/abs/2601.18812)
*Ernesto Mamedaliev,Vladyslav Libov,Albert Nieto-Morales,Oskar Słowik,Arit Kumar Bishwas*

Main category: quant-ph

TL;DR: A framework for benchmarking Variational Quantum Algorithms on QUBO problems using feasibility, quality, and reproducibility metrics with Shannon entropy and quality diagrams.


<details>
  <summary>Details</summary>
Motivation: Benchmarking VQAs is challenging due to their stochastic nature and lack of standardized performance criteria, making systematic evaluation difficult for combinatorial optimization on NISQ devices.

Method: Proposes a general evaluation framework with three metrics: feasibility (solution validity), quality (solution optimality), and reproducibility (using Shannon entropy). Introduces quality diagrams visualizing success probability vs. computational resources, and a decision rule for algorithm selection under constraints.

Result: Applied framework to several VQAs using CVaR cost functions with different shot counts on a 16-qubit QUBO instance, demonstrating systematic benchmarking capabilities and supporting adaptive algorithm selection.

Conclusion: The framework provides a foundation for systematic VQA benchmarking and enables adaptive algorithm selection in hybrid quantum-classical workflows, addressing the standardization gap in VQA evaluation.

Abstract: Variational Quantum Algorithms (VQAs) are promising methods for solving combinatorial optimization problems on noisy intermediate-scale quantum (NISQ) devices. However, benchmarking VQAs is difficult due to their stochastic behavior and the lack of standardized performance criteria. This work introduces a general framework for evaluating VQAs applied to Quadratic Unconstrained Binary Optimization (QUBO) problems. The framework uses three complementary metrics: feasibility, quality, and reproducibility. It also introduces a quality diagram that visualizes trade-offs between success probability and computational resources. Reproducibility is formalized using Shannon entropy, and a decision rule is defined for selecting algorithms under resource constraints. As a demonstration, the framework is applied to several VQAs using Conditional Value at Risk (CVaR) cost functions and different shot counts on a 16-qubit QUBO instance. The results show how the framework supports systematic benchmarking and provides a foundation for adaptive algorithm selection in hybrid quantum-classical workflows.

</details>


### [5] [Lightweight Quantum-Enhanced ResNet for Coronary Angiography Classification: A Hybrid Quantum-Classical Feature Enhancement Framework](https://arxiv.org/abs/2601.18814)
*Jingsong Xia*

Main category: quant-ph

TL;DR: A Lightweight Quantum-Enhanced ResNet (LQER) improves coronary angiography image classification by combining classical ResNet18 features with quantum feature enhancement via parameterized quantum circuits, achieving over 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: Coronary angiography interpretation is operator-dependent, and conventional deep learning struggles with complex vascular morphology and fine-grained texture patterns. There's a need for more robust classification methods.

Method: Hybrid quantum-classical approach using pretrained ResNet18 as classical feature extractor, with parameterized quantum circuit (PQC) for quantum feature enhancement at high-level semantic feature space. Uses data re-uploading, entanglement structures, and residual fusion with classical features. End-to-end hybrid optimization with controlled qubit count.

Result: LQER outperformed classical ResNet18 baseline in accuracy, AUC, and F1-score on independent test set, achieving test accuracy exceeding 90%. Quantum enhancement improved discrimination of positive lesions, especially under class-imbalanced conditions.

Conclusion: Validates practical hybrid quantum-classical learning paradigm for coronary angiography analysis, providing feasible pathway for deploying quantum machine learning in medical imaging applications.

Abstract: Background: Coronary angiography (CAG) is the cornerstone imaging modality for evaluating coronary artery stenosis and guiding interventional decision-making. However, interpretation based on single-frame angiographic images remains highly operator-dependent, and conventional deep learning models still face challenges in modeling complex vascular morphology and fine-grained texture patterns.Methods: We propose a Lightweight Quantum-Enhanced ResNet (LQER) for binary classification of coronary angiography images. A pretrained ResNet18 is employed as a classical feature extractor, while a parameterized quantum circuit (PQC) is introduced at the high-level semantic feature space for quantum feature enhancement. The quantum module utilizes data re-uploading and entanglement structures, followed by residual fusion with classical features, enabling end-to-end hybrid optimization with a strictly controlled number of qubits.Results: On an independent test set, the proposed LQER outperformed the classical ResNet18 baseline in accuracy, AUC, and F1-score, achieving a test accuracy exceeding 90%. The results demonstrate that lightweight quantum feature enhancement improves discrimination of positive lesions, particularly under class-imbalanced conditions.Conclusion: This study validates a practical hybrid quantum--classical learning paradigm for coronary angiography analysis, providing a feasible pathway for deploying quantum machine learning in medical imaging applications.

</details>


### [6] [Phase Diagrams of Information Backflow: Unifying Entanglement Revivals and Entropy Overshoots in Minimal Non-Markovian Models](https://arxiv.org/abs/2601.18822)
*Koichi Nakagawa*

Main category: quant-ph

TL;DR: The paper proposes a unified framework for comparing quantum and classical non-Markovian memory effects through information-backflow phase diagrams, using a common backflow functional to quantify both quantum entanglement revivals and classical entropy overshoots.


<details>
  <summary>Details</summary>
Motivation: Current approaches to diagnosing non-Markovian dynamics use either quantum-correlation revivals or non-monotonic classical information measures separately, lacking a unified minimal framework to compare their "backflow phases" on equal footing.

Method: Develops an information-backflow phase-diagram approach using a common backflow functional N_I = ∫_{İ>0} İ dt. On quantum side: fractional (Caputo) extension of two-state dissipative model with thermo-field dynamics, yielding closed-form intrinsic entanglement component. On classical side: three-state model with Markov generator promoted to exponential-kernel generalized master equation or semi-Markov process with Erlang-2 waiting times, plus fractional Mittag-Leffler memory kernel introduction.

Result: Quantum analysis shows sharp boundary near α ≃ 1/2 in (α, ω/λ) plane for integrated revival measure N_qe. Classical analysis reveals analogous backflow transition around α ≃ 1/2 when using fractional Mittag-Leffler memory kernel, indicating boundary originates from kernel's mathematical structure rather than quantumness per se.

Conclusion: Provides compact, model-agnostic route to classify non-Markovianity via phase diagrams of information backflow, with shared embedding narrative: memory stored in hidden degrees of freedom returns to observed sector as non-monotonic information flow, establishing quantum-classical symmetry in memory effects.

Abstract: Memory effects in non-Markovian dynamics are often diagnosed either via quantum-correlation revivals or via non-monotonic classical information measures, yet a unified minimal framework comparing their ``backflow phases'' is still lacking. Here we propose an information-backflow phase-diagram approach that places \emph{quantum entanglement revivals} and \emph{classical entropy overshoots} on the same footing through a common backflow functional $N_I=\int_{\dot I>0}\dot I\,dt$. On the quantum side, we employ a fractional (Caputo) extension of a two-state dissipative model embedded by thermo-field dynamics (TFD), yielding a closed-form intrinsic entanglement component $b^{(α)}_{qe}(t)=\frac14[E_α(-λ^αt^α)]^2\sin^2(ωt)$ and an integrated revival measure $N_{qe}$ that delineates a sharp boundary near $α\simeq 1/2$ in the $(α,ω/λ)$ plane. On the classical side, we consider a three-state model whose Markov generator is promoted either to an exponential-kernel generalized master equation (with exact Markov embedding) or to a semi-Markov process with Erlang-2 waiting times. We quantify non-monotonicity by the entropy overshoot $ΔH$ and KL-based diagnostics on the probability simplex. To strengthen the quantum--classical symmetry, we further introduce a \emph{fractional Mittag--Leffler memory kernel} in the classical dynamics and show that an analogous backflow transition emerges around $α\simeq 1/2$, indicating that the boundary originates from the kernel's mathematical structure rather than from quantumness per se. Overall, our results provide a compact, model-agnostic route to classify non-Markovianity by phase diagrams of information backflow and to interpret them via a shared embedding narrative: memory stored in hidden degrees of freedom returns to the observed sector as non-monotonic information flow.

</details>


### [7] [Feshbach-Villars Hamiltonian Approach to the Klein-Gordon Oscillator and Supercritical Step Scattering in Standard and Generalized Doubly Special Relativity](https://arxiv.org/abs/2601.18836)
*A. Boumali,N. Jafari,Y. Chargui*

Main category: quant-ph

TL;DR: A first-order Feshbach-Villars Hamiltonian framework for spin-0 relativistic quantum dynamics with Planck-scale kinematic deformations from generalized doubly special relativity, applied to Klein-Gordon oscillator and scattering problems.


<details>
  <summary>Details</summary>
Motivation: To develop a consistent Hamiltonian framework for studying relativistic quantum dynamics with Planck-scale kinematic deformations described by generalized doubly special relativity (G-DSR), enabling analysis of quantum systems like oscillators and scattering problems in the presence of quantum gravity effects.

Method: Develop a first-order Feshbach-Villars Hamiltonian framework starting from generic nonlinear momentum-space maps, derive modified dispersion relations at leading order in Planck length, construct consistent FV linearization of deformed Klein-Gordon operator, and maintain σ₃-pseudo-Hermitian structure at O(lₚ).

Result: The Hamiltonian remains σ₃-pseudo-Hermitian at O(lₚ), ensuring conservation of FV charge and current. For the Klein-Gordon oscillator, controlled O(lₚ) branch-resolved spectral shifts show how deformations reshape level spacing and high-energy spectral compression. For scattering, deformation-induced shift of supercritical (pair-production) threshold is quantified, with MS-type deformations delaying onset of supercritical regime and reducing negative transmitted flux.

Conclusion: The Feshbach-Villars framework provides a consistent Hamiltonian approach for studying relativistic quantum systems with Planck-scale kinematic deformations, enabling quantitative analysis of quantum gravity effects on oscillator spectra and scattering phenomena, with specific predictions about how different G-DSR realizations affect supercritical thresholds.

Abstract: We develop a first-order Feshbach-Villars (FV) Hamiltonian framework for spin-0 relativistic quantum dynamics in the presence of Planck-scale kinematic deformations described within generalized doubly special relativity (G-DSR). Starting from a generic nonlinear momentum-space map, we derive the corresponding modified dispersion relation (MDR) at leading order in the Planck length \(l_p\) and construct a consistent FV linearization of the deformed Klein-Gordon operator. The resulting two-component Hamiltonian remains \(σ_3\)-pseudo-Hermitian at \(\mathcal{O}(l_p)\), which guarantees conservation of the FV charge and current and provides a current-based definition of reflection and transmission in stationary scattering.
  As applications, we study two benchmark settings in which the FV metric structure is essential: (i) the one-dimensional Klein-Gordon oscillator and (ii) scattering from electrostatic step and barrier potentials. For the oscillator, we obtain controlled \(\mathcal{O}(l_p)\) branch-resolved spectral shifts and show how kinetic versus mass-shell deformations reshape the level spacing and the high-energy spectral compression. For step and barrier scattering, we compute reflection and transmission coefficients directly from the pseudo-Hermitian FV current and quantify the deformation-induced shift of the supercritical (pair-production) threshold. A comparative analysis of the Amelino-Camelia and Magueijo-Smolin realizations indicates that MS-type deformations generally delay the onset of the supercritical regime and reduce the magnitude of the negative transmitted flux within the validity domain \(l_p E \ll 1\).

</details>


### [8] [Quantum Simulation of the Polaron-Molecule Transition on a NISQ Device](https://arxiv.org/abs/2601.18839)
*Hugo Catala,Ezequiel Valero,German Rodrigo*

Main category: quant-ph

TL;DR: Quantum simulation framework for Fermi polarons and BEC-BCS crossover using gate-based quantum processors, validated on quantum hardware with hybrid variational methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of simulating strongly correlated fermionic systems due to exponential Hilbert space growth and fermionic sign problem, particularly for Fermi polarons and BEC-BCS crossover physics.

Method: Develop unified Hamiltonian formalism bridging pairing superfluidity and impurity physics, map to quantum processor via Jordan-Wigner transformation, implement first-order Trotter-Suzuki decomposition, and use Ramsey interferometry protocol for real-time dynamics and spectral response.

Result: Demonstrate smooth transition from dressed quasiparticle (polaron) regime to stable molecular bound state with linear energy renormalization in strong-coupling limit; validate against exact classical benchmarks; successfully execute on Barcelona Supercomputing Center quantum hardware; hybrid variational approach shows resilience to noise.

Conclusion: Digital quantum simulation framework successfully captures complex fermionic physics including Fermi polarons and BEC-BCS crossover, demonstrating practical implementation on noisy quantum hardware with hybrid variational methods providing noise resilience.

Abstract: The simulation of strongly correlated fermionic systems remains one of the most significant challenges in computational physics due to the exponential growth of the Hilbert space and the fermionic sign problem. In this work, we present a digital quantum simulation framework to explore the Fermi polaron and the Bose-Einstein Condensate (BEC) to Bardeen-Cooper-Schrieffer (BCS) crossover. We develop a unified Hamiltonian formalism that bridges pairing superfluidity and impurity physics, mapping the system onto a gate-based quantum processor via the Jordan-Wigner transformation. Using a first-order Trotter-Suzuki decomposition, we implement a Ramsey interferometry protocol to extract the real-time dynamics and spectral response of the system.
  Our results demonstrate a smooth transition from a dressed quasiparticle (polaron) regime to a stable molecular bound state, characterized by a linear energy renormalization in the strong-coupling limit. We validate our simulation against exact classical benchmarks and report successful execution on the Barcelona Supercomputing Center quantum hardware. Despite the inherent noise of the quantum hardware, the hybrid variational approach shows remarkable resilience, accurately capturing the bifurcation of the spectral density

</details>


### [9] [Operationally induced preferred basis in unitary quantum mechanics](https://arxiv.org/abs/2601.18856)
*Vitaly Pronskikh*

Main category: quant-ph

TL;DR: The paper addresses measurement problems in quantum mechanics by showing that the preferred-basis and definite-outcome issues persist even with unitary detector models, proposing that measurement outcomes emerge from the interface between continuous unitary symmetry and discrete Boolean event algebras, with probabilities uniquely determined by Gleason-type theorems.


<details>
  <summary>Details</summary>
Motivation: To resolve the persistent measurement problem in quantum mechanics, particularly the preferred-basis problem and definite-outcome issue, which remain even when detectors are modeled unitarily. The motivation stems from the fundamental mismatch between the continuous unitary symmetry of quantum theory and the discrete Boolean event algebra required for experimental records.

Method: The paper employs mathematical analysis of the interface between group-based kinematics (continuous unitary symmetry) and set-based counting (Boolean event algebras). It uses Gleason-type uniqueness theorems (Gleason for d>2 and Busch's extension for POVMs including d=2) to establish the trace rule as the unique probability measure. A compact qubit-pointer model demonstrates how detectors induce measurement bases through unsharp POVMs, and a non-composability lemma addresses nested-observer paradoxes.

Result: The measurement basis for recorded outcomes is not determined solely by the system Hamiltonian but is induced by the measurement mapping (detector channel plus coarse-grained readout). The trace rule emerges uniquely from symmetry and measure theory requirements. A qubit-pointer model yields an induced unsharp POVM E± = ½(I ± ησ_z) with η fixed by pointer resolution, explicitly showing detector-induced basis selection. Nested-observer paradoxes are resolved through a non-composability lemma showing joint outcome assignment requires joint instruments.

Conclusion: The core of the measurement problem lies in the necessary interface ('cut') between continuous unitary symmetry and discrete Boolean event algebras. Randomness originates from the stochasticity of transition rules rather than fundamental indeterminism, with measurement outcomes emerging from the structural requirements of experimental recording rather than being intrinsic to quantum systems alone.

Abstract: The preferred-basis problem and the definite-outcome aspect of the measurement problem persist even if the detector is modeled unitarily, because experimental data are necessarily represented in a Boolean event algebra of mutually exclusive records whereas the theoretical description is naturally formulated in a noncommutative operator algebra with continuous unitary symmetry. This change of mathematical type constitutes the core of the 'cut': a structurally necessary interface from group-based kinematics to set-based counting. In the presented view the basis relevant for recorded outcomes is not determined by the system Hamiltonian alone; it is induced by the measurement mapping, i.e., by the detector channel together with the coarse-grained readout that defines an instrument. The probabilistic mapping is anchored in symmetry and measure theory: by Gleason-type uniqueness (Gleason for projections in $d>2$ and Busch's extension for Positive Operator-Valued Measures (POVMs) including $d=2$), the trace rule is the unique probability measure consistent with additivity over exclusive events and basis-independence of the unitary sector. A compact qubit--pointer model yields an induced unsharp POVM $E_\pm=\tfrac12(\id\pm η\,σ_z)$ with $η$ fixed by pointer resolution, displaying explicitly how the detector induces the relevant basis. Finally, nested-observer paradoxes are tightened into a non-composability lemma: joint assignment of outcome propositions is obstructed unless a joint instrument exists. This relocates the origin of randomness to the stochasticity of the transition rules.

</details>


### [10] [Post-selection games](https://arxiv.org/abs/2601.18861)
*Víctor Calleja Rodríguez,Ivan A. Bocanegra-Garay,Mateus Araújo*

Main category: quant-ph

TL;DR: Post-selection games generalize nonlocal games by allowing rounds to be discarded, formalizing possibilistic proofs of nonlocality like Hardy's paradox, with algorithms for computing bounds and showing unbounded statistical power advantages for low-efficiency Bell tests.


<details>
  <summary>Details</summary>
Motivation: To formalize possibilistic proofs of nonlocality (like Hardy's paradox) where some experimental rounds are discarded, and to address Bell tests with low detection efficiency where traditional nonlocal games have limitations.

Method: Introduce post-selection games as a generalization of nonlocal games where rounds can be won, lost, or discarded by the referee. Develop algorithms for computing local and Tsirelson bounds of these games.

Result: Post-selection games have unbounded advantage in statistical power over traditional nonlocal games, making them ideally suited for analyzing Bell tests with low detection efficiency.

Conclusion: Post-selection games provide a powerful framework for analyzing quantum nonlocality in practical experimental settings with imperfect detection, formalizing possibilistic approaches and offering superior statistical power.

Abstract: In this paper, we introduce post-selection games, a generalization of nonlocal games where each round can be not only won or lost by the players, but also discarded by the referee. Such games naturally formalize possibilistic proofs of nonlocality, such as Hardy's paradox. We develop algorithms for computing the local and Tsirelson bounds of post-selection games. Furthermore, we show that they have an unbounded advantage in statistical power over traditional nonlocal games, making them ideally suited for analysing Bell tests with low detection efficiency.

</details>


### [11] [Experimental Demonstration of Commutation Relations Using Intensity Correlations](https://arxiv.org/abs/2601.18870)
*Hans Dang,Sebastian Luff,Martin Fischer,Markus Sondermann,Mojdeh. S. Najafabadi,Luis L. Sanchez-Soto,Gerd Leuchs*

Main category: quant-ph

TL;DR: Experimental verification of bosonic commutation relation for optical field operators using intensity correlation measurements, confirming quantum theory predictions for both single-photon and coherent states.


<details>
  <summary>Details</summary>
Motivation: The canonical commutation relation is fundamental to quantum theory and underpins the Heisenberg uncertainty principle, but while uncertainty relations have been extensively tested, direct experimental verification of the underlying commutation relation itself has remained elusive.

Method: Experimental demonstration based on measurements of two distinct intensity correlation functions to extract the expectation value of the field-operator commutator for both single-photon and coherent states.

Result: Measured values of the field-operator commutator expectation value are consistent with unity for both single-photon and coherent states, in quantitative agreement with quantum theory predictions.

Conclusion: The study provides direct experimental verification of the bosonic commutation relation for optical field operators, confirming a fundamental cornerstone of quantum theory that had previously only been indirectly tested through uncertainty relations.

Abstract: The canonical commutation relation is a cornerstone of quantum theory and underlies the Heisenberg uncertainty principle. Although uncertainty relations have been extensively tested, direct verifications of the underlying commutation relation itself have remained elusive. We report an experimental demonstration of the bosonic commutation relation for optical field operators based on measurements of two distinct intensity correlation functions. From these measurements, we extract the expectation value of the field-operator commutator for both a single-photon state and coherent state. In both cases, the measured values are consistent with unity, in quantitative agreement with quantum theory.

</details>


### [12] [Against probability: A quantum state is more than a list of probability distributions](https://arxiv.org/abs/2601.18872)
*Ladina Hausmann,Renato Renner*

Main category: quant-ph

TL;DR: Quantum state probability representations face an unavoidable tension: topological robustness (preserving closeness) conflicts with preserving essential structure like subsystem composition.


<details>
  <summary>Details</summary>
Motivation: Quantum states are often represented by probability vectors from measurement outcomes, appearing in quantum field theory (correlation functions) and quantum foundations (generalized probabilistic theories). However, there's a fundamental tension between making operationally meaningful statements (requiring topological robustness) and preserving essential quantum structure.

Method: Analyzes the mathematical properties of quantum state representations via probability vectors $\mathbf{P}_{\mathcal{M}}(ρ)$ for measurement sets $\mathcal{M}$. Examines the topological requirements for operational meaningfulness versus structural preservation requirements.

Result: Identifies an unavoidable trade-off: a probability representation that is topologically robust (preserving closeness between states) cannot simultaneously retain other essential quantum structure, such as subsystem composition.

Conclusion: There exists a fundamental tension in quantum probability representations between topological robustness needed for operational meaningfulness and preservation of essential quantum structure like subsystem composition, representing a no-go result for certain representation approaches.

Abstract: The state $ρ$ of a quantum system can be represented by a vector $\mathbf{P}_{\mathcal{M}}(ρ)$ of outcome probabilities for a set of measurements $\mathcal{M}$. Such representations appear throughout physics, for example, in quantum field theory via correlation functions and in quantum foundations within generalized probabilistic frameworks. In this work, we identify an unavoidable tension: to enable operationally meaningful statements, the map ${ρ\mapsto \mathbf{P}_{\mathcal{M}}(ρ)}$ must be topologically robust $\unicode{x2013}$ preserving the notion of closeness between states. Yet, a probability representation that is topologically robust cannot simultaneously retain other essential structure, such as the subsystem structure.

</details>


### [13] [Multivariate Multicycle Codes for Complete Single-Shot Decoding](https://arxiv.org/abs/2601.18879)
*Feroz Ahmed Mian,Owen Gwilliam,Stefan Krastanov*

Main category: quant-ph

TL;DR: Multivariate multicycle (MM) codes are a new family of quantum CSS codes that generalize several existing code families, featuring metachecks and high confinement for complete single-shot decoding while potentially enabling logical non-Clifford gates.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework for quantum error correcting codes that combines the advantages of single-shot decoding capability with algebraic structure for logical gate operations, addressing limitations of existing code families.

Method: Construct MM codes from length-t chain complexes (t ≥ 4) using Koszul complex framework, which simplifies obtaining explicit boundary maps (parity check and metacheck matrices). Perform numerical search to identify promising code candidates.

Result: Identified several high-performance MM code candidates with parameters including [[96,12,8]], [[216,12,12]], [[486,24,12]], [[648,60,9]], etc. These codes achieve superior confinement profiles surpassing all known single-shot decodable quantum CSS codes of practical blocksize.

Conclusion: MM codes provide a general parameterization that unifies multiple quantum code families, offering both single-shot decoding capability and potential for logical non-Clifford gates, with demonstrated high rates and distances in practical code sizes.

Abstract: We introduce multivariate multicycle (MM) codes, a new family of quantum error correcting codes that unifies and generalizes bivariate bicycle codes, multivariate bicycle codes, abelian two-block group algebra codes, generalized bicycle codes, trivariate tricycle codes, and n-dimensional toric codes. MM codes are Calderbank-Shor-Steane (CSS) codes defined from length-t chain complexes with $t \ge 4$. The chief advantage of these codes is that they possess metachecks and high confinement that permit complete single-shot decoding, while also having additional algebraic structure that might enable logical non-Clifford gates. We offer a framework that facilitates the construction of long-length chain complexes through the use of Koszul complex. In particular, obtaining explicit boundary maps (parity check and metacheck matrices) is particularly straightforward in our approach. This simple but very general parameterization of codes permitted us to efficiently perform a numerical search, where we identify several MM code candidates that demonstrate these capabilities at high rates and high code distances. Examples of new codes with parameters $[[n,k,d]]$ include $[[96, 12, 8]]$, $[[96, 44, 4]]$ $[[144, 40, 4]]$, $[[216, 12, 12]]$, $[[360, 30, 6]]$, $[[384, 80, 4]]$, $[[486, 24, 12]]$, $[[486, 66, 9]]$ and $[[648, 60, 9]]$. Notably, our codes achieve confinement profiles that surpass all known single-shot decodable quantum CSS codes of practical blocksize.

</details>


### [14] [Fault-tolerant quantum simulation of the Pauli-Breit Hamiltonian for ab initio hybrid quantum-classical molecular design with applications to photodynamic therapy](https://arxiv.org/abs/2601.18898)
*Emil Zak*

Main category: quant-ph

TL;DR: Quantum algorithm for simulating relativistic spin effects in molecules using Pauli-Breit Hamiltonian with efficient LCU circuits and spin-controlled SWAP networks, reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Relativistic spin effects are crucial for molecular phenomena like intersystem crossing in photodynamic therapy and spin-mediated catalysis, but classical simulation of the full Pauli-Breit Hamiltonian becomes intractable due to exponential Hilbert space growth and complex two-body spin terms.

Method: Fault-tolerant quantum algorithm using block-encoding of relativistic Hamiltonian in second-quantized, doubly factorized representation; reformulation in symmetry-adapted Majorana basis; efficient linear-combination-of-unitaries (LCU) circuits for spin-orbit interactions; spin-controlled Pauli-SWAP networks to decouple spin and orbital control logic.

Result: Explicit spin degrees of freedom do not worsen asymptotic scaling; prefactor reduced by factor of two compared to direct LCU approaches; enables unified treatment of relativistic spin mixing with modest overhead relative to spin-free simulations.

Conclusion: Proposed quantum algorithm enables accurate simulation of relativistic spin effects for applications like photodynamic therapy photosensitizers and artificial photosynthesis catalysts, overcoming classical computational limitations.

Abstract: Relativistic spin effects drive subtle molecular phenomena ranging from intersystem crossing in photodynamic therapy to spin-mediated catalysis and high-resolution spectroscopy. These effects are described by the Pauli-Breit Hamiltonian, which extends the nonrelativistic electronic Hamiltonian by including one- and two-electron spin-orbit and spin-spin interactions. First-principles simulations of the full Pauli-Breit Hamiltonian rapidly become intractable on classical computers due to the exponential growth of the Hilbert space and the complexity of two-body spin-dependent terms. We propose a fault-tolerant quantum algorithm for computing molecular energy levels and properties governed by the Pauli-Breit Hamiltonian. Our approach block-encodes the relativistic Hamiltonian in a second-quantized, doubly factorized representation. By reformulating the Hamiltonian in a symmetry-adapted Majorana basis, we construct efficient linear-combination-of-unitaries circuits that encode spin-orbit interactions without effective or mean-field approximations. We introduce spin-controlled Pauli-SWAP networks that decouple spin and orbital control logic, enabling a unified treatment of relativistic spin mixing with only modest overhead relative to spin-free simulations. We analyze quantum resources in terms of logical qubits and T-gate complexity, showing that explicit spin degrees of freedom do not worsen the asymptotic scaling. The prefactor is reduced by a factor of two compared to direct linear-combination-of-unitaries approaches. Finally, we outline a hybrid quantum-classical workflow for designing photodynamic therapy photosensitizers, artificial photosynthesis catalysts, and other systems where accurate relativistic spin effects are essential.

</details>


### [15] [Complete transparency with three active-passive-coupled optical resonators](https://arxiv.org/abs/2601.18937)
*Xiao-Bo Yan,Liu Yang,Bing He*

Main category: quant-ph

TL;DR: Complete optical transparency achieved in a three-resonator system with active gain and passive dissipation, enabling 100% transmission with tunable transparency window and output intensity.


<details>
  <summary>Details</summary>
Motivation: Existing induced transparency phenomena like EIT require demanding experimental conditions and have limitations in achieving near-100% transmission. The paper aims to demonstrate complete optical transparency using a simpler resonator structure.

Method: Uses a structure of three linearly coupled optical resonators: one active resonator with arbitrary optical gain and two passive resonators with dissipation. Achieves complete transparency through destructive interference that annihilates the intracavity field in the resonator directly coupled to the input.

Result: Demonstrates complete transparency exists for any feasible power of transmitted field and all realizable coupling strengths, as long as inter-cavity coupling between the other two resonators is adjustable. Enables free control of transparency window size and output field intensity by tuning two inter-cavity couplings.

Conclusion: The three-resonator system with active gain and passive dissipation provides a practical platform for achieving complete optical transparency with tunable properties, overcoming limitations of traditional EIT systems while displaying similar properties.

Abstract: The phenomena of induced transparency, with the typical examples of electromagnetically induced transparency (EIT) in atomic media and coupled optical resonators, have attracted tremendous interest since their discoveries. Due to the limitations of the involved elements, however, near-100\% transmissions were reported under highly demanding experimental conditions for atomic and other media. Based on a structure of three linearly coupled optical resonators, an active one carrying a possibly arbitrary optical gain and two passive ones simply with dissipation, we demonstrate that a transmitted light field can become completely transparent through the structure, which displays all properties similar to those of EIT. Manifested by a destructive interference to annihilate the intracavity field in the resonator directly coupled to the input, this complete transparency exists for any feasible power of the transmitted field and all realizable coupling strengths of the dark resonator with the input and the neighboring resonator, as long as the inter-cavity coupling for two other resonators is adjustable over a suitable range. A free control on the transparency window size and output field intensity can be realized by tuning two inter-cavity couplings without modifying the built-in system parameters.

</details>


### [16] [Krylov's State Complexity and Information Geometry in Qubit Dynamics](https://arxiv.org/abs/2601.18941)
*Carlo Cafaro,Emma Clements,Vishnu Vardhan Anuboyina*

Main category: quant-ph

TL;DR: Krylov state complexity and information-geometric complexity measure different aspects of qubit dynamics on the Bloch sphere, with Krylov complexity quantifying directional spread and IG complexity capturing effective trajectory volume.


<details>
  <summary>Details</summary>
Motivation: To compare and contrast two different measures of quantum complexity - Krylov's state complexity and information-geometric complexity - for understanding quantum evolution of two-level systems, specifically analyzing their geometric interpretations and fundamental differences.

Method: Formulated Krylov complexity in geometric terms (instantaneous and time-averaged) for qubit dynamics on the Bloch sphere, analyzed evolutions generated by both stationary and nonstationary Hamiltonians (geodesic and nongeodesic trajectories), and contrasted with IG complexity characterized by efficiency and curvature.

Result: The two measures reflect fundamentally different aspects: Krylov complexity quantifies directional spread of evolving state relative to initial state, while IG complexity captures effective volume explored along the trajectory on the Bloch sphere, explaining their inequivalent behavior.

Conclusion: Krylov's state complexity and information-geometric complexity are complementary notions that capture distinct geometric aspects of quantum dynamics - directional spread versus trajectory volume - highlighting the multifaceted nature of complexity in quantum systems.

Abstract: We compare Krylov's state complexity with an information-geometric (IG) measure of complexity for the quantum evolution of two-level systems. Focusing on qubit dynamics on the Bloch sphere, we analyze evolutions generated by stationary and nonstationary Hamiltonians, corresponding to geodesic and nongeodesic trajectories. We formulate Krylov complexity in geometric terms, both instantaneously and in a time-averaged sense, and contrast it with an IG complexity of quantum evolutions characterized in terms of efficiency and curvature. We show that the two measures reflect fundamentally different aspects of quantum dynamics: Krylov's state complexity quantifies the directional spread of the evolving state relative to the initial state, whereas the IG complexity captures the effective volume explored along the trajectory on the Bloch sphere. This geometric distinction explains their inequivalent behavior and highlights the complementary nature of state-based and information-geometric notions of complexity in quantum systems.

</details>


### [17] [Reinforcement Learning for Quantum Technology](https://arxiv.org/abs/2601.18953)
*Marin Bukov,Florian Marquardt*

Main category: quant-ph

TL;DR: Reinforcement learning is becoming a key tool for solving quantum technology challenges, with applications spanning quantum state preparation, gate optimization, circuit design, feedback control, error correction, and quantum metrology.


<details>
  <summary>Details</summary>
Motivation: Quantum technology faces numerous complex challenges that require adaptive decision-making and optimization. Reinforcement learning offers a powerful framework for addressing these challenges through interactive learning with quantum systems, enabling automated solutions to problems that are difficult to solve with traditional methods.

Method: The paper provides a comprehensive review of reinforcement learning applications in quantum technology. It introduces RL concepts for a physics audience, then systematically surveys applications including: state preparation in few- and many-body systems, quantum gate design and optimization, automated quantum circuit construction (including variational quantum eigensolvers and architecture search), quantum feedback control, quantum error correction, quantum reinforcement learning, and quantum metrology.

Result: RL has demonstrated significant success across multiple quantum technology domains: enabling efficient state preparation, designing high-fidelity quantum gates, automating circuit construction, improving feedback control systems, enhancing error correction protocols, and advancing quantum metrology. Experimental implementations show RL's increasing practical impact on quantum technology development.

Conclusion: Reinforcement learning is emerging as a transformative tool for quantum technology, with demonstrated success across diverse applications. Key open challenges include scalability to larger quantum systems, interpretability of RL solutions, and better integration with experimental platforms. Future research should address these limitations while expanding RL's role in shaping quantum technology development.

Abstract: Many challenges arising in Quantum Technology can be successfully addressed using a set of machine learning algorithms collectively known as reinforcement learning (RL), based on adaptive decision-making through interaction with the quantum device. After a concise and intuitive introduction to RL aimed at a broad physics readership, we discuss the key ideas and core concepts in reinforcement learning with a particular focus on quantum systems. We then survey recent progress in RL in all relevant areas. We discuss state preparation in few- and many-body quantum systems, the design and optimization of high-fidelity quantum gates, and the automated construction of quantum circuits, including applications to variational quantum eigensolvers and architecture search. We further highlight the interactive capabilities of RL agents, emphasizing recent progress in quantum feedback control and quantum error correction, and briefly discuss quantum reinforcement learning as well as applications to quantum metrology. The review concludes with a discussion of open challenges -- such as scalability, interpretability, and integration with experimental platforms -- and outlines promising directions for future research. Throughout, we highlight experimental implementations that exemplify the increasing role of reinforcement learning in shaping the development of quantum technologies.

</details>


### [18] [Quantum capacity analysis of finite-dimensional lossy channels](https://arxiv.org/abs/2601.18960)
*Sofia Cocciaretto,Vittorio Giovannetti*

Main category: quant-ph

TL;DR: Analysis of quantum capacity for 4-dimensional multi-level amplitude damping (MAD) channels using techniques beyond degradable/antidegradable conditions, with characterization of degradability regions for generic d-dimensional MAD channels.


<details>
  <summary>Details</summary>
Motivation: Higher-dimensional quantum architectures (qudits) offer advantages over qubit-based systems for quantum communication and computation. Multi-level Amplitude Damping (MAD) channels model energy decay in qudit signal transmission, requiring analysis of their quantum capacity.

Method: Developed techniques for computing quantum capacity of 4-dimensional MAD channels outside degradable/antidegradable conditions. Used analytical and semi-numerical methods to characterize complete degradability/antidegradability regions in parameter space for generic d-dimensional MAD channels.

Result: Computed quantum capacity for 4-dimensional MAD channels and fully characterized the parameter regions where these channels are degradable or antidegradable for arbitrary dimension d.

Conclusion: The work provides analytical tools for evaluating quantum capacity of higher-dimensional amplitude damping channels and establishes complete degradability characterization, advancing understanding of qudit-based quantum communication systems.

Abstract: Traditionally, Quantum Information, and Quantum Communication specifically, have been focused on qubit-based architectures. Recent results, however, highlighted that higher dimensional architectures (qudit-based) may present advantages both in terms of communication and computation; a family of channels called Multi-level Amplitude Damping (MAD) channels, which are a possible qudit generalization of the well known Amplitude Damping Channels, is able to model energy decay processes that may happen during signal transmission. In this work, the Quantum Capacity of 4-dimensional MAD's is studied, relying on a technique for computing it even outside of degradable and antidegradable conditions. We also characterized the complete region of antidegradability and degradability in the parameter space for a generic d-dimensional MAD using both analytical and semi-numerical methods.

</details>


### [19] [Private Proofs of When and Where](https://arxiv.org/abs/2601.18961)
*Uma Girish,Greg Gluch,Shafi Goldwasser,Tal Malkin,Leo Orshansky,Henry Yuen*

Main category: quant-ph

TL;DR: Zero-knowledge position verification extends quantum position verification to prove complex location statements while maintaining privacy about other location details, constructed from standard position verification and post-quantum one-way functions using position commitments.


<details>
  <summary>Details</summary>
Motivation: Classical position verification protocols are insecure even with computational assumptions, while quantum protocols enable secure position verification. However, existing quantum position verification only proves simple location statements ("I am at location L") without privacy for other location details or ability to prove more complex temporal-spatial statements.

Method: Introduces zero-knowledge position verification that generalizes position verification in two ways: (1) enables proving sophisticated statements about locations at different times, (2) maintains privacy about other location details. Constructs this from standard position verification and post-quantum one-way functions using a new primitive called "position commitments" that allow entities to privately commit to physical positions at specific moments.

Result: The paper introduces the formal notion of zero-knowledge position verification and provides a construction that achieves this functionality. The central technical contribution is the development of position commitments as a cryptographic primitive that enables the privacy-preserving features while maintaining the security guarantees of quantum position verification.

Conclusion: Zero-knowledge position verification extends the capabilities of quantum position verification to enable privacy-preserving proofs of complex location statements, addressing limitations of existing protocols while maintaining security through quantum mechanisms and post-quantum cryptographic primitives.

Abstract: Position verification schemes are interactive protocols where entities prove their physical location to others; this enables interactive proofs for statements of the form "I am at a location $L$." Although secure position verification cannot be achieved with classical protocols (even with computational assumptions), they are feasible with quantum protocols.
  In this paper we introduce the notion of zero-knowledge position verification, which generalizes position verification in two ways:
  1. enabling entities to prove more sophisticated statements about their locations at different times (for example, "I was NOT near location $L$ at noon yesterday").
  2. maintaining privacy for any other detail about their true location besides the statement they are proving.
  We construct zero-knowledge position verification from standard position verification and post-quantum one-way functions. The central tool in our construction is a primitive we call position commitments, which allow entities to privately commit to their physical position in a particular moment, which is then revealed at some later time.

</details>


### [20] [Qubit-qudit entanglement transfer in defect centers with high-spin nuclei](https://arxiv.org/abs/2601.18976)
*W. -R. Hannes,Guido Burkard*

Main category: quant-ph

TL;DR: Proposes a scheme for accumulating entanglement between long-lived nuclear spin qudits in defect centers using electron spins as communication qubits and Ising hyperfine interactions for deterministic entanglement transfer.


<details>
  <summary>Details</summary>
Motivation: To develop a method for generating and accumulating entanglement between long-lived quantum memories (nuclear spin qudits) in defect centers, which are promising for quantum networks due to their long coherence times, overcoming limitations of using electron spins alone as memories.

Method: Uses electron spins as communication qubits to establish entanglement between nodes via spin-photon interfaces. Leverages the Ising component of hyperfine interaction to repeatedly transfer entanglement onto nuclear spin qudits (dimension d ≤ 2I+1, where I is nuclear spin quantum number). For d as integer powers of two, achieves deterministic maximal entanglement generation without intermittent nuclear spin driving.

Result: Demonstrates that deterministic maximal entanglement accumulation is possible for qudit dimensions that are integer powers of two. The scheme is applicable to various defect center systems, specifically mentioning the 73Ge germanium vacancy in diamond as a candidate implementation.

Conclusion: Provides a practical scheme for building quantum networks using defect centers with nuclear spin qudits as long-lived quantum memories, enabling deterministic entanglement generation and accumulation through available hyperfine interactions without requiring complex nuclear spin control.

Abstract: We propose a scheme for accumulating entanglement between long-lived qudits provided by central nuclear spins of defect centers. Assuming a generic setting, the electron spin of each node acts as the communication qubit and may be entangled with other nodes, e.g., through a spin-photon interface. The generally available Ising component of the hyperfine interaction is shown to facilitate repeated entanglement transfer onto memory qudits of arbitrary dimension $d\le 2I+1$ with $I$ the nuclear spin quantum number. When $d$ is set to an integer power of two, maximal entanglement can be generated deterministically and without intermittent driving of nuclear spins. The scheme is applicable to several candidate systems, including the $^{73}$Ge germanium vacancy in diamond.

</details>


### [21] [Time-series based quantum state discrimination](https://arxiv.org/abs/2601.19057)
*Samuel Jung,Neel Vora,Akel Hashim,Yilun Xu,Gang Huang*

Main category: quant-ph

TL;DR: Using machine learning on raw analog readout signals instead of integrated values improves quantum state classification by preserving temporal information that helps distinguish ground state from decayed states.


<details>
  <summary>Details</summary>
Motivation: Quantum state readout fidelity is limited by poor SNR and T1 decay, and traditional clustering methods on integrated signals cannot distinguish between qubits initially in ground state versus those that decayed to ground state during measurement.

Method: Apply time-series classification models (specifically LSTM networks) to raw, non-integrated analog signals, combined with filtering and feature engineering, instead of using clustering algorithms on integrated readout signals.

Result: LSTM model consistently outperforms clustering methods, with largest improvements from reclassifying points in boundary regions between clusters that correspond to atypical measurement records with transient/noisy features lost during integration.

Conclusion: Sequence-aware models like LSTMs better discriminate quantum state trajectories by retaining temporal information, whereas clustering methods based on integrated values are more prone to misclassification, especially for boundary cases.

Abstract: Accurate quantum state readout is crucial for error correction and algorithms, but measurement errors are detrimental. Readout fidelity is typically limited by a poor signal-to-noise ratio (SNR) and energy relaxation ($T_1$ decay), a significant problem for superconducting qubits. While most approaches classify results using clustering algorithms on integrated readout signals, these methods cannot distinguish a qubit that was initially in the ground state from one that decayed to it during measurement. We instead propose using machine learning (ML) on the raw, non-integrated analog signal. We apply time-series classification models, such as a long short-term memory (LSTM) network, to the full data trajectory. We find that our LSTM model, combined with filtering and feature engineering, consistently outperforms clustering. The largest improvements come from reclassifying points in the boundary regions between clusters. These points correspond to atypical measurement records, likely due to transient or noisy features lost during data integration. By retaining temporal information, sequence-aware models like LSTMs can better discriminate these trajectories, whereas clustering methods based on integrated values are more prone to misclassification.

</details>


### [22] [The cost of quantum algorithms for biochemistry: A case study in metaphosphate hydrolysis](https://arxiv.org/abs/2601.19059)
*Ryan LaRose,Alan Bidart,Ben DalFavero,Sophia E. Economou,J. Wayne Mullinax,Mafalda Ramôa,Jeremiah Rowland,Brenda Rubenstein,Nicolas PD Sawaya,Prateek Vaish,Grant M. Rotskoff,Norm M. Tubman*

Main category: quant-ph

TL;DR: Quantum resource analysis for ATP hydrolysis simulation shows variational methods require fewer quantum resources and could run on near-future devices.


<details>
  <summary>Details</summary>
Motivation: ATP/metaphosphate hydrolysis is a crucial biological reaction with implications for metabolism, cellular signaling, and cancer therapeutics. Understanding quantum computing requirements for simulating such biochemical problems is essential for advancing quantum applications in biology.

Method: Evaluated three quantum algorithms (variational quantum eigensolver, quantum Krylov, and quantum phase estimation) for ground state energy estimation of ATP hydrolysis. Used exact classical simulation, numerical estimation, and analytical bounds to assess quantum resource requirements.

Result: Variational methods require substantially fewer overall quantum resources despite being more heuristic. They could feasibly address such biochemical problems on current or near-future quantum devices. Complete dataset of biomolecular Hamiltonians and code provided as benchmarks.

Conclusion: Variational quantum algorithms offer the most practical near-term approach for simulating important biochemical reactions like ATP hydrolysis, with quantum Krylov and phase estimation requiring more resources but potentially offering higher accuracy for future fault-tolerant quantum computers.

Abstract: We evaluate the quantum resource requirements for ATP/metaphosphate hydrolysis, one of the most important reactions in all of biology with implications for metabolism, cellular signaling, and cancer therapeutics. In particular, we consider three algorithms for solving the ground state energy estimation problem: the variational quantum eigensolver, quantum Krylov, and quantum phase estimation. By utilizing exact classical simulation, numerical estimation, and analytical bounds, we provide a current and future outlook for using quantum computers to solve impactful biochemical and biological problems. Our results show that variational methods, while being the most heuristic, still require substantially fewer overall resources on quantum hardware, and could feasibly address such problems on current or near-future devices. We include our complete dataset of biomolecular Hamiltonians and code as benchmarks to improve upon with future techniques.

</details>


### [23] [Inverse-Squeezing Kennedy Receiver for Near-Helstrom Discrimination of Displaced-Squeezed BPSK](https://arxiv.org/abs/2601.19093)
*Enhao Bai,Jian Peng,Tianyi Wu,Chen Dong,Kai Wen,Fengkai Sun,Zhenrong Zhang,Chun Zhou,Yaping Li*

Main category: quant-ph

TL;DR: Proposed IS-Kennedy receiver uses inverse-squeezing and PNR detection to convert squeezing resources into displacement gain, achieving near-Helstrom bound performance with 3dB constant factor gap and beating coherent-state limit at low photon numbers.


<details>
  <summary>Details</summary>
Motivation: Address discrimination problem of binary phase-shift keyed displaced squeezed vacuum states (S-BPSK) by developing a receiver that can effectively utilize squeezing resources to improve quantum state discrimination performance.

Method: IS-Kennedy receiver architecture: adds inverse-squeezing operator after displacement operation of conventional Kennedy receiver, mapping S-BPSK signals onto equivalent large-amplitude coherent states, then uses photon-number-resolving (PNR) detector for maximum a posteriori (MAP) decision-making.

Result: Under ideal conditions: error probability approaches Helstrom bound across entire energy spectrum with constant factor of 3dB gap; in low-photon-number regime (N≈0.6), surpasses coherent-state limit with error rate below 1%; analysis of non-ideal conditions shows PNR robustness against dark counts and reveals "parity photon-number step" saturation effect from squeezing parameter mismatch.

Conclusion: IS-Kennedy receiver effectively translates transmitter's squeezing resources into displacement gain, achieving near-optimal quantum state discrimination performance while demonstrating robustness to practical imperfections.

Abstract: To address the discrimination problem of binary phase-shift keyed displaced squeezed vacuum states (S-BPSK), this paper proposes an Inverse-squeezing Kennedy (IS-Kennedy) receiver. This architecture incorporates an inverse-squeezing operator following the displacement operation of a conventional Kennedy receiver, mapping the S-BPSK signals onto equivalent large-amplitude coherent states. Furthermore, it employs a photon-number-resolving (PNR) detector to perform maximum a posteriori (MAP) decision-making. Theoretical analysis demonstrates that, under ideal conditions, the IS-Kennedy receiver effectively translates the transmitter's squeezing resources into a displacement gain at the receiver. Consequently, its error probability approaches the Helstrom bound across the entire energy spectrum, remaining within a constant factor of 3 dB. In the low-photon-number regime ($N \approx 0.6$), the proposed scheme surpasses the coherent-state limit, achieving an error rate below 1\%. Furthermore, this paper provides an in-depth analysis of system performance under non-ideal conditions, revealing the robustness of PNR detection against background dark counts and a characteristic ``parity photon-number step'' saturation effect arising from squeezing parameter mismatch.

</details>


### [24] [Introduction to Quantum Entanglement Geometry](https://arxiv.org/abs/2601.19111)
*Kazuki Ikeda*

Main category: quant-ph

TL;DR: The paper presents a geometric framework for understanding entanglement in quantum many-body systems by viewing it through the lens of global geometry and algebraic geometry, specifically using Azumaya algebras and Severi-Brauer schemes to characterize when subsystem decompositions exist globally.


<details>
  <summary>Details</summary>
Motivation: To provide a geometric perspective on entanglement in finite-dimensional quantum systems, moving beyond traditional mathematical treatments of quantum states to specifically focus on entanglement as a phenomenon of global geometry, particularly when quantum systems vary over classical parameter spaces.

Method: Uses algebraic geometry concepts: describes quantum systems varying over parameter spaces using Azumaya algebras, obtains pure-state spaces as Severi-Brauer schemes, characterizes subsystem decomposition existence through reduction to stabilizer subgroup of Segre variety, identifies obstruction in Brauer class, and demonstrates concepts with spin system on torus example.

Result: Shows that entanglement yields natural filtration on Severi-Brauer scheme, demonstrates that holonomy of gluing can produce entangling quantum gates and appear as obstruction class distinct from Berry/Chern numbers, and reveals that even systems without traditional topological band structure can have entanglement-related global geometric universal quantities reflecting background geometry.

Conclusion: Entanglement in quantum many-body systems can be understood as a global geometric phenomenon, with obstructions to global subsystem decomposition appearing in Brauer classes, providing a new geometric framework that connects entanglement to background geometry through algebraic geometric structures.

Abstract: This article is an expository account aimed at viewing entanglement in finite-dimensional quantum many-body systems as a phenomenon of global geometry. While the mathematics of general quantum states has been studied extensively, this article focuses specifically on their entanglement. When a quantum system varies over a classical parameter space, each fiber may look like the same Hilbert space, yet there may be no global identification because of twisting in the gluing data. Describing this situation by an Azumaya algebra, one always obtains the family of pure-state spaces as a Severi-Brauer scheme.
  The main focus is to characterize the condition under which the subsystem decomposition required to define entanglement exists globally and compatibly, by a reduction to the stabilizer subgroup of the Segre variety, and to explain that the obstruction appears in the Brauer class. As a consequence, quantum states yield a natural filtration dictated by entanglement on the Severi-Brauer scheme.
  Using a spin system on a torus as an example, we show concretely that the holonomy of the gluing can produce an entangling quantum gate, and can appear as an obstruction class distinct from the usual Berry numbers or Chern numbers. For instance, even for quantum systems that have traditionally been regarded as having no topological band structure, the entanglement of their eigenstates can be related to global geometric universal quantities, reflecting the background geometry.

</details>


### [25] [Evolution of quantum geometric tensor of 1D periodic systems after a quench](https://arxiv.org/abs/2601.19152)
*Jia-Chen Tang,Xu-Yang Hou,Yu-Huan Huang,Hao Guo. Chih-Chun Chien*

Main category: quant-ph

TL;DR: The paper investigates post-quench dynamics of the quantum geometric tensor in 1D periodic systems, showing how its components relate to physical observables like position variance, energy variance, and quench-induced curvature, using the SSH model as an example.


<details>
  <summary>Details</summary>
Motivation: To understand how the quantum geometric tensor evolves after a sudden Hamiltonian quench in 1D periodic systems and establish connections between QGT components and measurable physical observables in nonequilibrium dynamics.

Method: Theoretical analysis of QGT components in post-quench dynamics, with numerical verification using the Su-Schrieffer-Heeger (SSH) model for different quench protocols. The method examines diagonal QGT components with respect to crystal momentum and time, as well as off-diagonal components.

Result: Numerical results confirm that post-quench QGT is governed by physical quantities and local geometric objects from both initial state and post-quench bands, including Berry connection, group velocities, and energy variance. The diagonal momentum component gives position variance with ballistic dispersion, while the time component reveals energy variance.

Conclusion: The quantum geometric tensor serves as a comprehensive probe for nonequilibrium phenomena, with its components directly connected to physical observables like wavepacket dispersion and energy fluctuations, making it valuable for studying post-quench dynamics in quantum systems.

Abstract: We investigate the post-quench dynamics of the quantum geometric tensor (QGT) of 1D periodic systems with a suddenly changed Hamiltonian. The diagonal component with respect to the crystal momentum gives a metric corresponding to the variance of the time-evolved position, and its coefficient of the quadratic term in time is the group-velocity variance, signaling ballistic wavepacket dispersion. The other diagonal QGT component with respect to time reveals the energy variance. The off-diagonal QGT component features a real part as a covariance and an imaginary part representing a quench-induced curvature. Using the Su-Schrieffer-Heeger (SSH) model as an example, our numerical results of different quenches confirm that the post-quench QGT is governed by physical quantities and local geometric objects from the initial state and post-quench bands, such as the Berry connection, group velocities, and energy variance. Furthermore, the connections between the QGT and physical observables suggest the QGT as a comprehensive probe for nonequilibrium phenomena.

</details>


### [26] [The complexity of semidefinite programs for testing $k$-block-positivity](https://arxiv.org/abs/2601.19159)
*Qian Chen,Benoît Collins*

Main category: quant-ph

TL;DR: Analysis of complexity for k-block-positivity testing algorithm using symmetry reduction with rectangular Young diagrams, connecting to U(d) irreducible representations, explaining SDP hierarchy collapse at k=d.


<details>
  <summary>Details</summary>
Motivation: Extend previous work by analyzing computational complexity of k-block-positivity testing algorithm, particularly investigating why semidefinite program hierarchy collapses in the k=d case.

Method: Symmetry reduction scheme based on rectangular shaped Young diagrams, connecting algorithm complexity to dimensions of irreducible representations of U(d), deriving explicit complexity formula.

Result: Derived explicit formula for complexity of k-block-positivity testing algorithm, clarified why semidefinite program hierarchy collapses when k=d through representation theory analysis.

Conclusion: Symmetry reduction using rectangular Young diagrams provides theoretical understanding of algorithm complexity and explains SDP hierarchy collapse at k=d through representation-theoretic connections.

Abstract: We extend \cite{chen2025srkbp} by analyzing the complexity of the $k$-block-positivity testing algorithm. In this paper, we investigate a symmetry reduction scheme based on rectangular shaped Young diagrams. Connecting the complexity to the dimensions of irreducible representations of $\mathrm{U}(d)$, we derive an explicit formula for the complexity, which also clarifies why the semidefinite program hierarchy collapses in the $k=d$ case.

</details>


### [27] [High-Performance Exact Synthesis of Two-Qubit Quantum Circuits](https://arxiv.org/abs/2601.19166)
*Andrew N. Glaudell,Michael Jarret,Swan Klein,Samuel S. Mendelson,T. C. Mooney,Mingzhen Tian*

Main category: quant-ph

TL;DR: Exact synthesis framework for two-qubit Clifford+T circuits that optimizes T-count exactly using bounded search, algebraic canonicalization, and lookup tables.


<details>
  <summary>Details</summary>
Motivation: Exact synthesis provides unconditional optimality and canonical structure, but is typically limited to small regimes. The authors aim to overcome these limitations for two-qubit circuits over Clifford+T gate sets.

Method: Combines meet-in-the-middle search with provable pruning rules and problem-specific arithmetic. Uses bounded search space, algebraic canonicalization to avoid redundancy, and constructs a lookup table that turns synthesis into a query operation.

Result: An exact, reusable synthesis engine with substantially improved practical performance for two-qubit Clifford+T circuits that optimizes T-count exactly.

Conclusion: The framework enables exact synthesis for two-qubit circuits with improved performance, making exact optimization of T-count practical through algorithmic innovations and hardware-aware design.

Abstract: Exact synthesis provides unconditional optimality and canonical structure, but is often limited to small, carefully scoped regimes. We present an exact synthesis framework for two-qubit circuits over the Clifford+$T$ gate set that optimizes $T$-count exactly. Our approach exhausts a bounded search space, exploits algebraic canonicalization to avoid redundancy, and constructs a lookup table of optimal implementations that turns synthesis into a query. Algorithmically, we combine meet-in-the-middle ideas with provable pruning rules and problem-specific arithmetic designed for modern hardware. The result is an exact, reusable synthesis engine with substantially improved practical performance.

</details>


### [28] [The strong converse exponent of composable randomness extraction against quantum side information](https://arxiv.org/abs/2601.19182)
*Roberto Rubboli,Marco Tomamichel*

Main category: quant-ph

TL;DR: Tight characterization of strong converse exponent for quantum randomness extraction using fidelity-based composable error criterion, expressed via club-sandwiched conditional entropy.


<details>
  <summary>Details</summary>
Motivation: Previous bounds for randomness extraction against quantum side information lacked tight characterizations using composable error criteria based on fidelity/purified distance. The paper aims to establish the first precise operational interpretation of club-sandwiched conditional entropies in quantum information theory.

Method: Employed a composable error criterion defined by fidelity (or purified distance) to a uniform distribution in product with the marginal state. Used the club-sandwiched conditional entropy framework recently introduced by Rubboli, Goodarzi and Tomamichel, extending the approach used by Li, Li and Yu for classical side information to the quantum setting.

Result: Achieved a tight characterization of the strong converse exponent for randomness extraction against quantum side information. This provides the first operational interpretation of club-sandwiched conditional entropies in quantum information theory, establishing their fundamental role in characterizing randomness extraction performance.

Conclusion: The work successfully bridges the gap between classical and quantum randomness extraction theory by extending the strong converse exponent characterization to quantum side information using composable error criteria, while establishing the operational significance of club-sandwiched conditional entropies.

Abstract: We find a tight characterization of the strong converse exponent for randomness extraction against quantum side information. In contrast to previous tight bounds, we employ a composable error criterion given by the fidelity (or purified distance) to a uniform distribution in product with the marginal state. The characterization is in terms of a club-sandwiched conditional entropy recently introduced by Rubboli, Goodarzi and Tomamichel and used by Li, Li and Yu to establish the strong converse exponent for the case of classical side information. This provides the first precise operational interpretation of this family of conditional entropies in the quantum setting.

</details>


### [29] [Quantum simulation of the nonlinear Schrödinger equation via measurement-induced potential reconstruction](https://arxiv.org/abs/2601.19184)
*Kaiwen Weng,Zhaoyuan Meng,Zixuan Yang,Guohui Hu*

Main category: quant-ph

TL;DR: Hybrid quantum-classical framework for simulating nonlinear Schrödinger equation using split-step Fourier method with quantum linear propagation and classical nonlinear potential reconstruction.


<details>
  <summary>Details</summary>
Motivation: The nonlinear Schrödinger equation (NLSE) is fundamental for modeling complex phenomena, but quantum simulation is challenging due to nonlinear terms that are difficult to implement directly on quantum computers.

Method: Hybrid quantum-classical framework based on split-step Fourier method: quantum handles linear propagation via kinetic evolution operator, classical reconstructs nonlinear potentials from measured Fourier components using Hadamard test, then implements phase transformation via quantum circuit using phase kickback technique.

Result: Numerical simulations of Gaussian wave packet, soliton wave, and wake flow past a cylinder show excellent agreement with classical solutions, validating algorithm efficacy.

Conclusion: Provides concrete basis for analyzing accuracy-cost trade-offs in quantum-classical simulations of nonlinear dispersive wave dynamics, demonstrating viable hybrid approach for NLSE simulation.

Abstract: The nonlinear Schrödinger equation (NLSE) is a fundamental model that describes diverse complex phenomena in nature. However, simulating the NLSE on a quantum computer is inherently challenging due to the presence of the nonlinear term. We propose a hybrid quantum-classical framework for simulating the NLSE based on the split-step Fourier method. During the linear propagation step, we apply the kinetic evolution operator to generate an intermediate quantum state. Subsequently, the Hadamard test is employed to measure the Fourier components of low-wavenumber modes, enabling the efficient reconstruction of nonlinear potentials. The phase transformation corresponding to the reconstructed potential is then implemented via a quantum circuit using the phase kickback technique. To validate the efficacy of the proposed algorithm, we numerically simulate the evolution of a Gaussian wave packet, a soliton wave, and the wake flow past a cylinder. The simulation results demonstrate excellent agreement with the corresponding classical solutions. This work provide a concrete basis for analyzing accuracy-cost trade-offs in quantum-classical simulations of nonlinear dispersive wave dynamics.

</details>


### [30] [Analytical construction of $(n, n-1)$ quantum random access codes saturating the conjectured bound](https://arxiv.org/abs/2601.19190)
*Takayuki Suzuki*

Main category: quant-ph

TL;DR: Analytical construction of optimal (n, n-1)-QRACs achieving conjectured success probability bound, with efficient quantum circuit implementation and analysis of information-theoretic limits.


<details>
  <summary>Details</summary>
Motivation: The (n, n-1)-QRACs serve as ideal models for verifying quantum advantage in high-dimensional spaces, but analytical derivation of optimal codes for general n has remained an open problem, hindering scalable implementation and deeper theoretical understanding.

Method: Established analytical construction method using explicit operator formalism, proved it achieves the conjectured upper bound of average success probability, presented systematic algorithm to decompose optimal POVM into standard quantum gates, and analyzed high-dimensional limits.

Result: Proved construction strictly achieves conjectured upper bound P = 1/2 + √((n-1)/n)/2 for all n; developed decoding circuit with O(n) depth under linear connectivity; demonstrated O(log n) information-theoretic gap from Holevo bound for symmetric encoding despite suppressed measurement non-commutativity.

Conclusion: Provides scalable implementation method for high-dimensional quantum information processing and offers new insights into mathematical structure at quantum-classical boundary, solving long-standing open problem in QRAC optimization.

Abstract: Quantum Random Access Codes (QRACs) embody the fundamental trade-off between the compressibility of information into limited quantum resources and the accessibility of that information, serving as a cornerstone of quantum communication and computation. In particular, the $(n, n-1)$-QRACs, which encode $n$ bits of classical information into $n-1$ qubits, provides an ideal theoretical model for verifying quantum advantage in high-dimensional spaces; however, the analytical derivation of optimal codes for general $n$ has remained an open problem. In this paper, we establish an analytical construction method for $(n, n-1)$-QRACs by using an explicit operator formalism. We prove that this construction strictly achieves the numerically conjectured upper bound of the average success probability, $\mathcal{P} = 1/2 + \sqrt{(n-1)/n}/2$, for all $n$. Furthermore, we present a systematic algorithm to decompose the derived optimal POVM into standard quantum gates. Since the resulting decoding circuit consists solely of interactions between adjacent qubits, it can be implemented with a circuit depth of $O(n)$ even under linear connectivity constraints. Additionally, we analyze the high-dimensional limit and demonstrate that while the non-commutativity of measurements is suppressed, an information-theoretic gap of $O(\log n)$ from the Holevo bound inevitably arises for symmetric encoding. This study not only provides a scalable implementation method for high-dimensional quantum information processing but also offers new insights into the mathematical structure at the quantum-classical boundary.

</details>


### [31] [Universal Operational Privacy in Distributed Quantum Sensing](https://arxiv.org/abs/2601.19206)
*Min Namkung,Dong-Hyun Kim,Seongjin Hong,Yong-Su Kim,Su-Yong Lee,Hyang-Tag Lim*

Main category: quant-ph

TL;DR: A universal operational privacy framework for distributed quantum sensing using classical Fisher information matrix to ensure individual parameter privacy against untrusted parties, experimentally demonstrated with Heisenberg-limited precision using fewer photons than parameters.


<details>
  <summary>Details</summary>
Motivation: Privacy is crucial in distributed quantum sensing networks where multiple clients estimate parameters using shared quantum resources with potentially untrusted servers, but existing privacy conditions rely on idealized quantum bounds and don't capture realistic measurement constraints.

Method: Introduce a universal operational privacy framework formulated in terms of experimentally accessible classical Fisher information matrix, applicable to arbitrary protocols with singular information structures, providing protocol-independent criterion for individual parameter privacy.

Result: Experimental demonstration that a distributed quantum sensing protocol using fewer photons than estimated parameters simultaneously satisfies the universal privacy condition and achieves Heisenberg-limited precision.

Conclusion: Establishes universal operational constraints governing privacy in distributed quantum sensing networks and provides foundation for practical, privacy-preserving quantum sensing beyond full-rank regimes.

Abstract: Privacy is a fundamental requirement in distributed quantum sensing networks, where multiple clients estimate spatially distributed parameters using shared quantum resources while interacting with potentially untrusted servers. Despite its importance, existing privacy conditions rely on idealized quantum bounds and do not fully capture the operational constraints imposed by realistic measurements. Here, we introduce a universal operational privacy framework for distributed quantum sensing, formulated in terms of the experimentally accessible classical Fisher information matrix and applicable to arbitrary protocols characterized by singular information structures. The proposed condition provides a protocol-independent criterion ensuring that no information about individual parameters is accessible to untrusted parties. We further experimentally demonstrate that a distributed quantum sensing protocol employing fewer photons than the number of estimated parameters simultaneously satisfies the universal privacy condition and achieves Heisenberg-limited precision. Our results establish universal operational constraints governing privacy in distributed quantum sensing networks and provide a foundation for practical, privacy-preserving quantum sensing beyond full-rank regimes.

</details>


### [32] [Pareto-Front Engineering of Dynamical Sweet Spots in Superconducting Qubits](https://arxiv.org/abs/2601.19209)
*Zhen Yang,Shan Jin,Yajie Hao,Guangwei Deng,Xiu-Hao Deng,Re-Bing Wu,Xiaoting Wang*

Main category: quant-ph

TL;DR: A framework for optimizing fluxonium qubit coherence using periodic flux modulation at dynamical sweet spots, achieving 3-5× dephasing time improvement while maintaining microsecond relaxation times, with identified robust operating bands and high-fidelity gate protocols.


<details>
  <summary>Details</summary>
Motivation: Suppressing decoherence from low-frequency flux noise in superconducting qubits by operating at dynamical sweet spots, with the open question of fundamental limits on coherence extension and tradeoffs between relaxation and dephasing.

Method: Introduces a fully parameterized, multi-objective periodic-flux modulation framework that simultaneously optimizes energy relaxation (T₁) and pure dephasing (T_φ) for fluxonium qubits, identifying double-DSS regions insensitive to both DC and AC flux.

Result: Achieves 3-5× enhancement in T_φ compared to existing DSS strategies while maintaining T₁ in the hundred-microsecond range, establishes fundamental upper bound on achievable T₁, identifies robust operating bands, and demonstrates high-fidelity gate operations.

Conclusion: Establishes a general framework for Pareto-front engineering of dynamical sweet spots that substantially improves coherence and gate performance in superconducting qubits, with practical applications in single- and two-qubit control protocols.

Abstract: Operating superconducting qubits at dynamical sweet spots (DSSs) suppresses decoherence from low-frequency flux noise. A key open question is how long coherence can be extended under this strategy and what fundamental limits constrain it. Here we introduce a fully parameterized, multi-objective periodic-flux modulation framework that simultaneously optimizes energy relaxation $T_1$ and pure dephasing $T_φ$, thereby quantifying the tradeoff between them. For fluxonium qubits with realistic noise spectra, our method enhances $T_φ$ by a factor of 3-5 compared with existing DSS strategies while maintaining $T_1$ in the hundred-microsecond range. We further prove that, although DSSs eliminate first-order sensitivity to low-frequency noise, relaxation rate cannot be reduced arbitrarily close to zero, establishing an upper bound on achievable $T_1$. At the optimized working points, we identify double-DSS regions that are insensitive to both DC and AC flux, providing robust operating bands for experiments. As applications, we design single- and two-qubit control protocols at these operating points and numerically demonstrate high-fidelity gate operations. These results establish a general and useful framework for Pareto-front engineering of DSSs that substantially improves coherence and gate performance in superconducting qubits.

</details>


### [33] [Reinforcement Learning for Enhanced Advanced QEC Architecture Decoding](https://arxiv.org/abs/2601.19279)
*Yidong Zhou,Lingyi Kong,Yifeng Peng,Zhiding Liang*

Main category: quant-ph

TL;DR: Reinforcement learning techniques, including hybrid and multi-agent approaches, are applied to enhance decoding of advanced quantum error correction architectures, achieving improved logical error rates and scalability compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Advanced quantum error correction codes offer better resource utilization and performance than surface codes but introduce increased complexity that demands innovative decoding methodologies. Traditional decoding methods struggle with the complexity of these advanced architectures.

Method: The paper investigates reinforcement learning techniques, including hybrid and multi-agent approaches, to enhance decoding of various advanced QEC architectures. RL agents learn optimal strategies from noisy syndrome measurements by exploiting structural properties of modern QEC models. Different RL algorithms are combined to address multifaceted aspects of decoding like code degeneracy and real-world noise characteristics.

Result: The autonomously trained RL agent successfully derives decoding schemes for complex decoding requirements of advanced QEC architectures. The approach demonstrates potential for improved logical error rates and scalability compared to traditional decoding methods.

Conclusion: Reinforcement learning represents a promising approach for decoding advanced quantum error correction architectures, offering advantages in handling complexity, improving error rates, and enhancing scalability for practical quantum computation.

Abstract: The advent of promising quantum error correction (QEC) codes with efficient resource utilization and high-performance fault-tolerant quantum memories signifies a critical step towards realizing practical quantum computation. While surface codes have been a dominant approach, their limitations have spurred the development of more advanced QEC architectures. These advanced codes often present increased complexity, demanding innovative decoding methodologies. This work investigates the application of reinforcement learning (RL) techniques, including hybrid and multi-agent approaches, to enhance the decoding of various advanced QEC architectures. By leveraging the ability of RL to learn optimal strategies from noisy syndrome measurements, we explore the potential for achieving improved logical error rates and scalability compared to traditional decoding methods. Our approach examines the adaptation of reinforcement learning to exploit the structural properties of these modern QEC models. We also explore the benefits of combining different RL algorithms to address the multifaceted nature of the decoding problem, considering factors such as code degeneracy and real-world noise characteristics. With our proposed method, we are able to demonstrate that an autonomously trained agent can derive decoding schemes for the complex decoding requirement of advanced QEC architectures.

</details>


### [34] [Graphene Josephson Junctions for Engineering Motional Quanta](https://arxiv.org/abs/2601.19324)
*Zhen-Yang Peng,Mehdi Abdi*

Main category: quant-ph

TL;DR: Hybrid quantum device using graphene Josephson junctions couples graphene membrane vibrations to superconducting circuits for strong tunable coupling, enabling parametric processes, non-classical state generation, and enhanced quantum sensing.


<details>
  <summary>Details</summary>
Motivation: To develop a hybrid quantum device that leverages graphene's mechanical degrees of freedom for quantum information processing by coupling graphene membrane vibrations to superconducting circuits through Josephson junctions.

Method: Utilizes graphene Josephson junctions where flexural modes control Cooper pair tunneling, creating strong tunable coupling at zero-point fluctuations level. Implements parametric processes through this interaction and explores applications with nonlinear interactions.

Result: Demonstrates efficient parametric process implementation, fast generation of non-classical mechanical states, and critically enhanced quantum sensing capabilities under suitable quantum control.

Conclusion: The work establishes graphene's motional degrees of freedom as viable resources for quantum information processing in circuit quantum nanomechanical architectures, opening new possibilities for hybrid quantum devices.

Abstract: We propose a hybrid quantum device based on the graphene Josephson junctions, where the vibrational degrees of freedom of a graphene membrane couple to the superconducting circuits. The flexural mode-controlled tunneling of the Cooper pairs introduces a strong and tunable coupling even at the zero-point fluctuations level. By employing this interaction, we show that a parametric process can be efficiently implemented. We then investigate foundational and technological applications of our hybrid device empowered by nonlinear interactions, with fast generation of non-classical mechanical states, and critically enhanced quantum sensing under suitable quantum control. Our work provides the possibility of employing the graphene motional degree of freedom for quantum information processing in circuit quantum nanomechanical structures.

</details>


### [35] [Beyond Photon Shot Noise: Chemical Limits in Spectrophotometric Precision](https://arxiv.org/abs/2601.19326)
*Georg Engelhardt,Dahai He,JunYan Luo*

Main category: quant-ph

TL;DR: Chemical processes impose fundamental precision limits in spectrophotometry, with phase measurements outperforming intensity measurements and sensitivity showing three regimes based on reaction rates.


<details>
  <summary>Details</summary>
Motivation: To investigate precision limitations in spectrophotometry (spectroscopic concentration measurements) imposed by chemical processes of molecules, since chemical reactions affect electronic and optical properties that determine measurement sensitivity.

Method: Using Photon-resolved Floquet theory (generalizing Maxwell-Bloch theory for higher-order measurement statistics) to analyze a molecular model system subject to chemical reactions where electronic and optical properties depend on chemical state.

Result: Three key findings: (1) Phase measurements are more sensitive than intensity measurements; (2) Sensitivity exhibits three regimes: photon-shot-noise limited, chemically limited, and intermediate; (3) Sensitivity shows a turnover as a function of reaction rate due to interplay between coherent electronic dynamics and incoherent chemical dynamics.

Conclusion: Chemical properties must be considered to estimate ultimate precision limits in optical spectrophotometry, as chemical processes impose fundamental constraints on measurement sensitivity beyond photon statistics.

Abstract: In this work, we investigate precision limitations in spectrophotometry (i.e., spectroscopic concentration measurements) imposed by chemical processes of molecules. Using the recently developed Photon-resolved Floquet theory, which generalizes Maxwell-Bloch theory for higher-order measurement statistics, we analyze a molecular model system subject to chemical reactions whose electronic and optical properties depend on the chemical state. Analysis of sensitivity bounds reveals: (i) Phase measurements are more sensitive than intensity measurements; (ii) Sensitivity exhibits three regimes: photon-shot-noise limited, chemically limited, and intermediate; (iii) Sensitivity shows a turnover as a function of reaction rate due to the interplay between coherent electronic dynamics and incoherent chemical dynamics. Our findings demonstrate that chemical properties must be considered to estimate ultimate precision limits in optical spectrophotometry.

</details>


### [36] [Continuous-mode analysis of improved two-way CV-QKD](https://arxiv.org/abs/2601.19348)
*Yanhao Sun,Jiayu Ma,Xiangyu Wang,Song Yu,Ziyang Chen,Hong Guo*

Main category: quant-ph

TL;DR: The paper develops a security analysis framework for continuous-mode improved two-way CV-QKD, accounting for temporal modes, finite-size effects, and practical device nonidealities, showing the protocol maintains performance advantages over one-way systems.


<details>
  <summary>Details</summary>
Motivation: Practical CV-QKD implementations face device nonidealities that drive optical fields from single-mode to continuous-mode regimes, requiring proper characterization and security analysis for the improved two-way protocol.

Method: Introduces temporal modes to characterize optical field evolution in improved two-way CV-QKD, establishes continuous-mode security analysis framework using adaptive normalization with calibrated shot-noise unit, and incorporates finite-size effects.

Result: The improved two-way protocol retains performance advantage over one-way counterpart even in continuous-mode scenario with practical nonidealities and finite-size effects.

Conclusion: The analysis provides practical guidance for implementation and optimization of improved two-way CV-QKD systems, establishing a comprehensive security framework for continuous-mode operation.

Abstract: Continuous-variable quantum key distribution (CV-QKD) enables information-theoretically secure key generation between legitimate parties. To further enhance system performance, an improved two-way CV-QKD protocol has been proposed, which is accessible in practice and exhibits increased robustness against excess noise. However, in practical implementations, device nonidealities inevitably drive the optical field from the single-mode regime into the continuous-mode regime. In this work, we introduce temporal modes to characterize the evolution of optical fields in the improved two-way protocol and establish a security analysis framework for the continuous-mode scenario based on adaptive normalization with calibrated shot-noise unit. In addition, finite-size effects are taken into account in the analysis. Our results demonstrate that the improved two-way protocol retains a performance advantage over one-way counterpart. The analysis provides useful guidance for the practical implementation and performance optimization of improved two-way CV-QKD systems.

</details>


### [37] [Experimental High-Accuracy and Broadband Quantum Frequency Sensing via Geodesic Control](https://arxiv.org/abs/2601.19356)
*Si-Qi Chen,Qi-Tao Duan,Teng Li,He Lu*

Main category: quant-ph

TL;DR: Experimental demonstration of a broadband quantum frequency sensing protocol using geodesic control with single NV centers in diamond, achieving bias-free frequency estimation with harmonic suppression and millihertz resolution.


<details>
  <summary>Details</summary>
Motivation: Accurate frequency estimation across broad bandwidths in quantum sensing is often compromised by spurious responses to higher-order harmonics in realistic multi-frequency environments, creating systematic errors that limit practical applications.

Method: Implemented geodesic control using electron spin of single nitrogen-vacancy center in diamond, engineering intrinsically single-frequency response to suppress harmonic-induced errors, and incorporated synchronized readout for enhanced resolution.

Result: Achieved bias-free frequency estimation with strong harmonic suppression across megahertz to gigahertz range, and obtained millihertz-level frequency resolution under noisy conditions through synchronized readout.

Conclusion: Geodesic control provides practical approach for high-accuracy quantum frequency metrology in realistic multi-frequency environments, with systematic experimental benchmarking establishing its viability for broadband sensing applications.

Abstract: Accurate frequency estimation of oscillating signals over a broad bandwidth is a central task in quantum sensing, yet it is often compromised by spurious responses to higher-order harmonics in realistic multi-frequency environments. Here we experimentally demonstrate a high-accuracy and broadband quantum frequency sensing protocol based on geodesic control, implemented using the electron spin of a single nitrogen-vacancy center in diamond. By engineering an intrinsically single-frequency response, geodesic control enables bias-free frequency estimation with strong suppression of harmonic-induced systematic errors across a wide spectral range spanning from the megahertz to the gigahertz regime. Furthermore, by incorporating synchronized readout, we achieve millihertz-level frequency resolution under noisy signal conditions. Our results provide systematic experimental benchmarking of geodesic control for quantum frequency sensing and establish it as a practical approach for high-accuracy metrology in realistic environments.

</details>


### [38] [Remote magnon-phonon entanglement in the waveguide-magnomechanics](https://arxiv.org/abs/2601.19391)
*Shi-fan Qi,Fan Li*

Main category: quant-ph

TL;DR: Protocol for generating remote magnon-phonon entanglement in hybrid waveguide-magnomechanical systems using tailored pulsed drives and engineered interactions, enabling diverse long-distance entanglement types.


<details>
  <summary>Details</summary>
Motivation: Generating long-distance quantum entanglement is crucial for advancing quantum information processing, requiring practical schemes for remote entanglement generation in hybrid quantum systems.

Method: Proposes a protocol using a hybrid waveguide-magnomechanical system where multiple spatially separated magnon modes couple to a common waveguide while interacting with their respective phonon modes. Applies tailored pulsed drives and engineers magnon-phonon interactions to create entanglement.

Result: The scheme enables creation of diverse long-distance and dynamically stable entanglement: basic magnon-phonon two-mode entanglement, genuine multimode entanglement between single phonon and multiple magnons, bipartite entanglement between single magnon and multiple phonons, and genuine four-mode entanglement involving two magnons and two phonons. Shows dissipative magnon-magnon interactions mediated by traveling photons generate substantially stronger long-distance entanglement than coherent couplings.

Conclusion: Provides an experimentally feasible scheme for remote generation of magnon-phonon entanglement, advancing quantum information processing capabilities through long-distance entanglement generation in hybrid quantum systems.

Abstract: Generating long-distance quantum entanglement is crucial for advancing quantum information processing. In this work, we propose a protocol for generating remote magnon-phonon entanglement in a hybrid waveguide-magnomechanical system, where multiple spatially separated magnon modes couple to a common waveguide while interacting with their respective phonon modes. By applying tailored pulsed drives and engineering the magnon-phonon interactions, our scheme enables the creation of diverse long-distance and dynamically stable entanglement. Beyond basic magnon-phonon two-mode entanglement, it supports genuine multimode entanglement between a single phonon and multiple magnons, bipartite entanglement between a single magnon and multiple phonons, as well as genuine four-mode entanglement involving two magnons and two phonons. Moreover, we show that dissipative magnon-magnon interactions mediated by traveling photons can generate substantially stronger long-distance entanglement than coherent couplings. Our work provides an experimentally feasible scheme for the remote generation of magnon-phonon entanglement.

</details>


### [39] [Nanomechanical sensor resolving impulsive forces below its zero-point fluctuations](https://arxiv.org/abs/2601.19392)
*Martynas Skrabulis,Martin Colombano Sosa,Nicola Carlon Zambon,Andrei Militaru,Massimiliano Rossi,Martin Frimmer,Lukas Novotny*

Main category: quant-ph

TL;DR: Optically levitated nanoparticle measures impulsive forces below quantum limit using reversible squeezing of center-of-mass motion.


<details>
  <summary>Details</summary>
Motivation: Mechanical transducers have fundamental sensitivity limits set by quantum fluctuations. The goal is to measure forces smaller than a particle's zero-point momentum uncertainty, pushing beyond standard quantum limits.

Method: Use optically levitated nanoparticle with reversible squeezing of its center-of-mass motion to coherently amplify perturbations. This allows measurement of single impulsive-force kicks below the sensor's zero-point value.

Result: Demonstrated resolution of single impulsive-force kicks as small as 6.9 keV/c, which is 0.6 dB below the sensor's zero-point momentum uncertainty value.

Conclusion: Quantum squeezing techniques enable measurement of forces below the standard quantum limit, opening possibilities for ultra-sensitive force detection and quantum metrology applications.

Abstract: The sensitivity of a mechanical transducer is ultimately limited by its inherent quantum fluctuations. Here, we use an optically levitated nanoparticle to measure impulsive forces smaller than the particle's zero-point momentum uncertainty. Our approach relies on reversibly squeezing the levitated particle's center-of-mass motion to coherently amplify the perturbation. We demonstrate resolving single impulsive-force kicks as small as 6.9 keV/c, a value 0.6 dB below the sensor's zero-point value.

</details>


### [40] [Mikado strategy for the detection of atoms in images of microtrap arrays](https://arxiv.org/abs/2601.19396)
*Marc Cheneau,François Goudail*

Main category: quant-ph

TL;DR: New alternating estimation-detection strategy improves atom detection in microtrap arrays without explicit posterior modeling, enhancing accuracy for poorly resolved sites and robustness in experimental conditions.


<details>
  <summary>Details</summary>
Motivation: To improve atom detection in high-resolution microtrap array images, particularly when sites are not optically well resolved, and to increase robustness against real experimental conditions beyond previous methods.

Method: Alternating estimation and detection steps that eliminate the need for explicit posterior occupancy probability modeling, building on previous work [arXiv:2502.08511].

Result: Improved detection accuracy compared to previous work for poorly resolved sites, with expected greater robustness in experimental conditions.

Conclusion: The alternating estimation-detection strategy provides a more effective approach for atom detection in microtrap arrays without requiring explicit posterior modeling, offering practical advantages for experimental applications.

Abstract: Building on top of our recent work [arXiv:2502.08511], we introduce a new strategy to solve the problem of detecting atoms in high-resolution images of microtrap arrays. By alternating estimation and detection steps, we get rid of the need for an explicit model to compute the posterior occupancy probability of each site given its a priori optimal estimate. As direct benefits, we show an improved detection accuracy compared to our previous work when the sites are not optically well resolved, and we expect a greater robustness against real experimental conditions.

</details>


### [41] [Quantum Zeno-like Paradox for Position Measurements: A Particle Precisely Found in Space is Nowhere to be Found in Hilbert Space](https://arxiv.org/abs/2601.19469)
*Xabier Oianguren-Asua,Roderich Tumulka*

Main category: quant-ph

TL;DR: Perfect position measurement of a quantum particle yields zero probability for any subsequent quantum measurement outcome, suggesting need for novel quantum states beyond Hilbert space.


<details>
  <summary>Details</summary>
Motivation: To understand the quantum state of a particle after a perfect position measurement, since standard Hilbert space formalism appears insufficient to describe such scenarios where subsequent quantum measurements yield impossible probabilities.

Method: Analyze a quantum particle in unit interval [0,1] with position measurement of inaccuracy 1/n, followed by quantum measurement of projection |φ⟩⟨φ|. Study limit as n→∞ (perfect precision) and examine probability of Y=1 outcome.

Result: In the limit n→∞ (perfect position measurement), probability of Y=1 tends to 0 for every φ. No density matrix (pure or mixed) can yield outcome 1 with probability 0 for any |φ⟩⟨φ| measurement.

Conclusion: Standard Hilbert space quantum mechanics cannot describe quantum particle after perfect position measurement. A novel type of quantum state beyond Hilbert space formalism is necessary.

Abstract: On a quantum particle in the unit interval $[0,1]$, perform a position measurement with inaccuracy $1/n$ and then a quantum measurement of the projection $|φ\rangle\langleφ|$ with some arbitrary but fixed normalized $φ$. Call the outcomes $X \in[0,1]$ and $Y \in\{0,1\}$. We show that in the limit $n\to\infty$ corresponding to perfect precision for $X$, the probability of $Y=1$ tends to 0 for every $φ$. Since there is no density matrix, pure or mixed, which upon measurement of any $|φ\rangle\langleφ|$ yields outcome 1 with probability 0, our result suggests that a novel type of quantum state beyond Hilbert space is necessary to describe a quantum particle after a perfect position measurement.

</details>


### [42] [Flux-tunable transmon incorporating a van der Waals superconductor via an Al/AlO$_x$/4Hb-TaS$_2$ Josephson junction](https://arxiv.org/abs/2601.19581)
*Eliya Blumenthal,Ilay Mangel,Amit Kanigel,Shay Hacohen-Gourgy*

Main category: quant-ph

TL;DR: Hybrid Al/AlO_x/4Hb-TaS_2 Josephson junction transmon integrated into 3D cavity shows flux-tunable operation with sub-microsecond T1, but reveals discrepancies in Josephson energy and unresolved subgap modes.


<details>
  <summary>Details</summary>
Motivation: To extend circuit-QED beyond conventional Al/AlO_x/Al tunnel junctions by incorporating van der Waals superconductors, enabling microwave probes of unconventional condensates and subgap excitations in quantum circuits.

Method: Fabricated flux-tunable transmon with Al/AlO_x/4Hb-TaS_2 Josephson junction using sequential deposition and in-situ oxidation of ultrathin Al on exfoliated 4Hb-TaS_2 flake, embedded in 3D copper cavity for spectroscopy measurements.

Result: Achieved sub-microsecond energy relaxation (T1: 0.08-0.69 μs), flux-tunable spectrum quantitatively reproduced by transmon-cavity Hamiltonian, but found discrepancy between spectroscopic Josephson energy and Ambegaokar-Baratoff prediction, with dephasing faster than 16 ns resolution.

Conclusion: Establishes practical integration route for 4Hb-TaS_2 into quantum circuits, providing baseline for future designs to enhance coupling to boundary and subgap degrees of freedom in vdW superconductors, though material-specific subgap modes remain unresolved.

Abstract: Incorporating van der Waals (vdW) superconductors into Josephson elements extends circuit-QED beyond conventional Al/AlO$_x$/Al tunnel junctions and enables microwave probes of unconventional condensates and subgap excitations. In this work, we realize a flux-tunable transmon whose nonlinear inductive element is an Al/AlO$_x$/4Hb-TaS$_2$ Josephson junction. The tunnel barrier is formed by sequential deposition and full in-situ oxidation of ultrathin Al layers on an exfoliated 4Hb-TaS$_2$ flake, followed by deposition of a top Al electrode, yielding a robust, repeatable hybrid junction process compatible with standard transmon fabrication. Embedding the device in a three-dimensional copper cavity, we observe a SQUID-like flux-dependent spectrum that is quantitatively reproduced by a standard dressed transmon--cavity Hamiltonian, from which we extract parameters in the transmon regime. Across measured devices we obtain sub-microsecond energy relaxation ($T_1$ from $0.08$ to $0.69~μ$s), while Ramsey measurements indicate dephasing faster than our $16$ ns time resolution. We also find a pronounced discrepancy between the Josephson energy inferred from spectroscopy and that expected from the Ambegaokar--Baratoff relation using room-temperature junction resistances, pointing to nontrivial junction physics in the hybrid Al/AlO$_x$/4Hb-TaS$_2$ system. Although we do not resolve material-specific subgap modes in the present geometry, this work establishes a practical route to integrating 4Hb-TaS$_2$ into coherent quantum circuits and provides a baseline for future edge-sensitive designs aimed at enhancing coupling to boundary and subgap degrees of freedom in vdW superconductors.

</details>


### [43] [Broadcasting quantum nonlinearity in hybrid systems](https://arxiv.org/abs/2601.19610)
*Alisa D. Manukhova,Andrey A. Rakhubovsky,Radim Filip*

Main category: quant-ph

TL;DR: Proposal for achieving universal quantum processing with linear oscillators through light-mediated nonlinearity broadcasting from a nonlinear source system to target linear systems.


<details>
  <summary>Details</summary>
Motivation: Linear oscillators are fundamental to quantum science but lack the nonlinear operations required for universal quantum processing. Current applications (sensing, memory, communication) are limited by this linearity constraint.

Method: Use light-mediated interaction to broadcast nonlinear operations from a source system with nonlinearity beyond quadratic potential to target linear oscillators. The nonlinear source must have potential equivalent to more than quadratic.

Result: Theoretical proposal showing that such broadcasting enables nonlinear operations on linear systems, verifiable through negative Wigner function values and squeezing of nonlinear quadrature combinations below Gaussian thresholds.

Conclusion: Optically levitated mechanical oscillators serve as flexible nonlinearity sources for proof-of-principle demonstrations, enabling universal quantum processing with linear systems like mechanical oscillators or atomic spin ensembles.

Abstract: Linear oscillators contribute to most branches of contemporary quantum science. They have already successfully served as quantum sensors and memories, found applications in quantum communication, and hold promise for cluster-state-based quantum computing. To master universal quantum processing with linear oscillators, an unconditional nonlinear operation is required. We propose such an operation using light-mediated interaction with another system that possesses a nonlinearity equivalent to more than a quadratic potential. Such a potential grants access to a nonlinear operation that can be broadcast to the target linear system. The nonlinear character of the operation can be verified by observing adequate negative values of the target system's Wigner function and the squeezing of the variance of a certain nonlinear combination of the quadratures below the thresholds attainable by Gaussian states. We explicitly evaluate an optically levitated mechanical oscillator as a flexible source of nonlinearity for a proof-of-principle demonstration of the nonlinearity broadcasting to linear systems, for example, mechanical oscillators or macroscopic atomic spin ensembles.

</details>


### [44] [DynQ: A Dynamic Topology-Agnostic Quantum Virtual Machine via Quality-Weighted Community Detection](https://arxiv.org/abs/2601.19635)
*Shusen Liu,Pascal Jahan Elahi,Ugo Varetto*

Main category: quant-ph

TL;DR: DynQ is a dynamic, topology-agnostic Quantum Virtual Machine that virtualizes quantum hardware using quality-weighted community detection on live calibration data, enabling adaptive execution regions that maximize internal gate quality while minimizing inter-region coupling.


<details>
  <summary>Details</summary>
Motivation: Current quantum cloud platforms lack virtualization, forcing each user program to monopolize entire quantum processors, preventing resource sharing, economic scalability, and quality-of-service differentiation. Existing QVM designs using topology-specific or template-based partitioning are brittle under hardware heterogeneity, calibration drift, and transient defects that dominate real quantum devices.

Method: DynQ models quantum processors as weighted graphs derived from live calibration data and uses quality-weighted community detection to automatically discover execution regions. This approach maximizes internal gate quality while minimizing inter-region coupling, operationalizing the classical virtualization principle of high cohesion and low coupling in a quantum-native setting.

Result: On hardware with pronounced spatial quality variation, DynQ achieves up to 19.1% higher fidelity and 45.1% lower output error compared to state-of-the-art QVM and standard compilation baselines. When transient hardware defects cause baseline executions to fail completely, DynQ adapts dynamically and achieves over 86% fidelity. Evaluation was conducted across five IBM Quantum backends using calibration-derived noise simulation and on two production devices.

Conclusion: DynQ transforms calibrated device graphs into adaptive virtual hardware abstractions, decoupling quantum programs from fragile physical layouts and enabling reliable, high-utilization quantum cloud services. It represents the first dynamic, topology-agnostic Quantum Virtual Machine that effectively virtualizes quantum hardware in the presence of real-world device imperfections.

Abstract: Quantum cloud platforms remain fundamentally non-virtualised: despite rapid hardware scaling, each user program still monopolises an entire quantum processor, preventing resource sharing, economic scalability, and quality-of-service differentiation. Existing Quantum Virtual Machine (QVM) designs attempt spatial multiplexing through topology-specific or template-based partitioning, but these approaches are brittle under hardware heterogeneity, calibration drift, and transient defects, which dominate real quantum devices. We present DynQ, the first dynamic, topology-agnostic Quantum Virtual Machine that virtualises quantum hardware using quality-weighted community detection. Instead of imposing fixed geometric regions, DynQ models a quantum processor as a weighted graph derived from live calibration data and automatically discovers execution regions that maximise internal gate quality while minimising inter-region coupling. This operationalises the classical virtualisation principle of high cohesion and low coupling in a quantum-native setting, producing execution regions that are connectivity-efficient, noise-aware, and resilient to crosstalk and defects. We evaluate DynQ across five IBM Quantum backends using calibration-derived noise simulation and on two production devices, comparing against state-of-the-art QVM and standard compilation baselines. On hardware with pronounced spatial quality variation, DynQ achieves up to 19.1 percent higher fidelity and 45.1 percent lower output error. When transient hardware defects cause baseline executions to fail completely, DynQ adapts dynamically and achieves over 86 percent fidelity. By transforming calibrated device graphs into adaptive virtual hardware abstractions, DynQ decouples quantum programs from fragile physical layouts and enables reliable, high-utilisation quantum cloud services.

</details>


### [45] [Efficient Application of Tensor Network Operators to Tensor Network States](https://arxiv.org/abs/2601.19650)
*Richard M. Milbradt,Shuo Sun,Christian B. Mendl,Johnnie Gray,Garnet K. -L. Chan*

Main category: quant-ph

TL;DR: The paper introduces Cholesky-based compression (CBC), a new algorithm for applying tree tensor network operators to tree tensor network states, which outperforms most established methods by at least an order of magnitude in runtime and matches state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of tensor network methods by developing a more efficient algorithm for applying tree tensor network operators to tree tensor network states, which is a common subroutine in tensor network computations.

Method: Developed a new algorithm inspired by the density matrix method and Cholesky decomposition that efficiently applies tree tensor network operators to tree tensor network states. The method extends techniques commonly used for tensor train structures to general tree structures, with explicit inclusion of tensor train cases.

Result: CBC performs equivalently to current state-of-the-art methods while outperforming most established methods by at least an order of magnitude in runtime. In circuit simulation of tree-like circuits, more complex tree structures outperform simple linear structures with lower errors, and CBC remains among the most successful methods with less dependence on operator bond dimensions.

Conclusion: The Cholesky-based compression algorithm provides significant runtime improvements for tensor network computations while maintaining state-of-the-art accuracy, demonstrating that more complex tree structures can outperform simple linear structures in practical applications like circuit simulation.

Abstract: The performance of tensor network methods has seen constant improvements over the last few years. We add to this effort by introducing a new algorithm that efficiently applies tree tensor network operators to tree tensor network states inspired by the density matrix method and the Cholesky decomposition. This application procedure is a common subroutine in tensor network methods. We explicitly include the special case of tensor train structures and demonstrate how to extend methods commonly used in this context to general tree structures. We compare our newly developed method with the existing ones in a benchmark scenario with random tensor network states and operators. We find our Cholesky-based compression (CBC) performs equivalently to the current state-of-the-art method, while outperforming most established methods by at least an order of magnitude in runtime. We then apply our knowledge to perform circuit simulation of tree-like circuits, in order to test our method in a more realistic scenario. Here, we find that more complex tree structures can outperform simple linear structures and achieve lower errors than those possible with the simple structures. Additionally, our CBC still performs among the most successful methods, showing less dependence on the different bond dimensions of the operator.

</details>


### [46] [Transversal gates of the ((3,3,2)) qutrit code and local symmetries of the absolutely maximally entangled state of four qutrits](https://arxiv.org/abs/2601.19677)
*Ian Tan*

Main category: quant-ph

TL;DR: Bijection between LU orbits of AME states (perfect tensors) and LU orbits of quantum error correcting codes; uniqueness of 4-qutrit AME state and ((3,3,2))_3 code; connection between transversal gates and local symmetries.


<details>
  <summary>Details</summary>
Motivation: To establish a formal correspondence between absolutely maximally entangled (AME) states and quantum error correcting codes, specifically exploring the relationship between their local unitary orbits and symmetry groups.

Method: Proof of bijection between LU orbits of AME states and LU orbits of ((n-1,D,n/2))_D codes; analysis of 4-qutrit case using results from Rather et al. (2023); exploration of connection between transversal gates and local symmetries using Vinberg's theory of graded Lie algebras.

Result: Existence of bijection between LU orbits of AME states and QEC codes; uniqueness of 4-qutrit AME state and ((3,3,2))_3 code up to LU action; generators found for both transversal gates group and local symmetries group.

Conclusion: AME states and quantum error correcting codes are deeply connected through their LU orbits and symmetry structures, with the 4-qutrit case providing a concrete example where both the AME state and corresponding code are unique up to local unitaries.

Abstract: We provide a proof that there exists a bijection between local unitary (LU) orbits of absolutely maximally entangled (AME) states in $(\mathbb{C}^D)^{\otimes n}$ where $n$ is even, also known as perfect tensors, and LU orbits of $((n-1,D,n/2))_D$ quantum error correcting codes. Thus, by a result of Rather et al. (2023), the AME state of 4 qutrits and the pure $((3,3,2))_3$ qutrit code $\mathcal{C}$ are both unique up to the action of the LU group. We further explore the connection between the 4-qutrit AME state and the code $\mathcal{C}$ by showing that the group of transversal gates of $\mathcal{C}$ and the group of local symmetries of the AME state are closely related. Taking advantage of results from Vinberg's theory of graded Lie algebras, we find generators of both of these groups.

</details>


### [47] [Approximate Decoherence, Recoherence and Records in Isolated Quantum Systems](https://arxiv.org/abs/2601.19703)
*Philipp Strasberg,Joseph Schindler,Jiaozi Wang,Andreas Winter*

Main category: quant-ph

TL;DR: The paper studies quantum decoherence in isolated systems using decoherent histories framework, showing that imperfect decoherence limits detectable records and reveals recoherence patterns, with implications for Many Worlds Interpretation.


<details>
  <summary>Details</summary>
Motivation: To understand which past events leave detectable records in isolated quantum systems under realistic conditions where decoherence is approximate rather than perfect, and to explore implications for quantum foundations and interpretation.

Method: Uses decoherent histories framework with two approaches: 1) asymptotic analysis for large classes of (pseudo-)random histories, 2) numerically exact solution of a random matrix model to study decoherence structure for long histories.

Result: 1) Number of reliable records can be much smaller than possible events, depending on decoherence degree. 2) Reveals recoherence patterns: between histories with small Hamming distance, for localized histories with high purity Petz recovery states, and for maverick histories (statistical outliers from Born's rule).

Conclusion: The analysis provides insights into quantum measurement and interpretation: reveals a "branch selection problem" in Many Worlds Interpretation from viewing self-location as quantum state discrimination, and sheds light on emergence of Born's rule and theory confirmation problem.

Abstract: Using the framework of decoherent histories, we study which past events leave detectable records in isolated quantum systems under the realistic assumption that decoherence is approximate and not perfect. In the first part we establish -- asymptotically for a large class of (pseudo-)random histories -- that the number of reliable records can be much smaller than the number of possible events, depending on the degree of decoherence. In the second part we reveal a clear decoherence structure for long histories based on a numerically exact solution of a random matrix model that, as we argue, captures generic aspects of decoherence. We observe recoherence between histories with a small Hamming distance, for localized histories admitting a high purity Petz recovery state, and for maverick histories that are statistical outliers with respect to Born's rule. From the perspective of the Many Worlds Interpretation, the first part -- which views the self-location problem as a coherent version of quantum state discrimination -- reveals a "branch selection problem", and the second part sheds light on the emergence of Born's rule and the theory confirmation problem.

</details>


### [48] [Robust topological quantum state transfer with long-range interactions in Rydberg arrays](https://arxiv.org/abs/2601.19713)
*Siri Raupach,Beatriz Olmos,Mathias B. M. Svendsen*

Main category: quant-ph

TL;DR: Long-range interactions in 1D systems enable fast, robust topological quantum state transfer via edge states, with improved efficiency over nearest-neighbor models.


<details>
  <summary>Details</summary>
Motivation: Develop theoretical framework for fast, robust, high-fidelity topological quantum state transfer in 1D systems with long-range couplings, motivated by Rydberg atom chains with dipole-dipole interactions.

Method: Utilize extended Su-Schrieffer-Heeger and Rice-Mele models with long-range interactions supporting topologically protected edge states. Implement both time-independent protocols (coherent edge state dynamics) and time-dependent protocols (adiabatic parameter modulation) for excitation transfer.

Result: Long-range couplings enhance relevant energy gaps, substantially improving transfer efficiency compared to nearest-neighbor models. Transfer is robust against positional disorder due to topological origin.

Conclusion: Long-range interacting platforms (like Rydberg atom chains) show strong potential for reliable quantum state transfer, combining topological protection with enhanced performance through extended interactions.

Abstract: We develop a theoretical framework for fast, robust and high-fidelity topological quantum state transfer in one-dimensional systems with long-range couplings, motivated by chains of Rydberg atoms with dipole-dipole interactions. Such long-range interactions naturally give rise to extended Su-Schrieffer-Heeger and Rice-Mele models supporting topologically protected edge states. We show that these edge states enable high-fidelity edge-to-edge excitation transfer using both time-independent protocols, based on coherent edge state dynamics, and time-dependent protocols, based on adiabatic modulation of system parameters. Long-range couplings play a central role by enhancing the relevant energy gaps, leading to a substantial improvement in transfer efficiency compared to nearest neighbour models. The resulting transfer is robust against positional disorder, reflecting its topological origin and highlighting the potential of long-range interacting platforms for reliable quantum state transfer.

</details>


### [49] [Optimally Driven Dressed Qubits](https://arxiv.org/abs/2601.19719)
*Alon Salhov,Sagi Nechushtan,Alex Retzker*

Main category: quant-ph

TL;DR: A new dressed-qubit control protocol eliminates counter-rotating terms, removing the need for rotating-wave approximation and improving multiple performance metrics.


<details>
  <summary>Details</summary>
Motivation: Current dressed-qubit control protocols suffer from counter-rotating terms that force operation within rotating-wave approximation regimes, limiting applicability and performance across key operational aspects.

Method: Introduces a dressed-qubit control protocol using only a single coupling axis in the laboratory frame that optimally removes counter-rotating terms, provides general parameterization, and uses Floquet-based coherence-time expression.

Result: Eliminates need for rotating-wave approximation, delivers substantial improvements in single-qubit gate speed, two-qubit gate fidelity, spectroscopic range, clock stability, and coherence preservation.

Conclusion: The protocol represents state-of-the-art strategy for qubit control, enabling wider class of quantum technologies using dressed-qubit architectures.

Abstract: The applicability and performance of qubits dressed by classical fields are limited because their control protocols give rise to an undesired counter-rotating term (CRT). This in turn forces operation in a regime where a (dressed) rotating-wave approximation (RWA) is valid, thereby restricting key aspects of their operation. Here, using only a single coupling axis in the laboratory frame, we introduce a dressed-qubit control protocol that optimally removes the CRT, eliminating the need for the RWA and delivering substantial improvements in multiple performance metrics, including single-qubit gate speed, two-qubit gate fidelity, spectroscopic range, clock stability, and coherence preservation. In addition, we provide a general parameterization together with a Floquet-based coherence-time expression, which elucidates the protocol's working principles and lowers the barrier to adoption. Collectively, these advances position our scheme as the state-of-the-art strategy for qubit control, paving the way for a wider class of quantum technologies to be realized using dressed-qubit architectures.

</details>


### [50] [A two-mode model for black hole evaporation and information flow](https://arxiv.org/abs/2601.19737)
*Erfan Bayenat,Babak Vakili*

Main category: quant-ph

TL;DR: Two-oscillator model for black hole evaporation with geometric and radiation modes as coupled harmonic oscillators with opposite-sign Hamiltonians, showing energy exchange and entanglement generation.


<details>
  <summary>Details</summary>
Motivation: To develop a minimal model that captures key qualitative features of black hole evaporation, specifically energy transfer and information flow between geometric degrees of freedom and Hawking radiation modes.

Method: Develop a two-oscillator model where geometric degree of freedom and Hawking radiation mode are described by coupled harmonic oscillators with opposite signs in their free Hamiltonians. Analytical normal-mode structure derivation, introduction of smooth envelope functions for continuous effective description, and numerical simulations in truncated Fock space.

Result: Normal-mode structure obtained analytically; oscillators exchange quanta in approximately out-of-phase manner consistent with effective conservation of ⟨n_x⟩ - ⟨n_y⟩; reduced entropy S_x(t) shows periodic growth indicating entanglement generation.

Conclusion: Even a minimal two-mode framework can capture key qualitative features of energy transfer and information flow during black hole evaporation, demonstrating the model's utility for studying fundamental aspects of the evaporation process.

Abstract: We develop and analyze a two-oscillator model for black hole evaporation in which an effective geometric degree of freedom and a representative Hawking radiation mode are described by coupled harmonic oscillators with opposite signs in their free Hamiltonians. The normal-mode structure is obtained analytically and the corresponding modal amplitudes determine the pattern of energy exchange between the two sectors. To bridge the discrete and semiclassical pictures, we introduce smooth envelope functions that provide a continuous effective description along the geometric variable. Numerical simulations in a truncated Fock space show that the two oscillators exchange quanta in an approximately out-of-phase manner, consistent with an effective conservation of $\langle n_x\rangle - \langle n_y\rangle$. The reduced entropy $S_x(t)$ exhibits periodic growth, indicating entanglement generation. These results demonstrate that even a minimal two-mode framework can capture key qualitative features of energy transfer and information flow during evaporation.

</details>


### [51] [Quantum Circuit Pre-Synthesis: Learning Local Edits to Reduce $T$-count](https://arxiv.org/abs/2601.19738)
*Daniele Lizzio Bosco,Lukasz Cincio,Giuseppe Serra,M. Cerezo*

Main category: quant-ph

TL;DR: Q-PreSyn uses reinforcement learning to optimize quantum circuit representations before Clifford+T synthesis, achieving up to 20% T-count reduction without approximation error.


<details>
  <summary>Details</summary>
Motivation: T gates dominate fault-tolerant quantum computing costs, and existing local synthesis methods produce suboptimal results that depend heavily on circuit representation, leading to inefficient compilations.

Method: Proposes Q-PreSyn strategy that uses a reinforcement learning agent to identify effective sequences of local equivalence-preserving edits on circuit representations before synthesis, optimizing for reduced T-count.

Result: Experimental results show up to 20% reduction in T-count on circuits with up to 25 qubits when applied on top of existing synthesis algorithms, with no additional approximation error introduced.

Conclusion: Q-PreSyn effectively addresses the representation dependency problem in quantum circuit compilation by using RL to pre-optimize circuit structures, significantly reducing T-gate costs for fault-tolerant quantum computing.

Abstract: Compiling quantum circuits into Clifford+$T$ gates is a central task for fault-tolerant quantum computing using stabilizer codes. In the near term, $T$ gates will dominate the cost of fault tolerant implementations, and any reduction in the number of such expensive gates could mean the difference between being able to run a circuit or not. While exact synthesis is exponentially hard in the number of qubits, local synthesis approaches are commonly used to compile large circuits by decomposing them into substructures. However, composing local methods leads to suboptimal compilations in key metrics such as $T$-count or circuit depth, and their performance strongly depends on circuit representation. In this work, we address this challenge by proposing \textsc{Q-PreSyn}, a strategy that, given a set of local edits preserving circuit equivalence, uses a RL agent to identify effective sequences of such actions and thereby obtain circuit representations that yield a reduced $T$-count upon synthesis. Experimental results of our proposed strategy, applied on top of well-known synthesis algorithms, show up to a $20\%$ reduction in $T$-count on circuits with up to 25 qubits, without introducing any additional approximation error prior to synthesis.

</details>


### [52] [Inter-branch message transfer on superconducting quantum processors: a multi-architecture benchmark](https://arxiv.org/abs/2601.19762)
*Cameron V. Cogburn*

Main category: quant-ph

TL;DR: The paper benchmarks near-term superconducting quantum processors using Wigner's-friend circuit message transfer, comparing IBM Eagle, Nighthawk, and Heron processors for message sizes up to n=32 without error mitigation.


<details>
  <summary>Details</summary>
Motivation: To establish a practical benchmark for near-term superconducting quantum processors using inter-branch message transfer in Wigner's-friend circuits as a test case, enabling comparison of different IBM quantum hardware architectures.

Method: Implemented Violaris' unitary message-transfer primitive across IBM Eagle, Nighthawk, and Heron processors for message sizes up to n=32. Studied three message families (sparse/one-hot, half-weight, dense) and measured conditional string success probability, memory erasure after uncomputation, and correlation diagnostics including branch contrast and bitwise mutual information.

Result: Sparse messages compiled to essentially constant two-qubit depth, providing a depth-controlled noise probe: at n=32, conditional success probability spanned ≈0.07 to ≈0.68 across backends. Half-weight and dense messages incurred rapidly growing routing overhead, with transpiler-seed variability becoming a limitation near coherence frontier. Additional amplitude and divergence "cousins" sweeps quantified degradation with branch-conditioned complexity.

Conclusion: The Wigner's-friend message transfer circuit serves as an effective benchmark for near-term quantum processors, revealing significant performance variations across IBM architectures and highlighting the practical limitations imposed by routing overhead and transpiler variability, especially for complex message families.

Abstract: We treat inter-branch message transfer in a Wigner's-friend circuit as a practical benchmark for near-term superconducting quantum processors. Implementing Violaris' unitary message-transfer primitive, we compare performance across IBM Eagle, Nighthawk, and Heron (r2/r3) processors for message sizes up to $n=32$, without error mitigation. We study three message families -- sparse (one-hot), half-weight, and dense -- and measure conditional string success $p_{\mathrm{all}}=\Pr(P=μ\mid R=0)$, memory erasure after uncomputation, and correlation diagnostics (branch contrast and bitwise mutual information). The sparse family compiles to essentially constant two-qubit depth, yielding a depth-controlled probe of device noise: at $n=32$ we observe $p_{\mathrm{all}}$ spanning $\approx0.07$ to $\approx0.68$ across backends. In contrast, half and dense messages incur rapidly growing routing overhead, and transpiler-seed variability becomes a practical limitation near the coherence frontier. We further report an amplitude sweep (no-amplification test) and a divergence ``cousins'' sweep that quantifies degradation with branch-conditioned complexity. All data and figure-generation scripts are released.

</details>


### [53] [Spectral Codes: A Geometric Formalism for Quantum Error Correction](https://arxiv.org/abs/2601.19765)
*Satoshi Kanno,Yoshi-aki Shimada*

Main category: quant-ph

TL;DR: Quantum error correction reformulated through noncommutative geometry's spectral triples, where codes emerge as low-energy projections of Dirac operators, unifying classical/quantum codes and linking error correction performance to spectral gaps.


<details>
  <summary>Details</summary>
Motivation: To develop a unified geometric framework for quantum error correction that connects diverse code families (classical, stabilizer, GKP, topological) through a common mathematical language, enabling systematic analysis of error correction performance via spectral properties.

Method: Reformulate quantum error correcting codes as low energy spectral projections of Dirac-type operators in noncommutative geometry. Use spectral triples to encode locality, code distance, and Knill-Laflamme conditions as spectral/geometric properties of the Dirac operator's induced metric and spectrum.

Result: Recovered wide range of known codes (classical linear, stabilizer, GKP, topological) from single construction. Showed leakage from code space controlled by Dirac operator's spectral gap. Demonstrated code-preserving perturbations can systematically increase gap without altering logical subspace, providing geometric mechanism for enhancing error correction thresholds (illustrated for stabilizer code). Interpreted Berezin-Toeplitz quantization as mixed spectral code.

Conclusion: Quantum error correction can be viewed as universal low-energy phenomenon governed by spectral geometry. The spectral triple perspective provides common geometric language for classical/quantum codes, directly relates error correction performance to spectral properties, and offers geometric mechanism for threshold enhancement with implications for holographic quantum error correction.

Abstract: We present a new geometric perspective on quantum error correction based on spectral triples in noncommutative geometry. In this approach, quantum error correcting codes are reformulated as low energy spectral projections of Dirac type operators that separate global logical degrees of freedom from local, correctable errors. Locality, code distance, and the Knill Laflamme condition acquire a unified spectral and geometric interpretation in terms of the induced metric and spectrum of the Dirac operator. Within this framework, a wide range of known error correcting codes including classical linear codes, stabilizer codes, GKP type codes, and topological codes are recovered from a single construction. This demonstrates that classical and quantum codes can be organized within a common geometric language. A central advantage of the spectral triple perspective is that the performance of error correction can be directly related to spectral properties. We show that leakage out of the code space is controlled by the spectral gap of the Dirac operator, and that code preserving internal perturbations can systematically increase this gap without altering the encoded logical subspace. This yields a geometric mechanism for enhancing error correction thresholds, which we illustrate explicitly for a stabilizer code. We further interpret Berezin Toeplitz quantization as a mixed spectral code and briefly discuss implications for holographic quantum error correction. Overall, our results suggest that quantum error correction can be viewed as a universal low energy phenomenon governed by spectral geometry.

</details>


### [54] [Resolving Gauge Ambiguities of the Berry Connection in Non-Hermitian Systems](https://arxiv.org/abs/2601.19777)
*Ievgen I. Arkhipov*

Main category: quant-ph

TL;DR: The paper resolves gauge ambiguities in non-Hermitian quantum systems by introducing a covariant-derivative formalism based on the Hilbert space metric tensor, yielding a unique, real-valued Berry connection that transforms properly under gauge transformations.


<details>
  <summary>Details</summary>
Motivation: Non-Hermitian systems exhibit unique spectral and topological phenomena, but their geometric characterization is hindered by intrinsic ambiguities due to the biorthogonal GL(N,C) gauge freedom. This leads to four inequivalent definitions of the Berry connection, complex-valued holonomies, and inconsistent geometric frameworks.

Method: Introduces a covariant-derivative formalism built from the metric tensor of the Hilbert space underlying the non-Hermitian Hamiltonian. This approach yields a unique Berry connection that remains real-valued under arbitrary GL(N,C) frame changes and transforms as an affine gauge potential.

Result: The formalism resolves all gauge ambiguities, produces a unique real-valued Berry connection, reduces to conventional Berry/Wilczek-Zee connection in the Hermitian limit, and establishes a consistent geometric framework for Berry phases, non-Abelian holonomies, and topological invariants in non-Hermitian quantum systems.

Conclusion: The covariant-derivative approach provides an unambiguous, gauge-consistent geometric framework for characterizing Berry phases, holonomies, and topological invariants in quantum systems described by non-Hermitian Hamiltonians, resolving long-standing ambiguities in non-Hermitian quantum geometry.

Abstract: Non-Hermitian systems display spectral and topological phenomena absent in Hermitian physics; yet, their geometric characterization can be hindered by an intrinsic ambiguity rooted in the eigenspace of non-Hermitian Hamiltonians, which becomes especially pronounced in the pure quantum regime. Because left and right eigenvectors are not related by conjugation, their norms are not fixed, giving rise to a biorthogonal ${\rm GL}(N,{\mathbb C})$ gauge freedom. Consequently, the standard Berry connection admits four inequivalent definitions depending on how left and right eigenvectors are paired, giving rise to distinct Berry phases and generally complex-valued holonomies. Here we show that these ambiguities and the emergence of complex phases are fully resolved by introducing a covariant-derivative formalism built from the metric tensor of the Hilbert space of the underlying non-Hermitian Hamiltonian. The resulting unique Berry connection remains real-valued under an arbitrary ${\rm GL}(N,{\mathbb C})$ frame change, and transforms as an affine gauge potential, while reducing to the conventional Berry (or Wilczek-Zee) connection in the Hermitian limit. This establishes an unambiguous and gauge-consistent geometric framework for Berry phases, non-Abelian holonomies, and topological invariants in quantum systems described by non-Hermitian Hamiltonians.

</details>


### [55] [Simple broadband signal detection at the fundamental limit](https://arxiv.org/abs/2601.19816)
*Anthony M. Polloreno,Graeme Smith*

Main category: quant-ph

TL;DR: Quantum metrology protocol for broadband detection of weak oscillatory fields with unknown carrier frequency achieves near-optimal scaling using multi-resonant analog control and GHZ probes.


<details>
  <summary>Details</summary>
Motivation: Broadband detection of weak oscillatory fields with unknown carrier frequency is fundamental to applications like magnetometry, axion searches, and gravitational-wave sensing, requiring optimal quantum sensing strategies.

Method: Developed an all-analog multi-resonant protocol using a randomized Su-Schrieffer-Heeger control Hamiltonian and an m-register GHZ probe state, with theoretical analysis showing the Grover-like integration-time lower bound is a geometric corollary of an upper bound on integrated quantum Fisher information.

Result: Demonstrated near-optimal scaling through simulation, showing the protocol approaches fundamental metrological constraints for broadband field detection with unknown frequency.

Conclusion: The work establishes fundamental limits for broadband quantum sensing and provides a practical analog protocol that approaches these limits, advancing quantum metrology for applications requiring detection of weak oscillatory fields with unknown frequencies.

Abstract: Broadband detection of a weak oscillatory field with unknown carrier frequency underlies magnetometry, axion searches and gravitational-wave sensing. We show that the Grover-like integration-time lower bound for this task is a geometric corollary an upper bound on the integrated quantum Fisher information, a metrological constraint. We further give an all-analog multi-resonant protocol based on a randomized Su-Schrieffer-Heeger control Hamiltonian and an m-register GHZ probe and verify near-optimal scaling through simulation.

</details>


### [56] [Enhanced quantum state discrimination under general measurements with entanglement and nonorthogonality restrictions](https://arxiv.org/abs/2601.19820)
*Swati Choudhary,Aparajita Bhattacharyya,Ujjwal Sen*

Main category: quant-ph

TL;DR: Non-positive operator-valued measurements can achieve sub-Helstrom discrimination error without requiring initial system-auxiliary entanglement, challenging conventional assumptions about quantum state discrimination limits.


<details>
  <summary>Details</summary>
Motivation: The Helstrom limit for minimum error probability in quantum state discrimination assumes standard positive operator-valued measurements (POVMs). The paper investigates whether this bound can be surpassed using alternative measurement strategies that go beyond conventional POVMs, particularly examining whether initial entanglement between system and auxiliary is necessary for such improvements.

Method: The authors explore measurement strategies beyond standard POVMs, referred to as non-positive operator-valued measurements. They analyze scenarios with constrained resource access and demonstrate that initial product states (rather than entangled states) between system and auxiliary can give rise to effective non-positive measurements on the subsystem, enabling improved discrimination.

Result: The paper shows that error probability for discriminating two quantum states can be reduced below the Helstrom bound using non-positive operator-valued measurements. Crucially, initial entanglement between system and auxiliary is not necessary - even initial product states can generate effective non-positive measurements on the subsystem that achieve sub-Helstrom discrimination error.

Conclusion: The Helstrom bound is not fundamental when measurement strategies extend beyond standard POVMs. Initial system-auxiliary entanglement is not required for achieving sub-Helstrom discrimination error, as product states can also enable non-positive measurements that surpass conventional discrimination limits, challenging established assumptions about quantum measurement theory.

Abstract: The minimum error probability for distinguishing between two quantum states is bounded by the Helstrom limit, derived under the assumption that measurement strategies are restricted to positive operator-valued measurements. We explore scenarios in which the error probability for discriminating two quantum states can be reduced below the Helstrom bound under some constrained access of resources, indicating the use of measurement operations that go beyond the standard positive operator-valued measurements framework. We refer to such measurements as non-positive operator-valued measurements. While existing literature often associates these measurements with initial entanglement between the system and an auxiliary, followed by joint projective measurement and discarding the auxiliary, we demonstrate that initial entanglement between system and auxiliary is not necessary for the emergence of such measurements in the context of state discrimination. Interestingly, even initial product states can give rise to effective non-positive measurements on the subsystem, and achieve sub-Helstrom discrimination error when discriminating quantum states of the subsystem.

</details>


### [57] [A Folded Surface Code Architecture for 2D Quantum Hardware](https://arxiv.org/abs/2601.19823)
*Zhu Sun,Zhenyu Cai*

Main category: quant-ph

TL;DR: The paper proposes using short-range qubit shuttling in 2D devices to achieve effective 3D connectivity, enabling native implementation of folded surface codes with constant-time logical Clifford gates and CNOTs, plus improved magic-state distillation efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of strictly 2D quantum computing architectures by leveraging qubit shuttling to create effective 3D connectivity, enabling more efficient quantum error correction and logical operations.

Method: Builds on Cai et al.'s scalable architecture using short-range shuttling to achieve quasi-3D connectivity on 2D hardware. Implements folded surface codes natively, develops explicit protocols for logical operations, and introduces a "virtual-stack" layout for efficient multilayer routing.

Result: Achieves constant-time logical Clifford gates and CNOTs (vs. O(d) in conventional lattice surgery), reduces spacetime volume of 8T-to-CCZ magic-state distillation by >10× compared to standard 2D approaches, and enables efficient multilayer routing via virtual-stack layout.

Conclusion: Short-range qubit shuttling enables effective 3D connectivity on 2D devices, significantly improving quantum error correction efficiency through folded surface codes, faster logical operations, and more compact magic-state distillation.

Abstract: Qubit shuttling has become an indispensable ingredient for scaling leading quantum computing platforms, including semiconductor spin, neutral-atom, and trapped-ion qubits, enabling both crosstalk reduction and tighter integration of control hardware. Cai et al. (2023) proposed a scalable architecture that employs short-range shuttling to realize effective three-dimensional connectivity on a strictly two-dimensional device. Building on recent advances in quantum error correction, we show that this architecture enables the native implementation of folded surface codes on 2D hardware, reducing the runtime of all single-qubit logical Clifford gates and logical CNOTs within subsets of qubits from $\mathcal{O}(d)$ in conventional surface code lattice surgery to constant time. We present explicit protocols for these operations and demonstrate that access to a transversal $S$ gate reduces the spacetime volume of 8T-to-CCZ magic-state distillation by more than an order of magnitude compared with standard 2D lattice surgery approaches. Finally, we introduce a new "virtual-stack" layout that more efficiently exploits the quasi-three-dimensional structure of the architecture, enabling efficient multilayer routing on these two-dimensional devices.

</details>


### [58] [Theory of low-weight quantum codes](https://arxiv.org/abs/2601.19848)
*Fuchuan Wei,Zhengyi Han,Austin Yubo He,Zimu Li,Zi-Wen Liu*

Main category: quant-ph

TL;DR: The paper proves NP-hardness of computing optimal code weight for stabilizer codes, characterizes weight ≤3 codes, develops LP methods for weight-constrained parameter bounds, and applies to practical architectures like IBM's 127-qubit chip.


<details>
  <summary>Details</summary>
Motivation: Low check weight is crucial for fault-tolerant quantum computing, driving interest in qLDPC codes. The paper aims to establish theoretical foundations for weight-constrained stabilizer codes, addressing the computational complexity of weight optimization and practical feasibility boundaries.

Method: 1) Prove NP-hardness of computing optimal code weight; 2) Systematic investigation of feasible code parameters with weight constraints; 3) Provide analytical lower bounds; 4) Complete characterization of stabilizer codes with weight ≤3; 5) Develop linear programming (LP) scheme for setting code parameter bounds with weight constraints; 6) Refine constraints considering generator weight distribution and overlap; 7) Apply methods to practical architectures like IBM 127-qubit chip.

Result: 1) Optimal code weight computation is NP-hard; 2) Stabilizer codes with weight ≤3 have distance 2 and code rate ≤1/4; 3) LP scheme yields exact optimal weight values for all code parameters with n≤9; 4) Methods successfully applied to practical quantum computing architectures.

Conclusion: The study establishes weight as a crucial parameter in quantum coding theory, provides theoretical foundations for weight-constrained stabilizer codes, offers practical guidance for code design, and demonstrates applicability to real-world quantum computing systems like IBM's 127-qubit chip.

Abstract: Low check weight is practically crucial code property for fault-tolerant quantum computing, which underlies the strong interest in quantum low-density parity-check (qLDPC) codes. Here, we explore the theory of weight-constrained stabilizer codes from various foundational perspectives including the complexity of computing code weight and the explicit boundary of feasible low-weight codes in both theoretical and practical settings. We first prove that calculating the optimal code weight is an $\mathsf{NP}$-hard problem, demonstrating the necessity of establishing bounds for weight that are analytical or efficiently computable. Then we systematically investigate the feasible code parameters with weight constraints. We provide various explicit analytical lower bounds and in particular completely characterize stabilizer codes with weight at most 3, showing that they have distance 2 and code rate at most 1/4. We also develop a powerful linear programming (LP) scheme for setting code parameter bounds with weight constraints, which yields exact optimal weight values for all code parameters with $n\leq 9$. We further refined this constraint from multiple perspectives by considering the generator weight distribution and overlap. In particular, we consider practical architectures and demonstrate how to apply our methods to e.g.~the IBM 127-qubit chip. Our study brings the weight as a crucial parameter into coding theory and provide guidance for code design and utility in practical scenarios.

</details>


### [59] [Symmetric and Antisymmetric Quantum States from Graph Structure and Orientation](https://arxiv.org/abs/2601.19857)
*Matheus R. de Jesus,Eduardo O. C. Hoefel,Renato M. Angelo*

Main category: quant-ph

TL;DR: Graph states' exchange symmetry corresponds to graph completeness: complete graphs yield fully symmetric states, while complete directed graphs with specific orientations generate fully antisymmetric states for odd qudit numbers.


<details>
  <summary>Details</summary>
Motivation: To establish a precise correspondence between graph topology and exchange symmetry in multipartite quantum states, providing a unified graph-theoretic framework for describing both bosonic (symmetric) and fermionic (antisymmetric) exchange properties.

Method: 1. Prove that standard graph states (generated by controlled-Z interactions) are fully symmetric under particle permutations if and only if the underlying graph is complete. 2. Introduce a generalized graph-based construction using a non-commutative two-qudit gate (GR) that requires directed edges and explicit vertex ordering. 3. Show that complete directed graphs with appropriate orientations generate fully antisymmetric multipartite states for odd numbers of qudits.

Result: 1. Established necessary and sufficient condition: graph state symmetry ⇔ complete graph. 2. Demonstrated that complete directed graphs with proper orientations produce fully antisymmetric states for odd qudit numbers. 3. Provided unified graph-theoretic description of bosonic and fermionic exchange symmetry based on graph completeness and edge orientation.

Conclusion: Graph completeness determines exchange symmetry: complete undirected graphs yield symmetric states, while complete directed graphs with specific orientations yield antisymmetric states. This provides a unified graph-theoretic framework for describing both bosonic and fermionic multipartite entanglement through graph topology and edge orientation.

Abstract: Graph states provide a powerful framework for describing multipartite entanglement in quantum information science. In their standard formulation, graph states are generated by controlled-$Z$ interactions and naturally encode symmetric exchange properties. Here we establish a precise correspondence between graph topology and exchange symmetry by proving that a graph state is fully symmetric under particle permutations if and only if the underlying graph is complete. We then introduce a generalized graph-based construction using a non-commutative two-qudit gate, denoted $GR$, which requires directed edges and an explicit vertex ordering. We show that complete directed graphs endowed with appropriate orientations, for an odd number of qudits generate fully antisymmetric multipartite states. Together, these results provide a unified graph-theoretic description of bosonic and fermionic exchange symmetry based on graph completeness and edge orientation.

</details>


### [60] [Distinguishing synthetic unravelings on quantum computers](https://arxiv.org/abs/2601.19889)
*Eloy Piñol,Piotr Sierant,Dustin Keys,Romain Veyron,Miguel Angel García-March,Tanner Reese,Morgan W. Mitchell,Jan Wehr,Maciej Lewenstein*

Main category: quant-ph

TL;DR: Experimental demonstration that different quantum measurement schemes (unravelings) produce distinct trajectory statistics despite identical ensemble-averaged dynamics, implemented on superconducting qubits.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that different monitoring/intervention schemes (unravelings) of the same GKSL master equation produce distinct quantum trajectory statistics beyond what's captured by ensemble averages, and to provide an accessible experimental demonstration using quantum hardware.

Method: Designed two synthetic unravelings (projective measurement and random-unitary "kick" unravelings) as quantum circuits on 1-2 qubits that share identical ensemble-averaged evolution. Implemented protocols on IBM Quantum superconducting-qubit hardware to access trajectory-level information.

Result: Variance across trajectories and ensemble-averaged von Neumann entropy distinguish the unravelings in both theory and experiment, while unconditional state and linear expectation values remain identical.

Conclusion: Quantum trajectories encode information about measurement backaction beyond what is fixed by unconditional dynamics, providing an accessible demonstration of unraveling-dependent nonlinear statistics.

Abstract: Distinct monitoring or intervention schemes can produce different conditioned stochastic quantum trajectories while sharing the same unconditional (ensemble-averaged) dynamics. This is the essence of unravelings of a given Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) master equation: any trajectory-ensemble average of a function that is linear in the conditional state is completely determined by the unconditional density matrix, whereas applying a nonlinear function before averaging can yield unraveling-dependent results beyond the average evolution. A paradigmatic example is resonance fluorescence, where direct photodetection (jump/Poisson) and homodyne or heterodyne detection (diffusive/Wiener) define inequivalent unravelings of the same GKSL dynamics. In earlier work, we showed that nonlinear trajectory averages can distinguish such unravelings, but observing the effect in that optical setting requires demanding experimental precision. Here we translate the same idea to a digital setting by introducing synthetic unravelings implemented as quantum circuits acting on one and two qubits. We design two unravelings - a projective measurement unraveling and a random-unitary "kick" unraveling - that share the same ensemble-averaged evolution while yielding different nonlinear conditional-state statistics. We implement the protocols on superconducting-qubit hardware provided by IBM Quantum to access trajectory-level information. We show that the variance across trajectories and the ensemble-averaged von Neumann entropy distinguish the unravelings in both theory and experiment, while the unconditional state and the ensemble-averaged expectation values that are linear in the state remain identical. Our results provide an accessible demonstration that quantum trajectories encode information about measurement backaction beyond what is fixed by the unconditional dynamics.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [61] [Observation of Erratic Non-Hermitian Skin Localization and Transport](https://arxiv.org/abs/2601.19749)
*Jia-Xin Zhong,Jee Woo Kim,Stefano Longhi,Yun Jing*

Main category: cond-mat.dis-nn

TL;DR: Discovery of erratic non-Hermitian skin localization (ENHSL) - a new regime where wave packets exhibit ballistic transport despite strong spectral localization, decoupling eigenstate localization from transport dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional understanding of localization in disordered systems shows that disorder strongly impedes transport, typically resulting in dynamical localization, sub-ballistic, or diffusive dynamics. The authors seek to identify and characterize a previously unobserved regime that could challenge this conventional wisdom.

Method: Experimental realization using an acoustic lattice implementing a disordered Hatano-Nelson chain with imaginary gauge fields. Combined Green's-function-based spectroscopy with time-resolved measurements on the same platform to reconstruct full complex spectrum and eigenstates, and directly observe wave-packet dynamics.

Result: Observation of ballistic transport despite strong spectral localization - a counterintuitive phenomenon. Experimental demonstration of ENHSL featuring macroscopic, disorder-dependent localization at irregular bulk positions with subexponential decay. Development of transport theory connecting dominant propagation site to maximal random-walk excursion within expanding light cone, predicting universal Levy-arcsine statistics that quantitatively matches experimental data.

Conclusion: ENHSL represents a new paradigm for wave dynamics that decouples eigenstate localization from transport, challenging traditional understanding of localization phenomena in disordered non-Hermitian systems.

Abstract: Localization is a pervasive phenomenon across physics, shaping transport from electrons in solids to light and sound in engineered media. In traditional settings, disorder strongly impedes transport, resulting in dynamical localization or, at best, sub-ballistic or diffusive dynamics. A distinct and previously unobserved regime, erratic non-Hermitian skin localization (ENHSL), can arise in globally reciprocal non-Hermitian lattices with disorder. It features macroscopic, disorder-dependent localization at irregular bulk positions with subexponential decay, linked to stochastic interfaces governed by the universal order statistics of random walks. We realize this regime experimentally in an acoustic lattice implementing a disordered Hatano-Nelson chain with imaginary gauge fields. Using Green's-function-based spectroscopy together with time-resolved measurements on the same platform, we reconstruct the full complex spectrum and eigenstates, and directly observe wave-packet dynamics. Remarkably, we observe ballistic transport despite strong spectral localization. We develop a transport theory that connects the dominant propagation site to the maximal random-walk excursion within an expanding light cone and predicts a universal Levy-arcsine statistics, in quantitative agreement with experiment. Our results decouple eigenstate localization from transport and establish ENHSL as a new paradigm for wave dynamics.

</details>
